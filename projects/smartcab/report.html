<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>smartcab</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Machine-Learning-Engineer-Nanodegree">Machine Learning Engineer Nanodegree<a class="anchor-link" href="#Machine-Learning-Engineer-Nanodegree">&#182;</a></h1><h2 id="Reinforcement-Learning">Reinforcement Learning<a class="anchor-link" href="#Reinforcement-Learning">&#182;</a></h2><h2 id="Project:-Train-a-Smartcab-to-Drive">Project: Train a Smartcab to Drive<a class="anchor-link" href="#Project:-Train-a-Smartcab-to-Drive">&#182;</a></h2><p>Welcome to the fourth project of the Machine Learning Engineer Nanodegree! In this notebook, template code has already been provided for you to aid in your analysis of the <em>Smartcab</em> and your implemented learning algorithm. You will not need to modify the included code beyond what is requested. There will be questions that you must answer which relate to the project and the visualizations provided in the notebook. Each section where you will answer a question is preceded by a <strong>'Question X'</strong> header. Carefully read each question and provide thorough answers in the following text boxes that begin with <strong>'Answer:'</strong>. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide in <code>agent.py</code>.</p>
<blockquote><p><strong>Note:</strong> Code and Markdown cells can be executed using the <strong>Shift + Enter</strong> keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Getting-Started">Getting Started<a class="anchor-link" href="#Getting-Started">&#182;</a></h2><p>In this project, you will work towards constructing an optimized Q-Learning driving agent that will navigate a <em>Smartcab</em> through its environment towards a goal. Since the <em>Smartcab</em> is expected to drive passengers from one location to another, the driving agent will be evaluated on two very important metrics: <strong>Safety</strong> and <strong>Reliability</strong>. A driving agent that gets the <em>Smartcab</em> to its destination while running red lights or narrowly avoiding accidents would be considered <strong>unsafe</strong>. Similarly, a driving agent that frequently fails to reach the destination in time would be considered <strong>unreliable</strong>. Maximizing the driving agent's <strong>safety</strong> and <strong>reliability</strong> would ensure that <em>Smartcabs</em> have a permanent place in the transportation industry.</p>
<p><strong>Safety</strong> and <strong>Reliability</strong> are measured using a letter-grade system as follows:</p>
<table>
<thead><tr>
<th style="text-align:center">Grade</th>
<th style="text-align:center">Safety</th>
<th style="text-align:center">Reliability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A+</td>
<td style="text-align:center">Agent commits no traffic violations,<br/>and always chooses the correct action.</td>
<td style="text-align:center">Agent reaches the destination in time<br />for 100% of trips.</td>
</tr>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">Agent commits few minor traffic violations,<br/>such as failing to move on a green light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 90% of trips.</td>
</tr>
<tr>
<td style="text-align:center">B</td>
<td style="text-align:center">Agent commits frequent minor traffic violations,<br/>such as failing to move on a green light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 80% of trips.</td>
</tr>
<tr>
<td style="text-align:center">C</td>
<td style="text-align:center">Agent commits at least one major traffic violation,<br/> such as driving through a red light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 70% of trips.</td>
</tr>
<tr>
<td style="text-align:center">D</td>
<td style="text-align:center">Agent causes at least one minor accident,<br/> such as turning left on green with oncoming traffic.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 60% of trips.</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">Agent causes at least one major accident,<br />such as driving through a red light with cross-traffic.</td>
<td style="text-align:center">Agent fails to reach the destination on time<br />for at least 60% of trips.</td>
</tr>
</tbody>
</table>
<p>To assist evaluating these important metrics, you will need to load visualization code that will be used later on in the project. Run the code cell below to import this code which is required for your analysis.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Import the visualization code</span>
<span class="kn">import</span> <span class="nn">visuals</span> <span class="kn">as</span> <span class="nn">vs</span>

<span class="c1"># Pretty display for notebooks</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Understand-the-World">Understand the World<a class="anchor-link" href="#Understand-the-World">&#182;</a></h3><p>Before starting to work on implementing your driving agent, it's necessary to first understand the world (environment) which the <em>Smartcab</em> and driving agent work in. One of the major components to building a self-learning agent is understanding the characteristics about the agent, which includes how the agent operates. To begin, simply run the <code>agent.py</code> agent code exactly how it is -- no need to make any additions whatsoever. Let the resulting simulation run for some time to see the various working components. Note that in the visual simulation (if enabled), the <strong>white vehicle</strong> is the <em>Smartcab</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-1">Question 1<a class="anchor-link" href="#Question-1">&#182;</a></h3><p>In a few sentences, describe what you observe during the simulation when running the default <code>agent.py</code> agent code. Some things you could consider:</p>
<ul>
<li><em>Does the Smartcab move at all during the simulation?</em></li>
<li><em>What kind of rewards is the driving agent receiving?</em></li>
<li><em>How does the light changing color affect the rewards?</em>  </li>
</ul>
<p><strong>Hint:</strong> From the <code>/smartcab/</code> top-level directory (where this notebook is located), run the command</p>
<div class="highlight"><pre><span></span><span class="s1">&#39;python smartcab/agent.py&#39;</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%</span><span class="k">run</span> -i smartcab/agent.py
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>
/-------------------------
| Training trial 1
\-------------------------

Simulating trial. . . 
epsilon = 0.9700; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-39.0369868766
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-38.7405730886
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.1177701045
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.18)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-4.09409272552
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.26836759384
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.34)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-0.0265372238181
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.64235370907
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-5.58680084519
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.613509365884
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.63)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.067453978759
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.02)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.29171982861
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.31)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-0.156194904222
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded -0.16)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.218013898793
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.22)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-19.0968624701
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.69)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-9.88461668592
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.19)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-38.9997489092
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.21)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.331052355831
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.34)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.444799950976
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.46)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.88236401613
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.94)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.944208149057
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 2
\-------------------------

Simulating trial. . . 
epsilon = 0.9409; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.36157437911
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.43)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-8.85609393751
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.13)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.824122121646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.85)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.0041654426
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-9.75483229834
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-19.2209102399
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.82)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-5.21472368583
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.529771496754
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove left instead of forward. (rewarded 0.55)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.83946327193
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.86)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.69968220009
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.72)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-38.4194190236
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.61)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-39.1209403526
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.33)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.92665110557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-5.7744375736
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-5.64358859307
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.82)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.935945739086
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 0.96)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.150931544406
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded -0.16)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-38.4673100316
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.66)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.89041519376
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-10.4056177161
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.73)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.2592285741
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.30)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.73200040054
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.79)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.3961332683
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-10.135371267
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.45)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.776498967331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.80)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
-4.26890339575
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.40)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.151619361983
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.13)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
-0.368939863702
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove left instead of forward. (rewarded -0.38)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
0.843040071164
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 0.87)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
0.787716524067
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.81)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 3
\-------------------------

Simulating trial. . . 
epsilon = 0.9127; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.742734690869
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 0.77)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24384553172
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-10.6407838566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.550631325136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 0.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-9.75730588133
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8761771791
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.93)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-9.78197499321
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.08)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.788575947695
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.77)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-19.5874484307
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.0811104022096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove left instead of forward. (rewarded 0.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.44649660805
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.49)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-5.14354960675
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.399434396708
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.41)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.10251350643
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.14)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.47605733701
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.55)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.55087442711
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.60)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-5.61662416044
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.79)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-4.46921387055
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.61)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-19.4418754987
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.04)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.93426684728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.96)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.816903508772
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.84)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.540809571372
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.47)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.4366215974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent drove right instead of forward. (rewarded -0.47)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.52860972439
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.58)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
-0.354798282673
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded -0.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 4
\-------------------------

Simulating trial. . . 
epsilon = 0.8853; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.1952976031
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.38)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.199191393489
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.21)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-9.54842588783
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.48806997135
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.20749712432
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.23)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-5.7083279405
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.36862219491
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-8.94248262635
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-10.3452052417
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.67)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-9.75377016189
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.84535770561
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.90)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.68761003196
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.74)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 5
\-------------------------

Simulating trial. . . 
epsilon = 0.8587; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23821396654
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.22)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.358785129115
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.35)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71682479155
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.77)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-8.8069592701
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.08)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.692070477511
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.71)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.20795474914
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.18)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.50938016674
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-20.2380725716
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.86)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.69702317867
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.75)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.410053735479
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.38)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.64199000853
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.69)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.895329781389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.90)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-0.031442835949
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.03)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.318896773335
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.33)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-10.3855609381
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.71)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-9.95681897905
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.26)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.506981825034
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.52)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.2390544777
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.28)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-9.91702257668
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.22)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.36893954106
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.39164871418
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.41)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 6
\-------------------------

Simulating trial. . . 
epsilon = 0.8330; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.51843112846
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-4.53201347514
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.91846443348
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.55940739179
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.61)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-5.75347401447
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.93)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.93089932175
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.91)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.911288128883
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.36635011475
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.35)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.29799269683
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.33)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-4.36067550369
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.929524273703
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.96)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.18228933012
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.22)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.429344589133
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.44)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.08877264358
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.12)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-4.55534533184
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.70)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.2186734759
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.23)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.841589988487
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.87)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-9.32763777994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.62)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.68571553276
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.74)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.40275542046
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.45)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-5.60211158552
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.78)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.608768377619
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.63)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.62423819894
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.687213252927
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.66)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.262232015995
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 7
\-------------------------

Simulating trial. . . 
epsilon = 0.8080; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20290272194
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.16124050063
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.0701450248
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.286079812435
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.45134587332
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.42563834813
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.47)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.44721499005
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.16400638982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.18)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.58798413922
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.59)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-10.2725356509
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.59)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.41041515731
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-18.6188824684
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.19)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.31732437529
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.36)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-9.13740021028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.42)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.236373757896
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.24)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.563431845225
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.54)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.626283861254
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.61)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.61319665218
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.271096999048
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.28)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-39.5617078668
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.79)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Simulating trial. . . 
epsilon = 0.7837; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-18.8330812903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.42)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.0244948783
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.06)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-38.7513729987
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.00234352629631
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.479904574394
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.45)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.37716654915
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.45)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.18709407608
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 2.25)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.625639258935
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.59)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.45597615547
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.467901779348
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.48)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.110951596662
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.280679887805
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.29)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.13773757033
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.17)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.06687334112
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.35591115041
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.36)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-38.9025377344
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.14398153835
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.14)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.562017955956
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove left instead of forward. (rewarded -0.58)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.69181864025
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.74)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 9
\-------------------------

Simulating trial. . . 
epsilon = 0.7602; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54814619129
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.55)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.3426759208
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.36)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82994913622
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.86)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.84121830675
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.90)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17981738345
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.25)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.06737463526
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.685155860615
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.81336007934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.92196012427
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.98)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.870323135762
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove left instead of forward. (rewarded 0.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.04520823627
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.03)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.41667616231
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.38)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.610738610101
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.63)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-20.1664832283
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.79)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-10.5476217666
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.87)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.02939505637
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.06)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-5.47928394056
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.65)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-20.2541525409
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.88)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.16726544285
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.20)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-10.0945021991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Simulating trial. . . 
epsilon = 0.7374; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.12706547444
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.64510448201
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove forward instead of right. (rewarded 1.70)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.884535180378
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87746909007
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.89)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-39.5224989336
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.74)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-10.4822515177
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.3259829052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.66124890027
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.7671662424
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.80156368917
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-4.08816250908
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.21)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.39827749206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 1.44)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.716955922555
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.68)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-5.47180406164
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.49823195718
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.53)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-9.07313871958
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.35)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.48357006938
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-39.244868099
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.46)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.341965909287
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.35)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.76178888848
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.79)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 11
\-------------------------

Simulating trial. . . 
epsilon = 0.7153; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.29544187052
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.488942567532
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.0561415620487
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 0.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.83823507562
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.848829598741
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.88)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.28798641394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.5076389977
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.82081887844
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.47053928812
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.249604459433
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.26)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.221726899268
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded -0.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-20.1119085433
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.73)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.743662024446
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.70)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.475087875836
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-38.8424617477
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.04)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.249756359669
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded -0.26)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-4.95218985435
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.18384292146
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.21)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.33432661157
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.27)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.93143040053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 12
\-------------------------

Simulating trial. . . 
epsilon = 0.6938; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.395926274975
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74826247081
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56734336581
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.61)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.41211512232
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.42675522232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.45)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.168650705601
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 0.12)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00574428819
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.01416225773
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 1.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-4.15051654057
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.396928257825
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.37)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.557098874444
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.55)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.57241512294
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 1.62)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-5.35736007781
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0214672105104
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.02)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.39100572368
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.46)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.867348353839
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.88)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.73340444123
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.00820219706599
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded -0.03)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.490120270366
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.48)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.216026680299
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.19)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 13
\-------------------------

Simulating trial. . . 
epsilon = 0.6730; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.704971680645
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.403341843447
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.39)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.65653841425
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-38.2973526711
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.48)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.68983712817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.37263188604
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.39)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.979692730768
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.00)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.61890813302
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.64)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.74747292313
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.71)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 14
\-------------------------

Simulating trial. . . 
epsilon = 0.6528; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.24070834169
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.23)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.76465392137
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.77)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.39011246351
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.39)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.189084545785
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.18)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.50353560783
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.425495346249
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 0.39)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24602338441
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.25)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.0265196968123
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 0.02)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.69827230488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.935085732738
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-38.2593761173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.44)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.01721464729
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.04)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.02546710855
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.06)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-19.9627307198
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.58)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.877589383043
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.89)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.21740088653
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-39.0753747069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.28)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.22780178687
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 1.26)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.37579044806
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.42)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-39.5839473486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.81)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-10.1813706224
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.50)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.653281571713
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 0.66)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
2.05077888017
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.11244725834
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.13)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.9898949057
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Simulating trial. . . 
epsilon = 0.6333; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.389704627587
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.35)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24585031781
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.27)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.55263868193
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded 1.58)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.66279402981
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.68)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.70786065083
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.76)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.45368038432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.08650595997
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.4950683117
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded 1.50)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07045133556
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.07)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.48749885625
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.50)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-9.06976854417
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.35)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.24032506235
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.24)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-5.77586081623
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.798983854687
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.82)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.47656128757
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.50)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.42224616793
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.47)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.36423599188
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.38)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.42502201343
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.39)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.70459167521
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.0399835594519
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of left. (rewarded -0.09)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-0.203886402458
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove left instead of right. (rewarded -0.21)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.39415398844
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 1.42)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.827935058817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.82)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-19.4922017132
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.10)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.742075205915
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.77)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.37988796447
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.38)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
1.08721021056
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.12)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
1.56421102188
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
-10.5859925093
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.91)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
1.9998816619
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

learned value
1.33062892377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.34)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

learned value
-0.1253629845
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded -0.17)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

learned value
-0.452065418115
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent drove right instead of forward. (rewarded -0.51)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

learned value
0.0510723454282
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.01)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

learned value
0.23205055706
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 0.18)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 16
\-------------------------

Simulating trial. . . 
epsilon = 0.6143; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.02858050384
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41673457729
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31032118673
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.35)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.47464831353
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.61448160064
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.65)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.25014628891
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.32)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.59613016269
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.60)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.37873940813
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.684176972725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.69)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.5309357034
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.55)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.26759446735
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.898518638093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.93)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.06927134724
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.13)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.952413397648
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 0.94)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.46466830221
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.48)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.74329999414
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.72)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-9.13268270879
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.42)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.22223712266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.26)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.1285041773
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.13)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.914717394443
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.90)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 17
\-------------------------

Simulating trial. . . 
epsilon = 0.5958; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.92939980161
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.99)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-4.9632408353
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11294175259
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.13)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.80985293765
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.4363872957
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.45)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.67343609275
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.71)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.77231139688
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-38.2094448864
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.193214777002
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.20)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.60816983667
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.815412333112
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.84)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.78426856494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.81)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.53757063173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.23408075078
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.21)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 18
\-------------------------

Simulating trial. . . 
epsilon = 0.5780; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.406352071
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.53299847172
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42515885104
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.50529222902
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.57773686254
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-0.0797409037597
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove left instead of forward. (rewarded -0.08)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.61340518467
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.59)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.28249931967
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.6774950115
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.73)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.52880745717
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.57)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.22055608764
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.79701541983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.79)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.74079719753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.57304226838
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.57)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-20.0953910925
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.72)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.56014067422
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.54)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 19
\-------------------------

Simulating trial. . . 
epsilon = 0.5606; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.485318896437
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.48)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0975949929908
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.09)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.70923633237
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.46461413243
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.48)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51741625433
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.49)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.57181575984
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92806121027
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62070580955
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.84120310813
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.87)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.87914139011
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.88)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.99835594203
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.37352306043
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.76649749771
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.79)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.57716583561
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.58)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.629093311253
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.63)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.50729117574
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.29263772984
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.68643857628
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.68)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 20
\-------------------------

Simulating trial. . . 
epsilon = 0.5438; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.07303314638
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.56264938097
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.53)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.95768229583
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.314657784301
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.29)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.98694475729
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.160220987924
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 0.12)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.87567061727
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.79686681096
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.85)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.7073860449
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.9277950043
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.90)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-39.3178105401
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.59827775334
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.61)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.57929630757
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.60)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.486324728982
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove forward instead of right. (rewarded 0.45)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.277168736674
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.370404629169
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.38)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.13984787897
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded -0.18)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-8.78111029069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.05)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.8854812109
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.662439691565
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.68)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.31044804521
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.41611225054
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.637613851545
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.66)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-0.770128240192
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.80)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.11536791452
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.08)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 21
\-------------------------

Simulating trial. . . 
epsilon = 0.5275; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.63543837286
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11258857819
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60223379481
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.4447203657
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.45)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06200188397
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.05)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.62164301142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.64)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-10.157428987
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.47)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.17243289942
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.63342329393
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.66)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.20286959334
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0508013884907
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of forward. (rewarded -0.06)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.725242289749
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.70)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.837621451802
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.82)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.882689385387
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.83)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.71263015285
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.77)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.964668252434
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.95)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.697874061401
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.68)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.9627258847
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.97)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.3210383364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.36)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.18130051798
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.13)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 22
\-------------------------

Simulating trial. . . 
epsilon = 0.5117; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.46182403537
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.1132900701
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.10)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03990719323
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.5911702053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.86646262086
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.0961854363
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-4.20793388525
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52746376052
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.52)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.233940005614
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 0.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.32113572511
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.39)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.18803259807
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.0102009324075
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.00)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.27970197953
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.31)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-9.25260204567
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.54)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.509889748048
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.48)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.177810507239
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove forward instead of right. (rewarded -0.20)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.842340103043
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.82)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.0787394811402
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded -0.11)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.90535007945
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.96)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.724798914569
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded -0.77)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 23
\-------------------------

Simulating trial. . . 
epsilon = 0.4963; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.3807294734
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.56030001958
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.73089745682
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.04233450001
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.958609318402
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.38406887839
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.41)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.17800375701
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.25)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.16723143582
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.19)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.280893010276
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.28)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.88407081018
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.17824428756
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-18.9855083606
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.69499547221
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.381160268817
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.37)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.6504113202
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.733949846167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.74)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.63418660221
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.63)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.200903717905
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded -0.25)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-5.18501845719
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.35)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.445010373733
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.46)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-19.6454167172
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.25)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.983456259349
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.01)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.239269336368
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of forward. (rewarded -0.25)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
2.05715383522
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.09)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.00928036761
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 24
\-------------------------

Simulating trial. . . 
epsilon = 0.4814; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58302847836
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43512329768
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove left instead of right. (rewarded 1.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.92066759991
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88537852383
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11825662475
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.94367788166
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.92)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.55449938437
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.30076394506
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44761912235
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.50479608302
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.54)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.824859027973
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.85)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 25
\-------------------------

Simulating trial. . . 
epsilon = 0.4670; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-19.8982711339
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.0933831285
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.14)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02917877267
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.00)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.683325787658
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.62074195728
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.731456922171
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.0549447497767
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove left instead of right. (rewarded -0.10)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-5.124429734
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.28)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60645892438
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.58)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.02802372808
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.01)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.769321479767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.77)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.07426642725
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.20771066813
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.717236699731
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.74)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 26
\-------------------------

Simulating trial. . . 
epsilon = 0.4530; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.67137498279
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.69)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.13855214185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.90018704071
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.96)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.28444435048
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.27)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.770882781393
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.24095697893
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.25)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64891032286
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.23808282333
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.23)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.04718800995
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.37770322291
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.70526700064
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.33515595203
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-39.7524593385
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.98)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.71045698093
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.835969120454
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.82)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.06969802457
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.57534524791
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.60)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.08103669905
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.04)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 27
\-------------------------

Simulating trial. . . 
epsilon = 0.4394; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.472415149227
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.94713850443
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.97)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.04409899291
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.11119985294
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.46526212356
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.23513893438
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.85552640089
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.89)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-10.616452997
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.52957169338
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.52)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.54970051647
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.57)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.83728412389
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.574517891789
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.59)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.97698169566
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.30605058686
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.30)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.847329673698
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.82)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.86955110752
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.3265140803
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded -0.34)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.50321852709
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.48)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-0.226495569862
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded -0.25)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.17379524782
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.21)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.49121408625
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.45)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.14153985386
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.84366615017
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.86)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.155261631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.13)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.42142852577
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 28
\-------------------------

Simulating trial. . . 
epsilon = 0.4262; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.2637540145
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.00269860207
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.16885443463
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.61715913455
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33343771539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.60695873974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62372043999
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.64)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.34304869111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.38)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.05591422116
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.05)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.58275035557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.62)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.13711197946
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.12)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.06477734413
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.35397884187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.39)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.35446090937
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.36)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.68545265029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-8.84975296787
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.12)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.85260853957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.32984745509
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.29)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 29
\-------------------------

Simulating trial. . . 
epsilon = 0.4134; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-9.33910316351
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.63)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.665229728736
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.69)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-9.12196770084
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.40)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.7896867171
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.83)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79274051128
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.85)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.155796604277
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.11)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-9.60277322385
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.90)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.61930107044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.66)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-8.9571577314
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.23)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.101558856275
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.08)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.261198554136
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.27)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.51612590603
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.93578275164
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.92)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.28807716899
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.857911774745
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.83)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.967159849428
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.96)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.20258317548
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.833606986099
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.44561914559
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-20.2623919351
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.89)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.575163393752
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.55)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.411046589823
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.35)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 30
\-------------------------

Simulating trial. . . 
epsilon = 0.4010; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.63411464299
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.68)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.39684858247
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.29704635477
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.26)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35284902563
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.02589637123
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.0969317252307
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.10)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.33334125559
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.39)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-38.8917938217
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.09)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-9.962959245
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.35546015153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.40)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.49971979672
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.16734376886
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.15)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.48178293516
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.51)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.712376234793
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.73)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.21436295416
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.20)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.844705894066
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.87)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.47807072612
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.49)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.97264980334
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.00)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.4342319559
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.93581594728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.96)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.28527231657
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.27)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.19676901656
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.16)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.01696906595
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.04)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.32424122017
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.514321193572
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 31
\-------------------------

Simulating trial. . . 
epsilon = 0.3890; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.972292132561
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.69131773138
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.77)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.38703589131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.31377227919
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.00652805182997
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.24136425072
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.24)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.49359796259
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.53)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-9.12778309262
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-9.71823017538
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.02)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.16032310344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.866994863494
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 0.83)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.83512394604
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-39.1195557821
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.33)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.85917995666
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.33149055375
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.29)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.973646269888
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.975175343331
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.461466363851
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.45)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.938168093974
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.97)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.792322907344
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.76)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 32
\-------------------------

Simulating trial. . . 
epsilon = 0.3773; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.31672943217
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.36)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07318315528
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.14)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43175850886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.97659927187
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.99)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.51095218269
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09616134011
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.05)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.0276693967406
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.03)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88763570666
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.89)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.08106664947
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.08)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.37172422801
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.38)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.76625751795
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.78)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.53829493807
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.85791511498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.25414525
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.402112290022
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.37)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.20545252396
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.27)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.3838842478
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.43)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.85106585136
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.84)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 33
\-------------------------

Simulating trial. . . 
epsilon = 0.3660; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-18.6528579386
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.23)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.01681134115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.52343981812
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.46680735817
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06013122037
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.03587264205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.88125873547
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.86)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-8.82137659735
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.09)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.859348036285
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.65459778128
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 34
\-------------------------

Simulating trial. . . 
epsilon = 0.3550; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.44115882166
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22323286575
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.23)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25625509209
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.67105878116
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.396375584
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.21507532764
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.21)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.63307249533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.453902557389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.42)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.48473131207
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.49)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.34854121609
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.66816495817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.64)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.69316581741
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 1.74)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.853289706228
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.82)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.09925308192
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 1.10)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-38.4832300965
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.67)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.241896886728
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded 0.22)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.34299134896
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.39)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.54413801107
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.55)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.21140577636
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.25)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.37075997995
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-0.0226996197879
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.07)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.19036518176
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.15)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.06221752989
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.06)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-0.071058685983
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.07)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
-0.816362962013
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent drove forward instead of right. (rewarded -0.85)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 35
\-------------------------

Simulating trial. . . 
epsilon = 0.3444; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.68831531673
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.67)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.82931813229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.48402948978
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88412089886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.93)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.12863616272
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.988754900195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.98)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.68393383141
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.65)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.25602914713
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.26)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.80045813189
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.24638529593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.28)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4137832229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.46)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.19384624362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.29104853588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.53260081593
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.17038524111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.847436856035
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.81)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.939420015691
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 36
\-------------------------

Simulating trial. . . 
epsilon = 0.3340; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.55914248344
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.263934920148
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.32455629134
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.22730534211
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-37.8824521464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.05)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.3456330044
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.32)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.0393445526
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.10)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 37
\-------------------------

Simulating trial. . . 
epsilon = 0.3240; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.46190268665
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-39.5375409881
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.60376695015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.60)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36353730759
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.38250198587
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.41)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79556650219
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.949627977192
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-9.56114131048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-38.4044966274
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.59)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.63338600749
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-10.6392826939
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.97)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.2047831151
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.21)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-18.6633155328
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.24)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.28970010066
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.27)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 38
\-------------------------

Simulating trial. . . 
epsilon = 0.3143; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.74415121677
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.76)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25038625896
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4178943479
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.07279697492
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-9.41352307519
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.70)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.88160252505
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-19.720796789
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.957446403541
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.49196291242
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.49)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.39896526505
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.41)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 39
\-------------------------

Simulating trial. . . 
epsilon = 0.3049; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-8.74374663972
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.01)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25924580495
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.24)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25356885134
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.47819622839
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.55)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.519063339598
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.52)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-39.4926637038
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.71)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.796054378658
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.78)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.4158695883
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.49)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.77253381941
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.82)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.87084862445
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.88)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.62654046496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.62)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.96150461008
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.97)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.62262579253
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.32847347371
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-4.00440637539
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.13)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.16454245368
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-39.4635858909
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.68)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.59197340789
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.64)
49% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 40
\-------------------------

Simulating trial. . . 
epsilon = 0.2957; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-19.0721377524
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.66)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.71176665793
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.12894335337
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08120799074
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.07)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.5885336785
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.698459404936
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38093083656
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.92354379463
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.93)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 41
\-------------------------

Simulating trial. . . 
epsilon = 0.2868; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.35050858915
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.8996650152
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.93)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58668061006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.60)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-39.7055651887
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.04914533476
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.90202261663
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.599385622828
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.62)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.42407527466
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.43)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.55323201214
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.78232262876
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.84)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.544870503587
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.48717835709
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.796801465286
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.74)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.53128531998
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.54)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.4137931073
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.92219298814
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.47554564932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-4.73165872947
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.904443579223
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.88)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.464696521367
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.46)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 42
\-------------------------

Simulating trial. . . 
epsilon = 0.2782; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.18220617274
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.172186459502
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.16)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.461568231984
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.46)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.63572574109
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.585527061569
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-0.0546869272991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.15822174516
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.15)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.05931954879
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 1.08)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.92849389505
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.92)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.5884091977
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.991379413707
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.01)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-38.2144914366
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.40)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.15907941057
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.14)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0270694372705
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded -0.01)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.52384552579
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.50)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.10862855737
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.14)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.489155413943
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.47)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 43
\-------------------------

Simulating trial. . . 
epsilon = 0.2699; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.56864481595
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.3018137053
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.31)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42306720984
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.84736418827
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.49203716761
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11816806094
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.164549123238
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.16)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.952832455411
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.98)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.12740429881
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.13)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07461735565
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.802791874701
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.82)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.63137374827
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.61587185332
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.62)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 44
\-------------------------

Simulating trial. . . 
epsilon = 0.2618; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-18.805816656
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.39)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74663252912
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.23831029779
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.31)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39869601935
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.49494076816
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.06448838878
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.09)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.80658871914
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.19607427841
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.12185676686
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.49244102088
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.53)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.26011020458
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.24)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-37.9967061301
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.734601739253
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.74)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.55077807774
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.55)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.34928315558
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.0946475910754
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded -0.10)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.990146951542
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-10.2831107868
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.60)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-10.2291367732
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.55)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.255402499233
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.26)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-9.01031569924
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.29)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.183233333
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.20)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.35118244105
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.34)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.74179189304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.76)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.08225003278
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 45
\-------------------------

Simulating trial. . . 
epsilon = 0.2539; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.28892841321
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.4763877619
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.26130737482
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.22853002971
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.54460631711
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.53)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.20969299509
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.20)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28656766941
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.425442209799
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.66108800447
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.64341941255
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.09544818024
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.10)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.32853631248
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.39136468311
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.39)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.25640287302
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded -0.30)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.8751054992
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.89)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.598573279772
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.55)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.478894805175
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.45)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.098061251
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.13)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 46
\-------------------------

Simulating trial. . . 
epsilon = 0.2463; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5802213713
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.59)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.63891790059
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58708969709
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.59)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.80537073274
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.88)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.3847369002
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.57472048057
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.57)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.33697877549
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.00600840113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.76624440146
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.33104046665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.993289687771
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.95)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.70351302316
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.74)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.43346620926
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.39)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.55123157865
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.60)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.57446294728
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.08046178418
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.854071536776
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.88)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.49609041648
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.54)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.643869381707
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.61)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.598993616396
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.62)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.60501313769
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.58)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.976085140796
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.214353579551
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded -0.27)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.014567687438
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.01)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.83656363149
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.84)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 47
\-------------------------

Simulating trial. . . 
epsilon = 0.2389; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-5.66680134224
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.52631604628
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.57)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08369989822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35612216969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.809596111808
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.80)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.379642548715
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.35)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-19.2726588332
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.111409258189
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.08)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.52811655077
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.57)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.47848559333
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.20383440889
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.22)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.25741027768
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.03554010651
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.20998568093
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.50433666626
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.52)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.91579837184
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.91)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.29705233757
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.31)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.96430227269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.91190290175
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.95)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.602464398867
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.54)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.17432925886
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.74502713443
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.85336402551
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.86)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 48
\-------------------------

Simulating trial. . . 
epsilon = 0.2318; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.04857608374
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.11)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-19.4217936525
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.02)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.13526286362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.20)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.83896877628
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.996406367766
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.94)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 49
\-------------------------

Simulating trial. . . 
epsilon = 0.2248; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.84220373553
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87688115507
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61669341338
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.46036966793
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11502387838
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.18903006459
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.32887642279
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.30)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.839998517044
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.85)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03803536137
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.01)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.335264585178
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.31)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.27735613632
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.11919274461
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.10)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.00656105997
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-19.8616728749
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.48)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-19.3410440022
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.94)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.00499776520905
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.04)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-38.2125192968
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.39)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-9.74869482656
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.05)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.610165576761
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.57)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.960881181017
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 50
\-------------------------

Simulating trial. . . 
epsilon = 0.2181; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37156145838
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.38)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.58893601127
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.19934005711
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.862489419852
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.489962408531
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.68293061528
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 0.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.00933206222
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.18049519506
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.16)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.45805578766
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.24663055279
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.28)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.54301509925
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.54)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.5549960227
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.56)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.422448818294
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.40)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-38.4702904991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.66)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.722974644329
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.67)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.679155903051
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.63)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.41038500738
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.419077886557
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.37)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.960973953308
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.98)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.262491143202
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.26)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 51
\-------------------------

Simulating trial. . . 
epsilon = 0.2115; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.164122946813
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 0.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.32779546247
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.23928065808
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.64743247628
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6982443331
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.66)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.35874093307
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 52
\-------------------------

Simulating trial. . . 
epsilon = 0.2052; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16286519231
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.16)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.67039538417
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.72)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42222657607
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.6840885802
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.315741485572
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.33)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79109331487
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.78)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.36346401315
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.37)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.63671472083
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.66)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.91786682225
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.72160141202
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.72)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.60542842
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.62)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.25862176844
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.27)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-38.3897882823
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.58)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.27861922648
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 53
\-------------------------

Simulating trial. . . 
epsilon = 0.1990; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.62024339832
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.40331904588
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.3933758419
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.88238123969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.89)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83635098228
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.41580880583
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.41)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.9906982036
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.61589440836
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.2184772491
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.40631481388
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.43)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 54
\-------------------------

Simulating trial. . . 
epsilon = 0.1931; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.38797244142
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.65280234598
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.70)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.59095307522
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.63)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.841521713208
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06298609294
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.03)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.510112091639
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.50)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.77277994125
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.61490571916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.65)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.773300033299
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.80)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.42790870239
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.78317794357
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.63296891267
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-9.91648725334
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.22)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.13967914126
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded -0.14)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.67829459256
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.68)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.15786289039
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.16)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 55
\-------------------------

Simulating trial. . . 
epsilon = 0.1873; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.61792593085
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.72455727766
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71591606051
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.74)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.89778537193
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.15527577003
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.18)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 56
\-------------------------

Simulating trial. . . 
epsilon = 0.1816; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72788023272
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71360738687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.2842868411
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.31)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-4.60742664724
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1530476985
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.14)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.40667555004
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.82028757015
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.09844523114
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.13)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16377937541
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.60158215772
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.60)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.06370494647
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.02)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.00616943842
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.98)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.06087171728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.37247547064
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.36)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-19.7253213461
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.34)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.897525792574
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.87)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.26401635219
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.26)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.26481049189
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.25)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-19.0437707147
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.63)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.854322902883
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.85)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.95095682035
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.93)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.25103288634
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.28)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.3716834794
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded 1.41)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-3.91550722383
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.757734206954
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.73)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.03683907561
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.438799773069
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.43)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.347736314803
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.29)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
1.52869828859
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
0.430689773384
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.40)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 57
\-------------------------

Simulating trial. . . 
epsilon = 0.1762; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.10989111646
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.12)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.804145585046
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.45379847064
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.38216710901
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.18277785109
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.23871292609
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 58
\-------------------------

Simulating trial. . . 
epsilon = 0.1709; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.45758508175
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23490094749
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.00500801017538
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.15890654124
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03115920006
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.72095466036
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.77)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28224345205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.0913722429
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.06)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07473395183
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.05)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.0729040159
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.08)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.1966902409
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.21)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-9.36334705492
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.65)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.23131938682
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.179619220444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.19)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.78197801935
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.77)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.39519925678
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.44)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-38.6948633132
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.89)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.15454255
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.18)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.299674286949
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.25)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.92587960539
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 59
\-------------------------

Simulating trial. . . 
epsilon = 0.1658; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.5228365705
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22027531199
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.22)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.996651604515
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.01191441574
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.67338661063
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.80049905379
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.49612814281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.868962471619
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.09676806152
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.44659654343
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.41)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.02165840812
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.07)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 60
\-------------------------

Simulating trial. . . 
epsilon = 0.1608; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-9.39869874863
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.75658697319
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.2471604039
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-19.4465499718
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.05)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71710803034
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.73)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.47577781504
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.49)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.21878201505
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.26)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.47441317665
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-10.2426165395
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.56)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.705075593
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.72)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.902735684234
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.86)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.55040677611
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-9.03602059997
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.32)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.44638653039
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.04801410759
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.86330197544
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.39569097639
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.39)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 61
\-------------------------

Simulating trial. . . 
epsilon = 0.1560; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-39.471315098
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0947155150789
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.21238594611
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.18539381542
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72225278774
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.89611265423
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.51396548784
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.93837835852
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.14634862227
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.33896878561
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.173989503313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.16)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.321281901306
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.31)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.829050530464
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.77)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.53181735979
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.60)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.226951638887
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of forward. (rewarded -0.28)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.3310611061
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 62
\-------------------------

Simulating trial. . . 
epsilon = 0.1513; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.05444115916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.05)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.10851004925
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61183956507
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.23717771681
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71543524408
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.06056249597
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.5813664105
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.59)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.53235950272
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.55)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-0.0900901119951
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded -0.09)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.80440371624
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.78)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.52386210687
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.56)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.35026891882
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.70666669244
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.74)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.716021030959
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.74)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.05095572062
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.09)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.894685666969
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.92)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.606962245152
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.58)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.125300229976
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.14)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.71676722518
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.71)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.395272542451
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.38)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.932329981454
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.88)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.04082658728
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.03)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.13950035474
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.14)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.523045640295
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.53)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.60962851131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
0.709722985694
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.69)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
2.23233229715
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
1.86190773741
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
2.10194801204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.14)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
1.6354927704
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 63
\-------------------------

Simulating trial. . . 
epsilon = 0.1468; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.38926846261
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.40)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.14742639998
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.50675115795
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.55990854883
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.505365506473
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.05496242344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.80024807486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.967757656007
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.845691756202
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.84)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.851625872337
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.85)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.14550405613
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-19.3703498886
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.97)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.64724817188
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.67)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.25888395001
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.23)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.77606264443
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.80)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.9975459328
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.80808599301
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.83)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.933311725764
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.96)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.530594592362
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.52)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.18414500173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.21)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.136163502726
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.09)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.05828535376
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.05)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.0959622276218
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded -0.15)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.4437932312
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.46)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.73901974754
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.75)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 64
\-------------------------

Simulating trial. . . 
epsilon = 0.1424; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.365183589796
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.32)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.81194108906
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.8334509667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87950254025
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.82748466542
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.85)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.49047361174
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.607035162548
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.59)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.24303468987
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.23)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.84368409247
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.28257943164
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.30)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.942352958849
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.92)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.4287951241
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.47)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.10727618711
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.71464999746
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.71)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.01750641211
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.05)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.729085102098
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.69)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.343139345087
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.36)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.372767480091
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded -0.42)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.938585468335
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.96)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.90622888309
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.86)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 65
\-------------------------

Simulating trial. . . 
epsilon = 0.1381; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.90425867141
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.38285577711
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46405665556
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.25722273218
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.954186484004
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.97)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32028250755
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.16019732676
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.19)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.693071192618
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.71)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26358676684
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.950052463957
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.92)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.317502366177
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-18.6714508776
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.25)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.57568066644
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.63)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.48823551393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.788903686735
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.75)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.570645867861
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.51)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.748794270748
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.71)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.938396619643
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.95)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.823054909316
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.82)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.12373096681
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.09)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 66
\-------------------------

Simulating trial. . . 
epsilon = 0.1339; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.17258110527
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.18)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.23532708844
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.25)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.22059345769
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-19.9352301453
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.741357558
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.33561088656
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.37)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 67
\-------------------------

Simulating trial. . . 
epsilon = 0.1299; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22217836866
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.18)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.47738467893
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.48)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.78739859629
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.62793236218
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.22136147638
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.14207033096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.18)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 68
\-------------------------

Simulating trial. . . 
epsilon = 0.1260; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-5.10996755516
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.27)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.71043588097
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25711095009
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.36833467397
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.37)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.12548683185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.62677873963
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.60)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.71419553703
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.70)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 69
\-------------------------

Simulating trial. . . 
epsilon = 0.1223; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.75348612165
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.56252541855
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.24055882102
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.41800295282
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.42)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.8025108345
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.07449368802
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.0343728186771
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.12610632753
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.15)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.335568295995
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.34)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.416458764918
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.43)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.07378306154
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.46507741872
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.47)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.42065196906
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.44)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.986236812961
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.98)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.3507581209
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.38)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.17395201111
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.17)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.574888989765
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.54)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.14132760424
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.14)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 70
\-------------------------

Simulating trial. . . 
epsilon = 0.1186; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.24855353414
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.58401454183
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.65)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.64550147607
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.36944768242
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.38098514206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.67187724903
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.59976871803
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.60)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.69826631932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.20406419318
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.17416440996
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.919101876713
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.01849537003
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.05)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.67133476434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.53247559596
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 71
\-------------------------

Simulating trial. . . 
epsilon = 0.1150; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.05590891389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-10.2611857539
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.58)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.90764033946
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.63128327049
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.84169884647
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.88)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.47422742152
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.51)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43903227198
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52266870812
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03560265332
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.99)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.4437252411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.41554495136
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.40)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.09520615203
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.05)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-0.209800779243
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove left instead of forward. (rewarded -0.24)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.30788030097
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.33)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.20136507276
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.19)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.05960072651
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.09)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 72
\-------------------------

Simulating trial. . . 
epsilon = 0.1116; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.806556701393
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.32790357131
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59680416443
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.81088879246
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06326774835
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.59872363157
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.74373806378
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.73)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.935407737411
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.91)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-37.8729550807
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.04)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.029136418053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.00)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.53233850076
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.93643048713
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.90)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.68672212802
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.65)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 73
\-------------------------

Simulating trial. . . 
epsilon = 0.1082; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.26625002856
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.72323658084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02971549373
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33778838562
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.42970586805
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72744047145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73899080594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.78)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.49994380553
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.79037700661
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.36837055862
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.131658133373
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.13)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.74451895756
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.73)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.95100502411
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.873896413334
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.86)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.767764254576
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.71)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-37.8386317469
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.01)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.31232044711
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.35)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.39145412701
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of right. (rewarded 1.43)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.967667745056
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.95)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.62440842132
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.62)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.40055377469
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.39)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.69285658977
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.964635398492
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.97)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 74
\-------------------------

Simulating trial. . . 
epsilon = 0.1050; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.444182039067
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.41)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.47538264123
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.50)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94029451442
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.52709966233
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.51)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.568894222
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46232176973
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.346204076
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.34)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.03826504606
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.02)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-10.357105592
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.68)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03151249752
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.03)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.14523298103
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.12)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.962551704912
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.93)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.66027160513
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded 1.67)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.72491136126
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.72)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.63558687174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.3152748094
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.70636509833
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.147967603139
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.12)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.39536202624
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.42)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.8343169865
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.81)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.674582254121
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.63)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.12402422776
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.11)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.09914405028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.10)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.52102203442
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.49)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.427647415765
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 75
\-------------------------

Simulating trial. . . 
epsilon = 0.1018; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.79312172107
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.801723865259
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54637429852
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.43906554634
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.67317550112
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.75)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.65944867704
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.67)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.76802380911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.77)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.25988784646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.24)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26063491576
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.66685800632
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.876493936496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.87)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 76
\-------------------------

Simulating trial. . . 
epsilon = 0.0988; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0415082775102
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.04)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.937799105699
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31214204482
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.62063858487
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.03064166996
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.26503737505
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.10261976463
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.675101852191
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.54812325347
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.56)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.73441792183
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.75)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.18409711694
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.21)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.14937665045
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.11)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.72815031076
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 77
\-------------------------

Simulating trial. . . 
epsilon = 0.0958; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.11181404581
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.16420229945
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.20)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.41200712806
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02897780972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.16613918794
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.31251394039
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65291028196
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.24924964332
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.22)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 78
\-------------------------

Simulating trial. . . 
epsilon = 0.0929; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.774788004986
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.80)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.31893212241
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.29)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.97334710399
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.97)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.06082043944
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.52066276629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.53)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.26722807979
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13781385727
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.909652592615
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.91)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.08717212403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.10)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.63407999384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.67)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.8076570539
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.82)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 79
\-------------------------

Simulating trial. . . 
epsilon = 0.0902; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.810289983155
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74905611333
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.74)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-4.68934833808
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.83)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.04415144106
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.03)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.108616732333
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.11)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-39.2251391379
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.44)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.39941293233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.93604234894
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44976104847
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.43)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.1338686182
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.18044587551
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.22)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.88576596428
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.92)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.25866429402
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.27)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.32645816429
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.7183250675
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.61209427543
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.63)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.984934386309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.94)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.54697390917
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 80
\-------------------------

Simulating trial. . . 
epsilon = 0.0874; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.8086461301
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.86)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-10.6607594958
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-9.10729825823
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.76842051961
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.64904042136
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.16897567428
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.18)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.24731908901
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.24)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-0.0825994749775
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent drove left instead of right. (rewarded -0.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.16695507314
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.14)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.801637869982
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.80)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.18053620064
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.19)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.01988808279
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.98)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.15434937576
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.829427328406
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.83)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 81
\-------------------------

Simulating trial. . . 
epsilon = 0.0848; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.88551282301
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.93)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87033132973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.24066761801
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.51213437191
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84386819392
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.90)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.54619729392
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.68338212769
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.68)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.18267145066
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 82
\-------------------------

Simulating trial. . . 
epsilon = 0.0823; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.19473380669
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-39.6049550988
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.83)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.81238391228
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-10.2202890255
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.61413034326
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.03642674779
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.03)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.857743538922
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.85)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.81655269925
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.86)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.21154405416
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.41920935632
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.04570351367
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.09214934204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.95559266391
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.81086783205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 83
\-------------------------

Simulating trial. . . 
epsilon = 0.0798; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.86316388118
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.27987154105
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.68555320214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11479267747
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.61358253665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.65546886757
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.675621350125
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.66)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.93675781894
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.06349490483
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 84
\-------------------------

Simulating trial. . . 
epsilon = 0.0774; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.28842504736
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.50539888512
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17158989183
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.76377338208
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.7463050547
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27090873488
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13162987686
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.13)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.895110902874
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.64876515835
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.67114787153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42028769195
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 85
\-------------------------

Simulating trial. . . 
epsilon = 0.0751; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.45023647972
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.42)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.17750605481
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.18)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.2224986912
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.22)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.73048107681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33295719287
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.32)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64069029036
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.66)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.41373258331
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.13349301594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.84522273845
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.87)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.60800649905
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
71% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 86
\-------------------------

Simulating trial. . . 
epsilon = 0.0728; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.46629145391
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.09107828388
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.67194573296
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.76393399433
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.99677834441
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.68920814208
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.49317955444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.70960575667
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.665369988648
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.34434790553
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.656275249963
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.67)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53558454977
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.50)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.71587895572
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.73)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.03216566177
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.03)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.71715118017
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.575360591298
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.54)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.39382286514
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.37)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-18.7922017336
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.37)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.44679994224
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.42)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.4639376373
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.43226764433
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.40)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.422158070959
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.41)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.943049987475
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.94)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.46336262836
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.49)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 87
\-------------------------

Simulating trial. . . 
epsilon = 0.0707; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.17579132941
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15838071913
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.41980409136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01801755214
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.25449563034
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.26)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.26068220226
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.464903023
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.44771134897
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64059903292
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.61)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 88
\-------------------------

Simulating trial. . . 
epsilon = 0.0685; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.15869007807
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23679672903
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.37488952752
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.40)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02530626288
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.47536937644
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.28986078495
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.46286026998
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.28491509452
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.31)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.02531933369
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.06)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.903186014174
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.11533385197
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.18732590134
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.42364934052
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0764050255605
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.134339133042
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.12)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.866461387152
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.89)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.3195378035
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.33)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.21862632398
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.21648235748
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.22)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.76349867084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 89
\-------------------------

Simulating trial. . . 
epsilon = 0.0665; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.79880435521
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.60949680134
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.67026051814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.65)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.0959420335814
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.10)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.71399155949
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.73)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.37797096463
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.961009840479
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.19870117556
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.20)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3479411062
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.83123404975
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.3926635989
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.38)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.333613412513
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.32)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-10.6131620488
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.94)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.940085110769
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.90)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 90
\-------------------------

Simulating trial. . . 
epsilon = 0.0645; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.78757517727
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.646717970238
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.688447993179
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.69)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.97686408323
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33921030001
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80081286797
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.84)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 91
\-------------------------

Simulating trial. . . 
epsilon = 0.0626; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-5.41888508586
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.59)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.1004535575
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.08)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.6910814432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.72)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.6931307553
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.73498596962
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.77)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.028965821
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.67159580301
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.67)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.97483403775
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.04)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.56510067618
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.59)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.4987576966
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.19514953567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.21)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.09114450177
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.08)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.09841215454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.12)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.24958204622
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.38196473804
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.90951898335
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.19238904822
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.45839035504
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of right. (rewarded 1.46)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.83963234285
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.78793825875
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.638928732083
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.64)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.47928893369
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.50)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.27623435684
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.29)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.49364793302
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.48)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
-38.0231151416
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.20)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.82307907504
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.84)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
-38.4810022097
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.67)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
1.42778019194
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.43)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
0.830479354983
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.79)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
-0.655286708478
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.68)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 92
\-------------------------

Simulating trial. . . 
epsilon = 0.0607; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.19207792734
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.23)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.205969980639
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.16)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.86362444261
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.88)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02141626604
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.03)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.28995253778
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.27)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.87542865731
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.91)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.643179726028
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.63)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.14653591657
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.56044934644
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.58)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 93
\-------------------------

Simulating trial. . . 
epsilon = 0.0589; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.99438208078
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.06)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.38493045006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.36301311247
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.07564406716
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.5216980344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11009365375
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.14)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62619869054
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.68)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.12330516625
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.16)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.0348381121601
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.01)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.19764820139
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0420471352366
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.31394194154
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.29)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 94
\-------------------------

Simulating trial. . . 
epsilon = 0.0571; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.287290003192
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71200718434
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42952492671
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.43)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.8916547246
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.897679198563
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.91)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.23082999657
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.18)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.54990177063
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.56)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.603278849
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.60)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71259282159
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.76)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-18.7182444728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.30)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.54714753299
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.40670087471
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.1776276534
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.19)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.485722920233
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.46)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.62040630623
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.58)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.15301065921
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.15)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.11426810712
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-18.8052143466
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.39)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-0.0850816567876
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove left instead of right. (rewarded -0.13)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.315205225611
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent drove forward instead of right. (rewarded -0.32)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 95
\-------------------------

Simulating trial. . . 
epsilon = 0.0554; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.42126510741
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.38)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.17492673718
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.149894969
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.52829142018
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.3277820074
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.32)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.36027559112
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73400215379
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.73)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.51500147774
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.54)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.68042445926
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.73)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.0498944802077
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.01)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.28266676045
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.32)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.330626768498
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.32)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.900848851304
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.92)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-4.58607544907
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.73)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.47822688482
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.51)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.59828297701
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.59)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.00728984903
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.97)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.02651201465
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.05)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.29233095226
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.29)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.706941377207
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.69)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.18440610807
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.25)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.30565456086
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.30)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.643587279198
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.58)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.99681064063
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.97)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.08264448749
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.07)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
0.528622118795
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.54)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.000463994043454
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.00)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
-0.0617369368881
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded -0.09)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
0.789850088897
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.81)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
1.59301977463
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.56)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 96
\-------------------------

Simulating trial. . . 
epsilon = 0.0537; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.716323118045
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.70)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.8039594066
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.89)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.12237095614
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.12)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.07948442962
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.90024136973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.93)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.78693029136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.7683106545
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.45549885055
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.17707847988
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.23542425751
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-10.5157601053
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.84)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.27878042914
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.32)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.7223871938
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.35129941854
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.34)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.03879325893
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.03)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.0466337071612
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.07)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.902609778654
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.86)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.44495044594
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.44)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.50674564005
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.55)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.51796724115
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.53)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.654734877165
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.63)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.42885875976
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.45)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.61001500278
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.61)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.850812650443
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.85)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.99818306927
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.99)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
-0.361352125825
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded -0.40)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
1.7288522911
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.74)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.42093855764
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.35)
7% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 97
\-------------------------

Simulating trial. . . 
epsilon = 0.0521; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.31469752121
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.85700731006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.85)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.39402336434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.999857131159
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.99)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.68108051909
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.66)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.44493478303
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.42)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 98
\-------------------------

Simulating trial. . . 
epsilon = 0.0505; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.10531468186
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.12)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.823147821901
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.05366142958
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.05)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79399713322
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.79942326807
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75752521245
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.77)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.79471206724
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37229938477
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 1.40)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.66564441901
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.06586564488
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.06)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4431952158
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.47560784464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.63022849903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.215594664083
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded -0.22)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.1310349704
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.14)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.7536748949
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.77)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 99
\-------------------------

Simulating trial. . . 
epsilon = 0.0490; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16751401535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.22110237239
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80581642038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.56115930936
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.97080826982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.28258147582
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-38.7954508085
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.00)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.29038009884
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.31)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.49045115216
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.52)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.19270649474
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.75541019214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.36484895059
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.70436868674
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.50829781832
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-39.4710240616
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.69)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.783060409815
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded 0.76)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.96666683599
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.97)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.12408208598
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.11)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.43765184177
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.45)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.819823926468
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.79)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.22463365974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.59399353744
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.61)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.8700914013
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.86)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 100
\-------------------------

Simulating trial. . . 
epsilon = 0.0476; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5634021372
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07249472843
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43241727631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.66169560032
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.63009519998
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.68)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-5.1676565044
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.31398551218
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 101
\-------------------------

Simulating trial. . . 
epsilon = 0.0461; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.7546289854
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23350195804
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46442530018
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.57339739928
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.56097576854
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.59)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.6568785083
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.985905267885
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.82177449961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.86)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.17878253539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03359710367
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.3502459966
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.90228257266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 102
\-------------------------

Simulating trial. . . 
epsilon = 0.0447; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.48772572022
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.56)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70934800245
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.82680155972
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.87)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72190085013
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.977572966637
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.95)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.87656495547
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.87)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62212244055
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.20809791425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.51902450588
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.2181874779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.798367070816
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.78)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.92171323885
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.96)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.22583008423
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.32907227327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 103
\-------------------------

Simulating trial. . . 
epsilon = 0.0434; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.87243583966
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.79223714048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.83)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.03822405895
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35212779232
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.81452032813
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.12429451015
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.13)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.1583853142
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.14)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 104
\-------------------------

Simulating trial. . . 
epsilon = 0.0421; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.63468502959
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.05241522038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.3260895098
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.587676470145
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.57)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.85528502976
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8445781775
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.66717392991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.55811191338
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.59)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71915644442
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.77)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.973752091832
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.92)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.51586534546
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.54)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.151714639009
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.14)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.435700110758
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.39)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.68090693367
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.68)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.229505418091
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.21)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.79833988797
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.82)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.500293750096
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded -0.52)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.384321670401
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded -0.40)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.40903457409
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.37)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.672152120239
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.66)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 105
\-------------------------

Simulating trial. . . 
epsilon = 0.0408; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.44941709619
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.49)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.63636059988
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.66)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.7267752366
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.64575521837
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.96332530953
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.97)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.80473649657
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.78)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43364949661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.44)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.98075465396
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.36182124384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.65485719372
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.316363879889
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.31)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.57329069648
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.75226552307
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.79)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.13350360003
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent drove right instead of left. (rewarded -0.16)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.23872037354
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.24)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-38.8303161198
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.03)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.920247104508
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.95)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-4.21328589949
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.11899894554
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.15)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.790119976015
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.79)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.79319718869
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.47124688976
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.46)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.67764047353
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.69)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.763665414
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.76)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.49430307377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.48)
17% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 106
\-------------------------

Simulating trial. . . 
epsilon = 0.0396; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65290038168
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.628656156093
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.62)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.07016787523
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.05)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.52290520071
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.14147657081
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.15)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.86191727389
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24408467208
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.30)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 107
\-------------------------

Simulating trial. . . 
epsilon = 0.0384; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.59879205813
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.48117165632
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.862463431444
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.86)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20219646661
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.39934864961
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.44)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.22681894586
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.23)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.982501689934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.99)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.0822095955321
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.05)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3224776036
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.31)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.81677160201
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56726907536
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.57)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.08177629884
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 108
\-------------------------

Simulating trial. . . 
epsilon = 0.0373; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0669310969934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.07)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.14042637744
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.20378615626
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.25533828686
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17994970783
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.20445493358
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.09522036251
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.07029453776
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.08)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.74099986121
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.76)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.03204536848
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.21809459015
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.21)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 109
\-------------------------

Simulating trial. . . 
epsilon = 0.0362; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.74771957186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.73)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.21635695172
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.65404820106
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.5911349908
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.94007396687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07904578857
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52499077966
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.68398564852
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.10937920572
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.10822629352
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.10)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.476286011472
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.45)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.95480749249
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.48998413861
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.48)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 110
\-------------------------

Simulating trial. . . 
epsilon = 0.0351; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23802590582
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-4.36861246516
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51053944171
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33666111073
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.07744491902
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.09)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 111
\-------------------------

Simulating trial. . . 
epsilon = 0.0340; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.72494092114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.76)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95592056949
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.98)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.38751306556
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.35)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.68878986622
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.71)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.76332544417
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.75)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-19.8715164912
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.49)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62542645491
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.64)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.987574545404
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.94)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-3.93056982747
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.05)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.66959804348
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.71)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.04166960589
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.99)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.17274518181
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.95574310882
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.40929791728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.44)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 112
\-------------------------

Simulating trial. . . 
epsilon = 0.0330; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.89853765434
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.92)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.71039507569
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.2205566232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.56185598403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33534479211
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.54209350258
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.57)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.23494356145
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.23)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.46271198924
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.46)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-39.4135306126
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.63)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.79227466908
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.82)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.63357411899
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.68)
69% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 113
\-------------------------

Simulating trial. . . 
epsilon = 0.0320; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.88695389314
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.88)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.84012389911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94318547195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.72388013687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.37070027359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.36723622582
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.240395109766
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.25)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.03068959371
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.09352911823
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.42382983632
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.11183708757
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.10)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.61958122797
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.59)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.04691222252
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.81449974689
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.79)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.28374032716
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.27)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.804502412565
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.79)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.11021415206
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.53513017884
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.58)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-18.4627249391
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.03)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.4059317834
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.40)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 114
\-------------------------

Simulating trial. . . 
epsilon = 0.0310; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.03910012498
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26493637664
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.990100167015
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.730536592294
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53734414752
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.602772245967
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.57)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-4.49786713681
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.64)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.774860867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.78)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.67969216274
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.512344763612
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.52)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.36428231179
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.35)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.772349509202
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.73)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.153449105473
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.11)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.72235244767
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.70)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.88596227588
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.89)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-5.29638149113
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.46)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.15027238092
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.19)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.47631906707
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.71638164883
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.72)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.03797007983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 115
\-------------------------

Simulating trial. . . 
epsilon = 0.0301; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.59449965965
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.64)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.86041725383
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31604935643
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.29)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.80775209712
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.80)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17025213433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.83307836601
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.32564238202
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.32)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08242567428
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.08)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 116
\-------------------------

Simulating trial. . . 
epsilon = 0.0292; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.8765446822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.4915022784
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.54433249307
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.98845086059
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.00)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17861263226
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.05914792349
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.04734910973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.23911656477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.56349466487
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.58)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.216029431163
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.17)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.48681123075
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.08829142248
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.09)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.36100596268
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.38)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.01989820245
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.04)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.930015031918
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.24738315596
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 117
\-------------------------

Simulating trial. . . 
epsilon = 0.0283; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65982979763
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.55432169304
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.32513300114
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.50870900708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.59)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.09907123485
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.73844454767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 118
\-------------------------

Simulating trial. . . 
epsilon = 0.0275; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.57598626528
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.47852086603
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.49)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.38260534835
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.44640046544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.45)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51880148868
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51496835149
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.3102076025
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.438564301672
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.45)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.583515834342
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.57)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.75701233239
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.76)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.67932330565
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.18682253921
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.20)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.23829631732
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.25)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.09433207042
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.09)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.42285852833
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.42)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 119
\-------------------------

Simulating trial. . . 
epsilon = 0.0267; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.34621210152
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-4.1801150526
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.31)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.7911692446
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.11455924513
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.01902185934
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.05)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.45795471837
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.46)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.967048254003
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.95118212095
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.97)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 120
\-------------------------

Simulating trial. . . 
epsilon = 0.0259; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.681962117042
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.68)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.56115800748
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.59)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.12598530318
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.44813091459
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.49)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0326026437
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.10)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.79607016324
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.82)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.27436080066
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.950794810653
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.87016506413
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.87)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 121
\-------------------------

Simulating trial. . . 
epsilon = 0.0251; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.292651848746
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.29)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.89920974876
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.47291232882
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.67841052978
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.64)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83021366864
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.83)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.42634435461
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.51650886992
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.49)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.84590576221
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 122
\-------------------------

Simulating trial. . . 
epsilon = 0.0243; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.29881439062
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.66278746299
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.754502659
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.27679845649
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.70162147866
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.225061389378
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00923413112
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.00)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.13104408374
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.17)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.402348286492
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.41)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.66835719146
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.12661836431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.11467627649
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.09)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.56914005377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.19734862477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.147226919625
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.14)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.44217715052
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.49)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.32760463455
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.33)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.37111964513
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.39)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.943260510285
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.93)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.33853187378
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.31)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.73549478317
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.75)
16% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 123
\-------------------------

Simulating trial. . . 
epsilon = 0.0236; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.58033680476
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.83285053543
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.48783718074
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36740868667
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.33)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.46652757418
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.339692362091
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.32)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.12022525224
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.96407366647
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.97)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.19601086483
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.00955700137
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.03)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.72362725473
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.059202867
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.03)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.2449613999
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.40184646732
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 124
\-------------------------

Simulating trial. . . 
epsilon = 0.0229; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.01748153509
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2008226036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.57994775401
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.70118360957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.48402018769
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.01932986982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.55255853131
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.18695661904
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.19)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 125
\-------------------------

Simulating trial. . . 
epsilon = 0.0222; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.812500761888
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67197669949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.71)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54345607779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.14442364611
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.17)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.18018594725
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.13)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.49012408028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.69804554057
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.32678154431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.28)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.6012319276
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.61)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38841358042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.09421136976
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.12)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.10879238114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.09)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.27483304176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.30)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.830946771588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.81)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-9.59480057924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.89)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.99617572363
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.649019456154
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.64)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.0863530659612
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded -0.07)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.31994249824
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.34)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.903796468805
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.85)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.29929246585
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.30)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.20252111698
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 126
\-------------------------

Simulating trial. . . 
epsilon = 0.0215; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.67306571194
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.77360788319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54524230586
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.31786005325
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.28)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84312672434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.09629127142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.12)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.7036878806
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.74796874328
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.4100629522
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.99356497837
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.4277652678
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.44)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-18.7873957544
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.37)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.06164590549
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.01)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 127
\-------------------------

Simulating trial. . . 
epsilon = 0.0209; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.11030031143
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.18)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35990621273
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.34)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.45780997962
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.96976470762
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11090108917
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.11)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.48008379079
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.46)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65283092193
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.71)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.881055831571
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.87)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-9.82866226036
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.13)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.23644876308
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.963871515863
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 128
\-------------------------

Simulating trial. . . 
epsilon = 0.0203; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.26824680057
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.13288157267
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.11)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.32859645918
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.76042305975
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.18894269518
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.17)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.57150238117
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.56)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.957497991335
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.2063760678
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.96729739344
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-38.8478704661
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.05)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.147129361574
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.10)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.26269714075
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.10763822682
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.09128681118
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 129
\-------------------------

Simulating trial. . . 
epsilon = 0.0197; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.88645351892
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.93)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.90783243176
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.90)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46676600178
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.00345337024
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.01)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.15555038988
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.13)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.91378425646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.7546727264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.581226815714
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.57)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.17543548841
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.21)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.34720918362
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.32)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42224360211
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.72598733267
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.74)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.09205165883
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.52015324507
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.42171034812
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.68818161419
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-19.0192239512
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.61)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.71245732606
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.73)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 130
\-------------------------

Simulating trial. . . 
epsilon = 0.0191; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22056227255
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.85267697059
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.91)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.08226727665
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.08)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.78940641037
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44730284944
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.50746645511
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.00197618956
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.04)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.29552887115
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.32)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.17488793431
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 131
\-------------------------

Simulating trial. . . 
epsilon = 0.0185; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.06258782992
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25857047232
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.37958787442
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.14932123067
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.27670174933
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.15300420136
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.14)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.163676267
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.05640204465
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 132
\-------------------------

Simulating trial. . . 
epsilon = 0.0179; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.7576084331
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.84041613513
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.62849609299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.73570591778
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.68576664892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.59266311912
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.1416837316
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.09)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.974420684468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.97)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.75072384481
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.10211077281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.74773903352
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.73)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.15640051001
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.72093210498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.76)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.95741114506
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.96)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.93598363469
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.96993793327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.99)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.35003064418
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 133
\-------------------------

Simulating trial. . . 
epsilon = 0.0174; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-18.7402568378
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.32)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.98946868578
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.55185891926
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.29567422711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.43668158885
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08969838851
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.86656016468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.05556082021
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.122601716146
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.13)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.22641754532
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.22)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.68357534245
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.71)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.64466345993
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.66)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.73334852414
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.75)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.2965198156
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.28)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-9.66460423424
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.96)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.872441100776
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.84)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.10649108734
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.06)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.293020526212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded -0.31)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.15894647761
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.940091305505
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.93)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.36147527321
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.38)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.23824853579
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.25)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.48635964397
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.48)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.65187268719
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.62)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.61419810166
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.63)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 134
\-------------------------

Simulating trial. . . 
epsilon = 0.0169; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.96335222635
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.38088110971
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.61264464123
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.512032848089
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.50)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.29698403162
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66239075947
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75445648174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.00003142431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.95)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.47353836764
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.51)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.73157811861
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.79)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.44788524128
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.49)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.07273405104
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.10)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.24964138074
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.25)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 135
\-------------------------

Simulating trial. . . 
epsilon = 0.0164; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.685286096517
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.71)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-39.055890027
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66492804743
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.65977704526
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.71)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51859251621
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.57)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-38.4987043318
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.69)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.23567913667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.19325666787
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.92582177973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.35208234594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.966641991758
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.98)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.35940081464
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.36)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.10950714835
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.11)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.49064364577
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.49)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.716248982596
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.74)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 136
\-------------------------

Simulating trial. . . 
epsilon = 0.0159; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.18502049339
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-5.21651263453
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61474887967
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.46395282873
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.39126053573
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.15583444141
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.17)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 137
\-------------------------

Simulating trial. . . 
epsilon = 0.0154; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68037323571
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.83793763497
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.11005549711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.25540515043
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.05394585939
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.97016852817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.98)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 138
\-------------------------

Simulating trial. . . 
epsilon = 0.0149; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.9579251609
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.97)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.2960637819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4254822676
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.52536297646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.50)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34962849754
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.380947911415
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.35)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.93442471292
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.0063269207
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.04)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.80194387674
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.14191275263
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.21)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.30142395982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.78152351905
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.80)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.34915596785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.38)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.53498245559
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.183690909131
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.22)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.792831635248
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.73)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.43373198425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.358257733233
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.33)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.6629599672
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.35810217145
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.33)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.865888972511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.84)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.874386832522
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.85)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.04955552322
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.04)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.60238469565
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.65)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.460659789249
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.45)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 139
\-------------------------

Simulating trial. . . 
epsilon = 0.0145; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.568667482515
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 0.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24498948912
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.09753167316
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.18380625646
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44775799865
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07593620809
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.21902723165
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.22)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.3293385459
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.46147294986
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.327512664079
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.34)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.00167406106
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.00)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.4705396358
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.50)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.32131583741
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.31)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.37233800924
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.36)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.77410486395
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.78)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.34594065613
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.35)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 140
\-------------------------

Simulating trial. . . 
epsilon = 0.0141; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-5.08140862535
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.45101795366
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.74078251006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.65532140327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-38.3920074818
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.33964679183
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.80956762173
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.85)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 141
\-------------------------

Simulating trial. . . 
epsilon = 0.0136; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.17192935563
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.82199135778
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.91)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.30449041407
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.30)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.20867773432
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83141931293
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.15968589113
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.15)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.962254433929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.81512034393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.34691546867
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.35)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.509314178856
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.68149025302
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.1800437992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.03459869924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.11781555885
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.13)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.02390864114
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.74921412807
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.35492267753
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.36)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.53318009476
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.54)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 142
\-------------------------

Simulating trial. . . 
epsilon = 0.0132; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.84735556187
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.86)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.35151368921
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.82296006257
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88013552897
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.89)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 143
\-------------------------

Simulating trial. . . 
epsilon = 0.0128; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.03993622928
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.23144201417
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.28520272866
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.29)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.65120462021
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72613702853
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.69361122798
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.70)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.40336948719
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.18911183914
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.6362836845
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.08436435647
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.06)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 144
\-------------------------

Simulating trial. . . 
epsilon = 0.0124; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54838415138
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.57)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.7627633922
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.77)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.44930417755
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.45)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72318642975
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.03036961025
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09009142548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00472667853
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.98)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.69802648168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71053069617
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.60297527266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.58)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.03445213187
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.06)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.66129392553
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.66)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 145
\-------------------------

Simulating trial. . . 
epsilon = 0.0121; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.94628186512
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.97)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.93610108264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.94)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.39769073179
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.45)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.32818558745
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.21707746581
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.4104054399
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.44)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.11793864279
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.14)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.10810114401
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.14)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.100657089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.20303760496
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.19)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.32362813876
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.35)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.45723601472
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.47)
66% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 146
\-------------------------

Simulating trial. . . 
epsilon = 0.0117; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.76544013299
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-10.3614939151
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.918160951801
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.33676152659
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.2956048675
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.23374959551
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.68758347728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.996843018344
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.07224845581
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.48443152325
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.49)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.29830619576
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.12536625203
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.1701199743
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.75289129501
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.73)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.49440222779
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.9723177796
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.00)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.02448396222
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.57373009183
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.52)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 147
\-------------------------

Simulating trial. . . 
epsilon = 0.0114; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.76561196175
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.82)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.18204513565
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.51336955778
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.18246695491
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.42753802564
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.84362448679
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.968085284046
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.07132890633
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.46499822872
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38425586313
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.53616526272
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 1.55)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.03724463282
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.01)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.93583889238
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.93)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 148
\-------------------------

Simulating trial. . . 
epsilon = 0.0110; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.00589710211
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.718458273
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.74118742553
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.74)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79540336142
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.15956728857
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.04300057758
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.15823056888
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.13)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.43891269458
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.88235241554
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.89)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.580863456675
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.66691480609
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.6421107446
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.96511759123
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.417626848609
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.40)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.245756535634
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.30)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.967872503552
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.93)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.06899059981
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.05)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.25746590091
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.27)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.28149411841
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.27)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.293277195736
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.22)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 149
\-------------------------

Simulating trial. . . 
epsilon = 0.0107; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23561788519
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.27)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.59300215058
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.45061459269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87667427346
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.62686006282
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.18440306635
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.53023420921
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.23084897184
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.25)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.24128158136
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.20)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 150
\-------------------------

Simulating trial. . . 
epsilon = 0.0104; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.2857504164
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.29)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.68823991642
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.3568571298
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.32632420109
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0748530407
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.12)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.35812926739
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.32)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 151
\-------------------------

Simulating trial. . . 
epsilon = 0.0101; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.317149101835
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.27)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35841928526
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.39)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.49759873936
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.51)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58627807357
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.57)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44763403946
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09386175608
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.09)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-39.006014
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.21)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.25650316837
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.09311065244
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.08)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.63328220991
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 152
\-------------------------

Simulating trial. . . 
epsilon = 0.0098; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.89446817344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.87)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.20014507984
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.18)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.0133707980735
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.00)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.8805787961
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.91)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.70028411577
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.23235136106
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.9817626631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.01)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.23674920941
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 153
\-------------------------

Simulating trial. . . 
epsilon = 0.0095; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.319681870935
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.31)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.60915595168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.61)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.47334258556
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.57960154692
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.477220995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.47)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30742743375
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.37)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.03053834283
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.8143473487
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.86)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.38888488836
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.43)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.42872743805
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.44)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-9.29435843756
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.58)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.13274820402
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.11)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-9.94904512213
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.26)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.839891576046
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.83)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.0375479114962
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.08)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.24424246475
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.28)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.56539360951
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.56)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.45168378581
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.41874456618
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.42)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.90580286377
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.94)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 154
\-------------------------

Simulating trial. . . 
epsilon = 0.0092; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.472144044044
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.49)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.84534189
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.82)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.21003255393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.77151328128
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.74)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.96365149048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.86947329364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.88)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.54889387535
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.365126215116
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.37)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.39375412544
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.488800270727
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.47)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.60432230641
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.61)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.35816808067
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.39977929521
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.57748142917
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.58)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.578915965873
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.55)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.51616839306
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.53)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.3315919586
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.35)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.741921676784
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.68)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 155
\-------------------------

Simulating trial. . . 
epsilon = 0.0089; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37165969973
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83835901663
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.25096686873
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.76553510906
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.88675223236
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.55053159149
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.60)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.136152194543
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.09)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.60061226931
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.62)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 156
\-------------------------

Simulating trial. . . 
epsilon = 0.0086; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-10.6537640164
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.73785614607
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.31215163745
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.94675356998
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.96600504019
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.81610488186
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.987205767166
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.95)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.11707846568
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.11)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.94825534578
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.94)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.43943605199
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.45)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 157
\-------------------------

Simulating trial. . . 
epsilon = 0.0084; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.39441193808
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24622013105
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.98391020029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.92118015988
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.91)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.54217492326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.71161671678
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.71)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23430286174
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.25)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.51751199184
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.50)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.937757169073
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.14968315244
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.1902079765
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.981417623435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.95)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.47160207584
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.50)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 158
\-------------------------

Simulating trial. . . 
epsilon = 0.0081; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.28644931763
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.29)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07792042039
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.10089126004
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35490261478
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.83532404228
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.86)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.85782877442
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.35800487828
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.71887456478
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.77)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.21490251383
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.10145083319
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.66730405592
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.70)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.47516127159
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.48)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 159
\-------------------------

Simulating trial. . . 
epsilon = 0.0079; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22069684221
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.52673125159
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.57233815137
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.62)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5865495912
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.56)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.10196289174
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.12)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.022495357376
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.04)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.52533131928
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.56)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.15042107227
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.16)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.69690192424
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.25918977992
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.21)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 160
\-------------------------

Simulating trial. . . 
epsilon = 0.0076; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.883661135607
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67784053033
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.56826043964
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.60)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.44392082994
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11706524951
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.15)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.40064150501
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.32281526894
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.32)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.43746774449
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71497377813
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.63055016979
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.15501237164
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.418597363
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.30259863058
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.28)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 161
\-------------------------

Simulating trial. . . 
epsilon = 0.0074; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.338269003875
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.54519665031
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.53)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.05680542984
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.49122741386
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.52)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.13744891497
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.38237781759
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.42)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.234325340868
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.19)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88911646052
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06198472565
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07840102699
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.92405514166
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.95)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.11585346251
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.12)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.89688893478
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.88)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.04320640881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.88273466163
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.88)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.36194899441
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.590355143386
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.57)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.439852019273
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.46)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.408264589958
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.40)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.935356446056
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 162
\-------------------------

Simulating trial. . . 
epsilon = 0.0072; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68181489463
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.73)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-9.71527328517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.02)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.27270096985
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.19316463714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.977284566957
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.77785863013
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.80000676488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.89)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.4557141629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.49)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.34731545994
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.41)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.15840231568
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.13)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 163
\-------------------------

Simulating trial. . . 
epsilon = 0.0070; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.34957176102
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.42)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70960334344
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.68)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03433723568
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.37312516761
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.37)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.70483467993
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.12561782643
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.11)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 164
\-------------------------

Simulating trial. . . 
epsilon = 0.0068; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.15042043024
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40607572287
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.10151022883
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.09648689518
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.12)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.31751636272
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.35)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.16904604993
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.96404985328
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.26376392684
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.32241000469
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.466002793327
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.44)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.963975632457
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.96)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.02472683212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.03)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.20550159464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.00246693533
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.23347739214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.23)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.825512115252
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.78)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.319047200428
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.34)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.24704111384
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.23)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.856460942271
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.81)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.177070617247
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 0.14)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 165
\-------------------------

Simulating trial. . . 
epsilon = 0.0066; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.413592308492
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.42)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.14129894129
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49485740239
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.71902292772
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.60707167071
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0214495826
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.99)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.04210277347
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.689142748855
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.69)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.91948754537
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.21530791879
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.21)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.26580747921
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.31291373892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.820635818429
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.78)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.23406370365
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 166
\-------------------------

Simulating trial. . . 
epsilon = 0.0064; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.60292181985
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0183665714365
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.01)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.3558126741
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.36)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.75916290596
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.73)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44285628779
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.48)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.45597170434
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.17841673277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.18)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.54672722053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.14678181293
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.14355587794
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.13)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.21664713628
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.21)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.40062924217
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.83117993763
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.841801180047
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.81)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 167
\-------------------------

Simulating trial. . . 
epsilon = 0.0062; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.10612869388
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81372040079
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.14091781655
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.16)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.83014481006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.87067080255
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.7309055065
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 168
\-------------------------

Simulating trial. . . 
epsilon = 0.0060; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.55640045395
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.58)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24291078722
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25390621882
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.23)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.41465640506
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.13622208359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.32402356234
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.78405434108
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.43449159542
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.12796755158
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.8067826857
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.82)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.839017513373
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.83)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.49347595217
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.46)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 169
\-------------------------

Simulating trial. . . 
epsilon = 0.0058; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.796907917299
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15109014928
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.18709937935
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.22)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.46440168051
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.44)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.07411479817
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.08)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.87088740837
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.89)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.8105016454
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 170
\-------------------------

Simulating trial. . . 
epsilon = 0.0056; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.23078437801
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.21)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26395130472
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.25)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.498180396982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.51)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93408403994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.96)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06558732072
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.05197346563
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.08)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23595992422
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.55600671078
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.74693130952
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.601869169707
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.59)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.83908994573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.86)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.33179078212
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.647008992359
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.66)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.41104501311
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.43)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.962112016472
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.95)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.71336032234
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.72)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.53358493441
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.52)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.50158981216
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.828243696373
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.78)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.300968556
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.30)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.08530235085
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.12)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.11477701841
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.09)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.05773050461
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.94830107414
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.00981547909
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.99)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.91584304877
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.91)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.723519163114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.68)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
1.87857326618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.85)
7% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 171
\-------------------------

Simulating trial. . . 
epsilon = 0.0055; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.262334688504
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.00924208156
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.02)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.77163738429
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.82)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01812418097
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.66141735065
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07877753687
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.05)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.381764074801
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.35)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.68131040682
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.69)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-0.104320059023
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded -0.14)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.953722914572
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.92)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.880632415943
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.89)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.05808672264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.08917892299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 1.12)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.21147833434
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-9.65910653463
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.96)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.92372194582
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.753790189341
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.72)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.918571735514
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.93)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.201671599524
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.15)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.40952776645
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 172
\-------------------------

Simulating trial. . . 
epsilon = 0.0053; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.2948531872
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.277758591819
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.19105959279
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.82687366616
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.75109628168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.90309704585
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.51880123762
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.97839544826
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 173
\-------------------------

Simulating trial. . . 
epsilon = 0.0051; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.80265642526
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.85)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24128291946
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.88786745787
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.91)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.85462352536
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.91106991605
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.53594995842
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.53)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.190873854714
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.18)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.14454825655
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.12)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.10273399788
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.06163721257
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.10)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.82606878953
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.81)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.06958899226
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.61927739282
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.63)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.41146858755
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.42)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-39.5210920518
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.74)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.0715324646903
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.05)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.47985185461
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.47)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.0636731443404
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.04)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.22759678767
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.19)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.78640039586
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.80)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.40844713007
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.45)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.4397269214
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.40)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.04641175062
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.08)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
-0.252509878531
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.30)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.13070907083
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.12)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.9999954989
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.98)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
2.40504350722
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.44)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
2.20124653136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
1.29343599686
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.29)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
1.26150851945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

learned value
0.586371733273
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.57)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

learned value
1.15210661683
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.14)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

learned value
0.320527660555
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.27)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

learned value
0.292503463536
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.23)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

learned value
0.695178429598
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.63)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 174
\-------------------------

Simulating trial. . . 
epsilon = 0.0050; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.86899448993
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.92)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40245555632
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.45)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.89588906958
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.92)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.438597635345
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.45)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.68636187068
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.68829386187
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.67)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 175
\-------------------------

Simulating trial. . . 
epsilon = 0.0048; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.7337846013
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.72)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.05012281231
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.06)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-19.8398116781
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.45)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.76666747945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.67722512631
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.69)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.79360680303
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.82)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.72677390799
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.75)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.26994195947
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44813968566
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.42)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.16844116674
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.12)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 176
\-------------------------

Simulating trial. . . 
epsilon = 0.0047; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-9.58319289591
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.88)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09464858562
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.13033480131
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.14)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.69420875391
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.93342739399
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.70487222204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.189795126986
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.17)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.89036128485
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.91)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.926126460376
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.95)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-18.9779753716
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.96593996236
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.98)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 177
\-------------------------

Simulating trial. . . 
epsilon = 0.0046; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.17413333119
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.72276522137
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.91523042791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.97)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35185129696
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.41326980043
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.41)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.04042104751
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.31449011382
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.58194195914
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.62219365623
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.504484967101
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.51)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.23897644654
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.61354820732
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-38.5665256749
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.76)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.29912163796
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.06006863435
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.733691520315
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.72)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.0289269877
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.26054857203
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 178
\-------------------------

Simulating trial. . . 
epsilon = 0.0044; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.92584983764
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.96)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81634866911
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.85)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.39993018089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26501393497
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.67467805201
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.65396673562
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.69)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.2986715566
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.87238282937
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.27699015405
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.49612753355
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0658233905348
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded -0.08)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.56462431461
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.48291773219
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.497230302159
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.47)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.68886708165
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.69)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.77678306568
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.78)
54% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 179
\-------------------------

Simulating trial. . . 
epsilon = 0.0043; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.02753194053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.82950424399
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.36301645846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.70526725593
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69480597503
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.52984777068
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.53)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28368312811
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.46916114651
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.50963518076
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.54)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.2721086148
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.43293394852
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.46)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.54399954239
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.31232267876
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.35)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.24881483102
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.26)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.21139719043
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.26430577229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.913351393006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.90)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.41782836233
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 180
\-------------------------

Simulating trial. . . 
epsilon = 0.0042; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.53659707692
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.42866833743
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.53563958748
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.53)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21966044351
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.50629855739
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.64529643061
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.10194136185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.93031549099
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.972905633047
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.95)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 181
\-------------------------

Simulating trial. . . 
epsilon = 0.0040; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.81981824908
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.83)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.66740238733
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.65100382909
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.71146126138
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.73)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 182
\-------------------------

Simulating trial. . . 
epsilon = 0.0039; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68685176976
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.15199360801
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.28534463953
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.30)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.66202169674
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.777957065
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.79)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 183
\-------------------------

Simulating trial. . . 
epsilon = 0.0038; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.05908263372
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.08)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.87728819829
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.81489575478
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.82)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.55484631998
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.51264180593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.13416477018
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.4965575336
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.49)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37744919205
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.78396321399
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-4.80771655915
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.96)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.63934918117
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.65)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 184
\-------------------------

Simulating trial. . . 
epsilon = 0.0037; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.0301921103
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65385830712
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94770196199
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.96)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.64508772023
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.72907654244
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.23145269719
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.23)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.3751606259
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.65898450635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77850307918
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.885907294781
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.85)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0975880007196
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of forward. (rewarded -0.12)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.62789237972
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.61)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.35677915575
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.89997757162
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.95)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.13426989214
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.60212222356
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.55)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.02241981252
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.05)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.473472399969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.9492601724
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.97)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.82121666578
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 185
\-------------------------

Simulating trial. . . 
epsilon = 0.0036; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.47215655264
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.43)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67099799541
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.68)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.6040083094
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.75776052233
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.8427017315
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.86)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.21096611106
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.38328902914
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.37433330437
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.42515690101
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.78143156243
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.79)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.8186080532
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.63584691394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 186
\-------------------------

Simulating trial. . . 
epsilon = 0.0035; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.415075917761
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.42)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2547392874
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02190266924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.38484365045
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.70885184975
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.71)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64318685961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.79559368978
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.30436380099
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.22485446246
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.10567277978
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.329243698984
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.32)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.66685641471
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.3630062305
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.36)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.35333770293
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.31)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.54659588353
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.992148650994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.60866277732
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.62)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.04101185033
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.01)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 187
\-------------------------

Simulating trial. . . 
epsilon = 0.0034; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.55349229383
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.54)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83930682157
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58925750559
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.46903979172
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.257251203324
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.24)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.459426970507
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.46)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.74255003835
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.67681028106
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.64)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.75660324353
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.77)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.32567775443
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.32)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.50167157061
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.51)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.89783729676
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.3202575197
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.31)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.872338793701
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.83)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 188
\-------------------------

Simulating trial. . . 
epsilon = 0.0033; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.29821814482
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.34611788005
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.31)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.90169912882
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.46802070876
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.62265462684
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80282550347
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.6435833247
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.69)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.68935121851
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.73)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.67875143566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.30368356607
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.67939464629
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.857844470892
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.81)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.764418828656
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.79)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.6675712377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.69)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 189
\-------------------------

Simulating trial. . . 
epsilon = 0.0032; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.051281096686
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.05)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.75083264466
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.81046389955
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.81)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.60913900706
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.62)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.337281269
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.7986933306
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.85)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.91991724327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2085983489
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.22)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07834863178
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.05)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.19073564184
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.33118569044
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.35)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.86007143441
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.4610367754
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.286980732211
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.28)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.56281337837
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.39046213491
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.2205756025
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.1770390136
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.16)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.63771369767
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.68)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.25527972509
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.90503949278
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.85)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.37973950455
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 190
\-------------------------

Simulating trial. . . 
epsilon = 0.0031; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.04537169364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.18862899646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80866966384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.95345973963
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.12458561356
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.11)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.13511881514
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.11)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.67390254475
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.2220218164
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.26)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.3084056623
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.22562580758
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.22)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54414984308
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.55)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.0578165425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.09)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.45365803019
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.48)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.18437389529
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.20)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.305040586236
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.31)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.866400088651
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.83)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.53761706478
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.51)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.30798637297
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 191
\-------------------------

Simulating trial. . . 
epsilon = 0.0030; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.83720603588
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.357837324067
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22404366984
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.22)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.0085930704
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06091593481
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.13845156771
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.16)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.356760321261
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.33)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.499659619
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.507351213
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.48)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.39311200728
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.41)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.16342319757
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.16)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.22655615311
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.26)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 192
\-------------------------

Simulating trial. . . 
epsilon = 0.0029; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.21395806615
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.46166226785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.45863123819
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.34477249958
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.30)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 193
\-------------------------

Simulating trial. . . 
epsilon = 0.0028; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.77079188449
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.01456590313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.75927413327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.78)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.10490702832
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34391657531
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.87294732496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.6840351457
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.70)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 194
\-------------------------

Simulating trial. . . 
epsilon = 0.0027; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.91683266758
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.98)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.309483491868
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.24817971574
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.79498334319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.91741526983
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.22434182017
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.39248227195
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.10425476544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.11)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.02447752147
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.38229048414
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.15315774994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.12)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.706193296697
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.71)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.28433776157
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.95700404433
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.95)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.98299659965
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.98)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.43717165671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.01217394994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.98)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.98431112081
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.99)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.930414285172
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.90)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.413770621577
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 195
\-------------------------

Simulating trial. . . 
epsilon = 0.0026; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.22084939096
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.19948961507
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.70053284604
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.69)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.66993159392
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.35954086021
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.39)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2901106045
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 196
\-------------------------

Simulating trial. . . 
epsilon = 0.0026; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.71839742933
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.89347301069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.92)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43501912756
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11624269093
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.42725117922
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.45)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.05994927411
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.01)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.70261853649
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.73)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.42222070174
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03406064382
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.04)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.21283997107
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.17)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.20956431978
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.23)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53120326862
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.54)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.490952875346
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.51)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.08982109799
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.10)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.79406905763
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.78)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.29690829224
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 197
\-------------------------

Simulating trial. . . 
epsilon = 0.0025; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.34490439786
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.35)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.47794455553
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.49)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.95632877225
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33082284058
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17173207714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.62102845935
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.62)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.610045331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.63)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.49516701477
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44294277659
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.45223685346
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.42)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.11068542496
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.66568359883
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.21048361883
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.42024921682
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.58048028313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.73496813969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.989354636677
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 198
\-------------------------

Simulating trial. . . 
epsilon = 0.0024; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58487519603
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.60)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.78562192889
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.01887079391
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.976778270736
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.37397849527
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.0415700251508
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.91343975038
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.93)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.43868579399
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.48)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60773142166
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.793892647801
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.82)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-5.108661855
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.27)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.59727276257
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.58)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.70152562679
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.68)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.1659998255
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.20)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.12202108524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.10)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 199
\-------------------------

Simulating trial. . . 
epsilon = 0.0023; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.32798344052
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.66509100367
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94378557359
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.96)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.22814595927
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19413720708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.03637646969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.65221454011
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.18927500123
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.18)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 200
\-------------------------

Simulating trial. . . 
epsilon = 0.0023; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.45266215455
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.50062822323
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.87259894731
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.39145104468
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.548749659729
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.835157945736
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.46552049797
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.45)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.83116648298
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.89)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.29627508814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.25549117586
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.86557003409
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.87)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.818616502432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.70049538035
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.72)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.4560752499
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.45)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.7193523312
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.73)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 201
\-------------------------

Simulating trial. . . 
epsilon = 0.0022; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.84066003477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.88)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40807955743
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.70276846643
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01327222374
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85722575443
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.89)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 202
\-------------------------

Simulating trial. . . 
epsilon = 0.0021; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68850497975
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43198971699
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.47711095682
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.78114281324
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.79)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33482184027
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.69499732939
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.68)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.61761289838
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 203
\-------------------------

Simulating trial. . . 
epsilon = 0.0021; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.31314552
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.29)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71904135071
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.73)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.86682710006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.98609792408
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.97)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.34849995703
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.35)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51423191833
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.974019177993
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.97)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.72697038875
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.73)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 204
\-------------------------

Simulating trial. . . 
epsilon = 0.0020; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11359386127
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.07)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35774025139
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.103339638
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21061787946
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19080969678
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.20565395504
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.21)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.89106525716
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.91903077471
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.82187791004
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.87)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.55610194734
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.2426255705
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.24)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.454463692648
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.43)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.48484775825
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.52)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.942298794833
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.90)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.315838324724
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.48598910133
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.49)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 205
\-------------------------

Simulating trial. . . 
epsilon = 0.0019; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.60080339145
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.0965920667
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.75252099631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.60822641639
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19864538895
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.991366464602
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.95)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.06937498021
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.06)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 206
\-------------------------

Simulating trial. . . 
epsilon = 0.0019; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.80369866073
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.82397567907
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.85)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02102213965
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.98)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.51170519628
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.56)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.50113244448
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75187034806
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.74)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62461612043
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.61)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.34576142429
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.83290039448
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.84)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.36110739655
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00883767423
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.35874460853
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.82811653401
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.775151710988
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.75)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.24513341363
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.25)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.73127433597
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.74065127192
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 207
\-------------------------

Simulating trial. . . 
epsilon = 0.0018; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.1113711511
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.07)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.48601434407
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17555009655
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.62019183196
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.80685079954
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.78977118175
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.84)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 208
\-------------------------

Simulating trial. . . 
epsilon = 0.0018; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.94279452494
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.97)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-8.94809603255
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.966875103594
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.95)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51664928405
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.35300723627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.38)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.04241407496
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64423785381
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.68)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.23587263684
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.25)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.52000987723
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.54)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.1889177658
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.19)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.28851355744
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.32)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.96085172006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.33803986052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.73446134944
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.33409818248
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.34)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.6129721408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.31370785035
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.83171738357
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.86)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.935523783252
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 209
\-------------------------

Simulating trial. . . 
epsilon = 0.0017; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.79403439668
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.47799206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58787623746
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.63)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.93987530199
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.28412589724
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0289509528
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.85589151087
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.84)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.29743630567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.28)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.69679694124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.71)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.34328865906
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.38)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 210
\-------------------------

Simulating trial. . . 
epsilon = 0.0017; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.79874971514
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.84)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.05854602708
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.24787479089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.66753407296
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.81210615349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51817537574
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.68175898782
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.65)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 211
\-------------------------

Simulating trial. . . 
epsilon = 0.0016; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.427652037281
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.40)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87373341895
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.90)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.13908564331
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36788068289
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.570473394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.63117964455
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.118319991854
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.07489815877
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.11)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.29371110041
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.835209436384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.82)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.38660642514
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.36)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.29581981721
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 212
\-------------------------

Simulating trial. . . 
epsilon = 0.0016; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.10664767598
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.14)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.93674475399
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.94)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.37146650663
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.36)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.63698111296
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.65)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.891601951914
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.92)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0122623068
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.79835995647
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.10908894149
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.12)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.4639681454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.84088844707
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.84)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 213
\-------------------------

Simulating trial. . . 
epsilon = 0.0015; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.34804591435
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.32)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.173557776827
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.18)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-10.3159985726
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.64)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.22419693032
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.23)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.990473650587
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.385006361475
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.40)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34838762581
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.36)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.39431307207
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.91471515976
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.95)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.75527581566
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.15129151061
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.19)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.61399321089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.62)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.09222375654
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.24935967191
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.56748249733
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.41194215996
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.49735418206
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.50)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.839477286927
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.78)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 214
\-------------------------

Simulating trial. . . 
epsilon = 0.0015; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.3504539309
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.34)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.78208677314
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.6278932918
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.62)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.61382858324
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.84311097861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.12103859248
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.10)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.15269823898
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.14291231061
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.46941445068
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.587221485506
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0383212659097
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded -0.05)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.80150803981
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.82)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.39735233418
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.39)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.956313615864
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 215
\-------------------------

Simulating trial. . . 
epsilon = 0.0014; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-10.5595758872
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.89)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.82864799938
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.69817844889
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.69)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.3340582591
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.38)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-4.21438135853
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.13188045902
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.17)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.37018865942
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.16928573381
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.74401835425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38659096198
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-8.82879431655
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.10)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-0.0796807661481
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.09)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.608069203
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.701630456591
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.68)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.78693891408
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.77)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.37360873685
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.37)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.929040707722
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.89)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 216
\-------------------------

Simulating trial. . . 
epsilon = 0.0014; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72479690069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.60995424618
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.0345747158
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.02)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.0655448365
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69577964991
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.802370314448
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.30563667089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.520493504384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.50)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.7680050323
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.80)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-18.4728369211
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.04)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42554829028
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.41)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.56086666983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.54071007407
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.53)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.832094399048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.3050016135
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.32)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.12868269667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.09)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.13223416998
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.64538133495
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 2.70)
49% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 217
\-------------------------

Simulating trial. . . 
epsilon = 0.0013; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.29084727206
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.32196982704
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.31)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.81629017055
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.85)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.53371464424
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.01802417158
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.30373767174
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.28)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.44431092985
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.48)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.93816537034
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.94)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.73718464997
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.80)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 218
\-------------------------

Simulating trial. . . 
epsilon = 0.0013; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.20187409231
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70241869272
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.03190888039
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-20.0352945896
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.65)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.35251907842
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.83715318111
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.82)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.91023267564
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.14899984983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.13)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.90796566701
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.92)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.6692184847
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.71)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.0665937133
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.05)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 219
\-------------------------

Simulating trial. . . 
epsilon = 0.0013; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16338088919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.19)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95679411457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.98)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.04154322796
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.07)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.99814845028
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.97)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.08590360679
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.08)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.19854681256
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62116630036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.62)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.18943786365
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.16)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64717781201
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.10054197077
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.07)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0596062088052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.02)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.35457856926
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.792129970933
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.78)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.11561847089
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded -0.15)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.07394901798
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.06)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.42515475789
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.41)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.518791325
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.67381075991
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.67)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-10.5350739738
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.86)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.07717543592
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.963946010967
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.19258971459
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.7900185356
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.80)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 220
\-------------------------

Simulating trial. . . 
epsilon = 0.0012; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.31421867071
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.68647813914
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.8303093752
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.82)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88997096331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.58437224307
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.60181865614
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.1440880191
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.26681281576
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.46465685462
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.87685327221
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42643831198
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.44)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.88219701162
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.90)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.04815660573
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.797011039328
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.71780517651
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.75)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.536631653063
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.52)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.623841715064
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.58)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 221
\-------------------------

Simulating trial. . . 
epsilon = 0.0012; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.78267964386
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.82)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.58428949311
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.07777606551
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.42397930562
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58270622026
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.59)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2617126698
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.88672212596
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.20788719942
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.20)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.48849190376
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.55)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.06390059423
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00789132387
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.00)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.28898521624
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.31)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.28283483287
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.26)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.85336486524
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.70773243472
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.41565362093
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.44)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.67768990378
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.73)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.33127818499
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.33)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.15107599461
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.20899471471
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.56993221868
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.51819418234
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.40885171963
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.2987384117
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.25)
31% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 222
\-------------------------

Simulating trial. . . 
epsilon = 0.0012; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.8081046844
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.346299338633
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.79922580595
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.95827722154
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.98)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 223
\-------------------------

Simulating trial. . . 
epsilon = 0.0011; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.81235097074
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.10092996321
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.08)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82353534805
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.18517833222
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.22)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.44603964129
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.94162651206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.04875918683
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.22487592236
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.57358950186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38046252293
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 224
\-------------------------

Simulating trial. . . 
epsilon = 0.0011; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.244846482908
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24605913696
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.99069166041
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.08639955916
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36073232165
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.74364479266
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.695667961567
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.20493472747
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.3254640585
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.41671147218
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.16654522419
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.22893428137
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.91211514961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.966417158234
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.44677882767
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.46)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.25800945937
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.05331158576
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.385865758374
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.34)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-0.735119019361
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded -0.77)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.87780599144
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 225
\-------------------------

Simulating trial. . . 
epsilon = 0.0011; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.40908557817
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.40)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.57350044941
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.23295109185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.702905425787
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.72)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.68476325205
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-18.5172574873
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.09)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.951309097265
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.67809705477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.73)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.08081758936
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.09)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 226
\-------------------------

Simulating trial. . . 
epsilon = 0.0010; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.105365208679
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.07)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.396131878578
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.69348135853
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35542850037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.47949773163
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.31532423235
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.87915875604
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.871976530351
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.87)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 227
\-------------------------

Simulating trial. . . 
epsilon = 0.0010; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.797045282904
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.79)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.4317137965
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.40)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.96987394591
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.00)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.18436284747
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.19)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.25553332034
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.41295050526
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62853552018
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.65837168716
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.87652468044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.90)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.19658878058
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.20)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.77323180961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.82)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.64808597518
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.66)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 228
\-------------------------

Simulating trial. . . 
epsilon = 0.0010; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.722561483265
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.96320832361
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.98)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58025918851
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.05477259819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.40107215457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79667979891
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.345371926
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.8284299819
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.83)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.66619295862
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.71)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 229
\-------------------------

Simulating trial. . . 
epsilon = 0.0009; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.8320392466
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.33814124886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.48212263192
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58566651089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.36784160221
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.36495419977
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82091620229
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.84)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2909310251
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.99023428718
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.890474265471
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.968200562391
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.88609718097
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.88)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.15835929162
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.57172568502
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.59)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.817454757
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.84)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.04659934013
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.07)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.88956347953
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.2137227653
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.22)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.273309172657
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.25)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.97368945651
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.95)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 230
\-------------------------

Simulating trial. . . 
epsilon = 0.0009; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.17284986864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67525375469
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.489624982823
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33790739257
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.33)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.04344609594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.08)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.968304757508
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.94)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86033972363
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.89)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.953527409162
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.92)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-20.3560160278
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.99)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38634244017
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.40)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 231
\-------------------------

Simulating trial. . . 
epsilon = 0.0009; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.46331096301
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.588337679687
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.61)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.9803782918
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.01)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.74797505345
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.80)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 232
\-------------------------

Simulating trial. . . 
epsilon = 0.0009; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.638982655194
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95313172264
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.43681609601
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.25452038729
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.01837539282
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.18501912677
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.16)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.31403149147
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08331128694
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 233
\-------------------------

Simulating trial. . . 
epsilon = 0.0008; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.91354429855
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.91)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.08987665214
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.33521602573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.35)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31340082817
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.33)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.09089944831
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66806883633
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.71)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.795007437478
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.79)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.3740244235
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.33)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.17742846832
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.17)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.8917569472
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.42120233989
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.42)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.09205045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.06279734697
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.866914721176
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.83)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.53728952427
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.55)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.13170080536
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.33423244051
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.918278148859
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.88)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.81371349897
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.87)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 234
\-------------------------

Simulating trial. . . 
epsilon = 0.0008; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0501781162171
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.6466979718
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.70)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.120411157746
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.12)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20396410777
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.15)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.97286395425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.54623591951
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.53)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.77800353844
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.79)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 235
\-------------------------

Simulating trial. . . 
epsilon = 0.0008; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.09320776071
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.326427896477
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.12144298099
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.91792697867
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.55147327053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.57)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.38826650795
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.49376145114
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.53)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.29569737974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.20834301359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.40419901605
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.87164282763
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.13581263309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.09)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.54142359192
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.42249510102
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.38)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.757485237862
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.75)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.194781708519
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.20)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.3020509408
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.29)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.02064015612
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.02)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.614512479175
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.62)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.432245686548
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 236
\-------------------------

Simulating trial. . . 
epsilon = 0.0008; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.78192332024
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.64102900611
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.83803548561
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.37129133287
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.996073557412
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.62373631976
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.60)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.336755769
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.4071099037
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.42)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.57681197118
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-5.47004525372
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.944699776893
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.94)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.86080029947
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.87)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.77467785612
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.77)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.44770169215
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.32241923175
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.638782746644
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.62)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.12104619847
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 237
\-------------------------

Simulating trial. . . 
epsilon = 0.0007; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.80200748746
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.8485306234
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.86)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-10.4799149541
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.3359544192
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.50173701517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.54)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.50273138704
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.49)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.52443331009
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.06955982457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.82294648514
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.84)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03578241711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.58801947415
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.77755348663
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.266624644528
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.27)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.03691067843
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.01)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.29643066812
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.29)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 238
\-------------------------

Simulating trial. . . 
epsilon = 0.0007; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.30661641394
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.66499242557
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-38.5183530748
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.71)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.15382049944
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.69166246744
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.86458375877
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.89)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82339176868
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08730870573
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.07)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.37958252743
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.81581233803
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.48327712961
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.06310893292
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.03)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 239
\-------------------------

Simulating trial. . . 
epsilon = 0.0007; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20982417349
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.65892484372
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.33320243224
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.51640518199
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.46582734112
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.24778574983
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.26)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.07604876049
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 240
\-------------------------

Simulating trial. . . 
epsilon = 0.0007; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.20314358949
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.56317460205
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.21086273916
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.21)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.23246770918
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.064567537453
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2419144092
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.23)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.12139870903
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.09)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41307596464
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.43)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 241
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72030639303
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.27352761738
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.75587004893
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.56120907448
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.74324429121
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.77)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 242
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0618123864829
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.04)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08570063912
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.35226864901
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.9056558782
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.90313925422
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.6674936124
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.31843772408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.32)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 243
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.783668258842
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.81)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.72894605998
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.76)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.69278268535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.45305076685
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87907582562
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.11094271277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.67981805761
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.72)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.76985497523
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.15234330468
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.13)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.64978127459
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.70)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.20030618742
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.19)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.07911516143
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.03)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 244
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.48898368391
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.57099184558
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85292489993
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.58464891864
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03433037442
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.78568282522
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.09048036654
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.76633574859
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.74)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 245
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.38487121863
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.41639006102
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.45)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.70972734667
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.60579857005
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.22949919395
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.12871005967
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.06522086435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 246
\-------------------------

Simulating trial. . . 
epsilon = 0.0006; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.20636452651
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.24)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.42944862485
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.41)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.26806571188
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.04824267467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.85597041549
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09702885971
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.09)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.54739782932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.32916172974
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.32)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.547977510264
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.56)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.63515783283
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.78921618876
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.43036597593
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.42)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.42872487905
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.49539614732
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.60597111915
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.9529764981
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.53192136134
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.59566033644
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 247
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.43790607296
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.51998718216
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.110534699
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.06)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35411755082
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.78955052761
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.77)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.02074915469
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.23414457168
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.27)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 248
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.38153098354
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70586547051
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17007903087
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.16)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.6512264962
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.56498113285
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.56)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64556933405
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.62)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.72583445253
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.17385944168
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.50729886406
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.52)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 249
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.71235262437
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.58806659357
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.56665547572
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.57271762733
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.549433425209
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.53)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.70435959741
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-5.47225746031
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.24238636112
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.970284361758
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.98)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.13554331381
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.998511264807
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.99)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.64904407323
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.00286096319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.30855851278
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.30788284328
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.32)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.09879611181
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.11)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.849074661326
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.84)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.72976007069
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.74)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.0132456448
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.04)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.750972823043
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.74)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.11089420823
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.08)
16% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 250
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.12987672141
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.16)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.94984886384
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.97)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.83533095051
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.83)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26532993107
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.24)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.75370705821
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.842493721563
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.84)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.32238454305
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.60931386974
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.57)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.753035976595
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.36944102548
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.219143460487
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.22)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.21990356618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.95107093822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.22583207681
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.89155886674
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.89)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.60980004987
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.4631894851
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.975708644538
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.97)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 251
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.14259888055
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.99362839402
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.01)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.92187256908
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.71977177851
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.74)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.325296919
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.65821692573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.233303274022
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.22)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48741077049
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.77177142059
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.25244872911
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.24)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.22926429874
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.24)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.01429469358
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-0.0991547982869
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.14)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.42597368093
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.49)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 252
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.70832155445
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0513452248346
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.00)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.06763057565
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.27956407814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.76844083265
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.74)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.546874839522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.51)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.984128044571
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.427472797943
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.76857599936
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.81)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.22653263206
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.26)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.61150809412
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.62)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.03861757637
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.6359857247
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.61)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 253
\-------------------------

Simulating trial. . . 
epsilon = 0.0005; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.71992665592
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.69)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.10999323296
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.13)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.57332008116
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.13357860505
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6806388161
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.53801989398
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.54)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.03966302714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.14431232033
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.16)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.70708369603
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.48068372708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.44)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.22438456636
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.34496129873
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.81678129322
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.83)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.01012652413
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.01)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.58151144072
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.58)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.874485025284
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.84)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.23482540071
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-19.1906391279
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.78)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.22556314339
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.23)
46% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 254
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.43274848519
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.81164786341
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71042845986
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60446182532
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.01219695386
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.98)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08446880945
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.13)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.42948623969
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.77228130135
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.19731740988
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 255
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23270561416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65936360057
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.70)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58514065417
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.64157665656
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84663896028
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.30003311088
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.29642193901
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.33)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.86732503661
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.87)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.864311919646
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.83)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.61453056297
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.63)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.75573462996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.72)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.66669026586
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.57413985985
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.57)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 256
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.50329282066
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.13151537829
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.14)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.83997867527
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.86)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.15252367462
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.14)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.89444749321
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.94)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.53657415662
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.29179925747
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.33)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88013784206
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77317070489
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07054936797
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.05)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.71795967327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.73)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.70833852929
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.69)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 257
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.67129170034
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.69)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.10927428625
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.68310376185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.68)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.54312607002
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.57)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.9462058447
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.94)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 258
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.81312483448
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29350238125
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54208592907
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.42728724362
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.18161828213
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.06535743996
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.08)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.59228859085
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.67446443381
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.5347633738
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.60222366194
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.62311518814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.63)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.40251983243
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.44)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.58314332532
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.58)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.15793681013
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.16)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-18.5199147993
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.09)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.30950151601
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.31)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.904451166162
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.88)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.11385367763
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.10)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 259
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.61949908478
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.63)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.39302505581
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.30938351969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60501529056
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.64)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.42595075106
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.1896875058
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62249561049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.60)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.28953181022
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.67447552309
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.38093146012
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.41)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.69624766441
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.62699147563
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.68)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.280329448167
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.26)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.778893059218
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.78)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.27179874781
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.603830677898
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.60)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.69189214824
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.69)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.809487928924
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.77)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.04728306482
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.03)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.43765684829
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.43)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.29621677693
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.31)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.536488572719
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.49)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.389125229668
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 0.34)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 260
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.86133561502
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40595957164
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.63363758407
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.65)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.85705325682
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.85)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11797197962
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75733288161
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.48853451395
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.48)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.74102135991
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.71)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.80074339822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.61462037767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.05903940104
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.03)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 261
\-------------------------

Simulating trial. . . 
epsilon = 0.0004; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.50430202392
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.52)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.59837496353
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61465296304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.16138828167
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.12)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.61538046375
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.52578231485
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.52)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 262
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0775553416284
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.05)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35941571711
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.35)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.214300260161
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.21)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.806324787548
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.81)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.2819835737
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.2913755958
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.81452895865
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48052950091
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.50931667233
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.72997802358
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.78)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.94826627213
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.04553078356
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.72389499775
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.73)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 263
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.04746434489
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.04)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.1137742133
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.51517326096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.72945520393
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.54659131379
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09824715131
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.10)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.2811816766
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48126258015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.22247155888
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.73757013593
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.75)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.860318591592
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.83)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.822027301406
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.77)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.83231780197
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.83)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.973716149957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.00)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.13825927253
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.97191865762
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.00)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.13679137955
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 264
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.61280905584
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.64)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.01724104473
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.01)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.8705666279
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.2000507619
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.87377106571
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 265
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.77486413352
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.57504593029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.05612343539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09148954337
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.09)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.20760988656
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.22)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.02432924173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.02)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.7966006133
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.46064821361
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.47902116155
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.43733913749
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.45968735905
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.46)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.40143935391
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.41)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.25861545856
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.41721667574
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.43)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.52901793332
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.53)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.52855964322
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.50)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 266
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.68257476977
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.70)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.01219683277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03913046154
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60440070995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.63)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.86287192064
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.26507751793
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.23)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34212773412
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.36)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.5436909218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.51)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71621574023
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.20331345451
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.993077178918
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.97)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.17608674466
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.31921526565
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.35)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.43235068774
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.42)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.06251527269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.03)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.51331637157
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.51)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.00401131721045
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.00)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.636146047065
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.59)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.27423501281
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.30)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.97319258728
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.00)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.12333169463
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.502938317885
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.46)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 267
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.376485826705
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.35)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41742635454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.38)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42272266471
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43796260127
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.46)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.74009437644
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.77)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.59942773545
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.57)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.47831076716
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.50)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62780858179
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.61)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.14676882394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.74223333799
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.72)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.18265362038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.51139208167
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.51)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.469785054
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.12476201362
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.16)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.42163377325
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.01971434995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.56093712829
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.56)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.23234730629
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.22)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.01343793067
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.99)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 268
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.79130552448
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 1.85)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.75646445108
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.81)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.9001223903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.23562248333
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.9305734564
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.95)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.83764237296
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.56674920924
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.00993043408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.96)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3172384202
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.29)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-0.029017045393
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.07)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.48041730171
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.55)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.49629138882
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.46)
66% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 269
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.65040411263
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.66)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.12768951354
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.13)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.13176188271
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.61107016506
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85235464544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.03318193768
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.458365199425
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.43)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.59221986366
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.42342384605
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.46)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.06263622503
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.08)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.65229202827
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.65)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.648583725581
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.67)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.71412489841
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.68)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.976064429754
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.97)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.50675403889
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.48)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.914699731267
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.58516696295
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.58)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 270
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.640210700614
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.62)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.85639436535
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.89)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.06433751311
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.62219663434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.04848648492
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2228702616
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-9.95642444617
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.25000015084
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.84983379866
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.9483899831
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.94)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.883075105557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.84)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.43659535791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.30504643043
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.33)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.31895438939
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 271
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.30547987734
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.00485395531
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60498093793
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.60)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39881055915
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33436845281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.04237460131
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.04)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 272
\-------------------------

Simulating trial. . . 
epsilon = 0.0003; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.13070355313
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.11)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83412579781
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.85)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.37843652747
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.38)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.33498672682
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.36)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.33987302474
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32584696778
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.57852239664
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.60)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.94318244052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.41409628745
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.39)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.23501645998
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.00954298881766
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.00)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.78592923024
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.074295799901
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.07)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.44542752508
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.42)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.42842815453
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 273
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.67163180703
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.68)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.89458600969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58976664281
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93133016264
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.94)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53982447533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.54)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.78908740881
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.76)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45658035394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.44)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-4.36513840132
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.33442253655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.80595572456
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.79)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.75453914222
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.93106472864
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.96)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.36690003649
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.30670478535
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.35)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.99151559688
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.00)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.35853886537
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.40)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 274
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.13428051552
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.00109042997
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.01)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42490566771
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58954593799
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.88767222527
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2974462398
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.82354948528
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.83)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 275
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.74359056136
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.77)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35164221661
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.31)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.28401895801
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.35)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.25538123972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.92640582973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.94)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.32005368506
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.48824897018
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.95248108573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.92)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.58483748308
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.87017731592
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.88)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.00424769725511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.03)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.36475325377
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.0237544135
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.03)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.33123807697
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.35)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.77623764762
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.77)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 276
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.91497858784
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.93)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.0560498694
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.68021293849
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08447919525
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.10)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.34290945723
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.34)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.82067876974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.37704401949
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-19.5521337986
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.16)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.2262111112
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.25)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.76184391656
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.79)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.98598157702
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.95)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.2769155583
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.24)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.64852961473
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.12695372275
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.08)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.05377305072
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 277
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.94727902925
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.47445497869
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.28346936666
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.972915674531
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36219149409
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.13908949981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.17)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.20095234051
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.17)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 278
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.68148884481
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.67)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2108060891
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17548154501
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.47265135029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.973738428638
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.59885901214
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.63)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24871923671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.44965976828
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.42)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.72817651697
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.34888438233
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.32)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 279
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20379848572
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.57531585042
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.95790018273
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.99)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.50552359081
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.87918602106
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.18022302173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.17)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.26388180867
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.20667910381
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.49159599222
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.49)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.58820671799
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.61)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.85161523751
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.5517251814
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.31673007117
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.34)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.70026699293
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.30826088614
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.04147578423
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.0378922610482
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.03)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.416109581534
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.34)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.459368604198
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.44)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.429565445734
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 280
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.9719034981
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.98)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.4380587533
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-38.8244795004
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.12081186459
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.11)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.54493523758
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.39530401087
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.01026539849
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.03494741981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.05)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.47529241977
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07061030885
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.6944611453
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.68)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.54848745318
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.59)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 281
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0779769182991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.90767254614
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53898844819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.58)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21516433628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.09062528784
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.82705146198
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62076320945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.51703201798
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.54)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.98732462049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.02)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.43121601949
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.02255332697
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.883921798089
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.91)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 282
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.84722215899
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.90)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.54515249919
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.53)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.3045986433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.90359472546
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.90)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06337186549
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.37141880916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.43)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.39033027319
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.42)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.08900522377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.68438613917
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.25647209977
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.890631806229
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.86)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 283
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.27383948821
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.0291619721
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.75031037223
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.28979649267
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6983787552
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8624505013
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.0319453364
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.04)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 284
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-39.5586596413
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.42996227143
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.47)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.51457212248
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.49)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.00039289542
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.36583953438
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.59994398684
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.14000762869
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.394423922504
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.14348038706
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.14)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.4967347843
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.48)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.38778752561
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.46)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.7388725536
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.61699964645
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.690446411179
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.67)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.852268442122
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.84)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.280429549524
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.29)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.6398673356
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.65)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.17162842928
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.18)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.14937687304
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.15)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.26675330673
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.68008349475
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.67)
16% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 285
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.71894523407
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.77)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.18382523833
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.67039828722
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43457361681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.44)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.27475435704
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.61013280013
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64811731898
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.29858779149
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.32)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.65621195113
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.66)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.45173987334
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.38624550921
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.37)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.59765473482
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.62)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.92478162303
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.96621795946
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.96)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 286
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.456213047428
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.45)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.33336184103
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.37)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.84623994595
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.60976098284
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.41626694397
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61488383273
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.65)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.33671844536
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.13715297224
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.15)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.81934625046
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.71070259106
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.35560786497
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.06486322927
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.4928392113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.07657251946
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.04)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.20980769048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.26335769819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.26)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-5.80504000124
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.98)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.78845564645
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.78)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.07885499254
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.06)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.41786741474
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.407633153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.540632812557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.48)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.529625319662
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.58)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.20875910331
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.733554803293
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.68)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 287
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.95323131931
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.97)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.94265467658
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17350822585
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.7778079514
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.13383768329
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.35578618792
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.325885192232
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.32)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.75526551696
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.74)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.978170584141
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.95)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.23669156183
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.24)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 288
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.60403738479
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81735131499
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.86)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59323784862
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.86741957251
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.89)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 289
\-------------------------

Simulating trial. . . 
epsilon = 0.0002; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.91033494364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.92)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.84751132863
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.15133766838
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21860957537
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.75410757281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51821396494
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.53)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 290
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.87857958286
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15059216596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.82265673731
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.30001898204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53333081485
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.38044606358
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.21726802403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.17)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52590428853
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.664547773828
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.15085435178
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.17)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 291
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.24458585513
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.28)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.27064892856
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.68726232504
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.7834219257
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.49037237026
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61259569827
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52018158528
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.49)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.875762277621
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.39764312866
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.42)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.69853987987
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.09552853043
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.957629318008
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.96)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.770842893482
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.73)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.65599442475
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.63)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.33449023871
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.33)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.158836313093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded -0.17)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.846042072324
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.80)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.344184521433
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.33)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.738326971777
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.75)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.169412728123
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.12)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 292
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.75539347514
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.84617490517
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80609697626
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.16455759601
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.78476701444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.21939718924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.30218880665
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.74774715122
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06154591661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.21852187414
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.42376364968
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 293
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65158065713
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.75065808869
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66045986379
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.6119057226
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71841951643
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.73)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.17015655204
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.967850947113
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.95)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.986339378015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.545561523933
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.53)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.67450256933
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.24180488711
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.26)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.65247053929
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.62)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.16875293269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.15)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.08262444508
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.09)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.5213886983
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.47836396693
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.48682376128
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.50)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.36306312831
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.36)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.00526664216
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.96)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.33454720873
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.71274855759
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.852160466873
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.81)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.926783608801
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 0.92)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.746262818056
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.68)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.968503844706
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.64463599628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.445251985523
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.41)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.515618334806
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.48)
7% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 294
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.76051549863
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.72252273928
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.16008975403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87841375508
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.87)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.62669036422
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.62)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.46337055274
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.46)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 295
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.28453208137
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.30)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.51570410625
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.84501050504
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.86)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.1294066231
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.8255033624
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.15141738273
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.574846950398
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.56)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.10997194216
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.09)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16377122959
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.0154051487585
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded -0.01)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.82664518159
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.85)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.22984354573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.37057899778
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.36)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.87008457325
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.54355506943
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.40641436176
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.44)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 296
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.32099506552
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74978198005
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.76)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.52570054769
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.89859408904
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.888842964232
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.90)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.30413853914
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.179394621269
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.13)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.24230207772
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.26)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.19430963017
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.97468165623
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.45294246251
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.0285510170131
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.03)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.71809194829
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.74)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.67012124644
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.903415046527
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.85)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.75204209422
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.72)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.688642052171
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.65)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.965249537143
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.91143964696
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.92)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 297
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.60275852053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.45169556601
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.39412288069
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.63366788089
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.54316524579
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.56)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.5079036207
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.53)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.21066302084
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.0134374229
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.99)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 298
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.676090844114
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.23757165992
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.22)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53017590685
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.53)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.55857686856
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.37334806498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.96441635677
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 299
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.995074005795
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.0834491362
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.06)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.29513385138
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.982563418001
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.17051138799
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.15)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.51541735269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.52)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.83731867786
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.84)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.99867829927
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.00)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64522078391
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.11974155449
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.00837706415
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.67453279246
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.67)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.06607623408
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.07)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.173034521728
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.13)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.36368131977
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.37)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.61592608008
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.61)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.25147070324
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.21)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.426687839657
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.40)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.76256857784
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.77)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.509164917936
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 300
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.1271724266
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.10)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15817477234
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.67372695314
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.52780926542
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.52)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.901697980093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.88)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.71829272122
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.67604484927
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.79247498887
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.22789792788
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.29460706108
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.3891273861
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.41897665343
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.27566895644
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.35)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.68566793836
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.35834564051
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.41)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.01539421955
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.00)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.7265894738
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.73)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.73241668335
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.74)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.17347226929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.16529708586
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 301
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.32824318535
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.08316673707
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56931512672
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.3296975125
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58766259389
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.54557997101
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.26538226775
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.28)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-37.9504424871
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.12)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.03668700084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.593483055588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.28183859885
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.31)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.05935323224
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.04)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.68707773544
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.07897920057
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.09)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.3120728568
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.23849529431
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.26815507201
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.27)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.87747308476
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.87)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.38270045274
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.38)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.15857214769
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.32793358836
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.5104968652
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.79683026177
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.83)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 302
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.8331277744
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.5573583535
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.72256919131
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.91929358468
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.26725413901
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80605797793
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.85)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.33496976325
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.32)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 303
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.03706427342
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.60006291799
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.63)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.81879573111
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.87474046996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36077131375
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.31)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27599276048
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.03535157229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.02)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.91707062425
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.90)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00009501466
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.01)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.81240564427
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.16279490357
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.8281200898
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.70880691581
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.72)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.95786456033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.97)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.04624311791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.05)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.869560873608
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.85)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.933772718003
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.92)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.54424011772
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 304
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.6287547932
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.76242554274
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80538445949
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.18148691483
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.365776259
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.69314863049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.00149284379
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.53292652553
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.53)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 305
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.2210955826
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.19)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.76976660061
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.0431454305
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58736808934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.58)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.6304426659
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.64)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.82519667791
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.90)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.49643779133
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.60715664687
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.60)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.37226213004
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.40)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 306
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.60675540288
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.59)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.93546595251
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02128406205
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.02)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.73736208672
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.74)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0563466639
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2655046726
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.24)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82776092126
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37753544951
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.892484251
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.87)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.90691155981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.89)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.13688071658
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.43798903893
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.054795711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.39287333984
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.44)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 307
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.10284902161
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.10)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.99007030382
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.02)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.54128308711
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20187074765
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.15)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03972654108
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.5447846881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.83671046727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.85)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.40307643701
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.43386835174
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.70009780521
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.74)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 308
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.49043329611
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.55)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.77069531045
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.79)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03677908688
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.34794133052
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.52318932642
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.50)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.28303946405
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.55560124834
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.23822181773
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.33733180861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.35)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.19387797894
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.637790934409
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.65)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.550239269
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.15305672616
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.12)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 309
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.9205484958
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.13266917942
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.33438988555
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.36)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.07367727699
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 310
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.21478396358
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.23)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.59372564423
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.01155201892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.00)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.67448538464
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.76604732526
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.39814267833
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.39)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.71550293847
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.3336897505
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.70300764131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.74)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.40489950353
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.38)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.77926475486
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.79)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.827822601699
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.83)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.67999371493
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.75425433103
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.07042157424
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.10)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.0274919944282
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.00)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.91759757597
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.92)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.352194318802
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.37)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.314057679885
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.24)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.28586558035
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 311
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.20434363984
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83228547525
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.34292293183
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.1436266334
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71909029923
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.20799696807
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.20)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.26331537532
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08397220482
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.09237250121
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.09)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.68389640606
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.69)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.830443631335
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.81)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.43163600631
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.225931756259
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.19)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.758010748265
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-8.90784271872
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.18)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.789683916197
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.73)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.49755679972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.46)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-5.58329430706
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.76)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.31570015992
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.28)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 312
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.82422625864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.79776771351
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11890692719
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.93511929084
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.92)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.675977277711
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.64)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.34242025551
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.51584429593
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.52)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.32248902867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.35)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.40385975931
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.43)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.14790636415
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.19)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.23719111629
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.966462575891
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.95)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.32215990014
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.57078940951
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.54706070385
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.56)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.11530399849
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.10634360257
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.09)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.810886880718
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.84)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 313
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.91310900706
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.98)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.60450345914
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.20094458341
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.03422666836
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87743063462
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46359428766
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64775415064
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.66)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.63441627788
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.3707776241
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.7451401185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.75)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.18155601002
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.96825916049
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.99)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 314
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.26324262999
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.40)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.70399423281
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.71)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.00053579447
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02521144919
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.10239422806
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0890499365
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.56634640218
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.68629134427
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.72)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 315
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.25732396195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2506013514
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.07242027823
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.20984888185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69376053594
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.64813846975
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86361219804
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.90)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.76998111021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.78)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.78492792444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.51812968437
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.55)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.65615010945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.96724030522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.19159763356
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.17)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.04611768072
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.62481932193
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.64)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.58882348533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.55595332078
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.655849833046
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.59)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 316
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.357915988
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.98532249717
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.97)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.11187660251
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.09)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.47052400479
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.52)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.09077409999
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.11)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.63493496977
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.60256561878
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.60)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.61783859353
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.70086239156
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.34678719198
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.675621153184
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.67)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.82469357069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.07927193206
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.84313894429
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.87)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.60702003968
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.60)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.69937673773
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.67)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-39.6546435662
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.88)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.28938920798
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 317
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.32970277394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.31)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.62755556926
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.67)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.60099943212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.64)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31632587186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.32)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85307165788
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.24072421478
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.444939194361
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.43)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.1589758297
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.11)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.12470708365
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.11)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.32147697686
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.36)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.67519498535
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.67389421744
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.67)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.40494880172
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.39)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.03941035711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.68718962651
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.75478557595
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.44328771354
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
51% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 318
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.76423642389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.23821355271
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.23333784172
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.88624913344
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65476169646
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.41352880686
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.44)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00238808094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.98)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.84439125787
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.13669781661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.51154316363
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.65951546025
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.65)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.959454540119
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.92)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.45229522339
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.45)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.28124093793
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.27)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.01667182056
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.814134839193
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.9319298553
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.93)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.725507091012
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.73)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 319
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.700863892786
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.64738974791
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.70)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.282763144635
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.890693248377
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.35617051267
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.35)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.84974733719
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.37757225966
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.1589967205
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.94350999858
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.92)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.76369258204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.76)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.0896034817
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-39.7219171101
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.95)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.15432341729
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.4053790674
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.39)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 320
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.14308011774
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.12)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.09025239957
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.09)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49100083461
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.06955505028
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.48986844704
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.01916294632
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.45468348964
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.46)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.8935372458
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.93)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.9702348086
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.98)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.82485066114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.81)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.32569365435
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.933046165347
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.89)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.956879989287
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.95)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.3262846853
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.44699667274
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.47)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.2370950069
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.25)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.97791248452
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.95)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.98657851107
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.00)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.06590199714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.14384232957
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.14)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 321
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.10664571664
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.11)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08223271798
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.06)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.5542548034
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09517076037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.47725832071
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.29284067015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.63578182508
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.64)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 322
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.73484021238
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.63502847956
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02711651769
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.98)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.9035629896
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.66325838004
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 323
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.53078318548
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83790104705
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.85)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46477724631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.80614694797
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.86)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.54709090778
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.59)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.04053281654
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.05)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.06512894185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.7687107814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.79)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.45110429606
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.46)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.69228684819
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.71)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4305618504
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.42)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.22020283898
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.77583228175
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.82)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.37294653416
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.84798349521
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.2962739761
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.57835043837
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.29187524143
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.29)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.87042497866
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.87)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.18037010934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.19)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.758175678475
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.74)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 324
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.70602874042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.57657674272
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.84056353965
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.54710333513
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.92273951844
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.93)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.35073415837
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.44070730374
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.45)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 325
\-------------------------

Simulating trial. . . 
epsilon = 0.0001; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.768759047357
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.84196605549
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.83)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.438463837992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.40)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.78839232993
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.5805673471
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.83236394627
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.09749167689
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.39788318143
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.75915280798
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.08511155686
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 326
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.38687238999
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.92452394243
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.92)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.89427554198
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.90)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.90031988447
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.91)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79427320239
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.82)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72773664806
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.45062288768
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88901547487
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.92)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.78829089258
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.79)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.6369453748
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 327
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.68749324235
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.41914427391
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85657182029
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.86)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.04309492997
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83287636441
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.86)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.76793170094
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.21514211519
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 328
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.35198813599
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.9938573316
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.33911557569
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.94055284061
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85403347405
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.09770081906
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.78916971172
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.91457952649
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.845344562068
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.32427695954
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.29)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.55016824738
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.84716951312
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.82460256834
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.09180942556
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.08)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.6389195292
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.62)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 329
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.25452244481
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.28)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.07425438606
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.561678211582
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.55)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93599550045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.08447218207
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.85268412145
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.0782279532
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.4954354176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60230703067
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.871102748618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56069906356
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.54)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.36106228075
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.035095924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.86748652777
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.84)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.326399229442
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.32)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.666084639366
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.62)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.422407518524
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded -0.45)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.11327856905
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.10)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.89707051778
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.930603394381
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.90)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 330
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.91987252036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.93)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.51413753985
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43114221653
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88701143771
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.19025703275
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.223080164
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.16124265055
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.65057283703
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.69521261673
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.37922523535
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.38)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 331
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.41850411162
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.89562094285
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.09150293681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.03619252098
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.30785408789
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.9916599063
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.99)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75385051898
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.71424448544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.73)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.58507684171
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.56)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.17715320728
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.14)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.20918004518
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.21)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 332
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.05124048369
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.07)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-5.35477178927
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43058226522
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.41)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01468074586
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.98)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04740031681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.57431204004
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.5357057083
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.56)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.892209857397
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.92)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.25734603677
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.22)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.75782009675
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.76)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.3418532623
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.36)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.71761358109
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.72)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.26289551358
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.54327856431
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.57)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.72348443662
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.73)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.27402093629
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.23315645439
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.07204332581
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.05)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.24055320431
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.24)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 333
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.15951813411
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.4537244395
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.2765755372
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.03955062216
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6211844974
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.59)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 334
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.60489123
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.40693697133
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.97880724959
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.96)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02085285212
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.88879698494
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.50254956458
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.49)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.98380907888
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.55498406255
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71097874591
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.76)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.934818665752
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.91)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 335
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.96365982673
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.99)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.45130493697
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.08935905264
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.93320246855
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.96)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.01008749903
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.38048117006
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.88523304567
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.88)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.302560713943
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.31)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.25820173401
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.27)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.69743592506
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.73713451504
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.74)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.6600694201
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.70)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 336
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.85485565417
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.83)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.27504855897
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.88241597408
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.26781457994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.8186302299
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.85934067829
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34992110209
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.950087524512
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.40506213642
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.39)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.01659977111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.00)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.35407593898
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.04228472082
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 337
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.64015188143
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24200507498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.48198382528
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.29553411036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.70548839083
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.2894860606
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.31)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.950800004086
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.91)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.71481516583
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.31324011541
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.17553609913
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.945462994962
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.94973201072
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.94)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.87806566516
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.79491382555
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.80)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 338
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.473509100803
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.88762981986
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.94)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.76982267721
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.7378965322
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.79)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79238074672
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.30450563983
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.55726151724
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.58)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.527949453614
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.52)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06933423037
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.68943403837
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.70728165429
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.75)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.2659033955
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.26)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.16131624202
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.15)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 339
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.31872212206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81356494908
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.61886784113
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51759802111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.30713162664
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.09904524299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.08333079947
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.08)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.47114847343
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.21572995051
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.36686617914
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.50698416733
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.50219336786
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.87498238781
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.86)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.02686557778
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.98)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 340
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.04211397587
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.807964188443
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.1038439484
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.10)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.57312850846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03414425098
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.25437578867
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.28)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65737001431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.11563262567
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.15)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.15726509314
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.19)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 341
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.4294968601
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.47)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.37191698987
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66176681615
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.39127287697
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.47046108727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07670123083
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.73063644347
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.75)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.29728299708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.34)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.07927842867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.11)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.70826997658
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.74)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.806394659389
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.77)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 342
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.41863174093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.40)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.643288978065
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.63)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.69022712146
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.00367059463
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44655464058
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.45)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 343
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23044564112
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.24)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.369981828035
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.33)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.06697677671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.32929667931
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.34)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.44446270489
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.209260033
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.19)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.43100112682
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.5444818594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.52)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.69408701487
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.42731421008
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.31270105761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.32)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.98407859373
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.00)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.53887206496
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.768922015801
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.75)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.302962589843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.16495986628
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.19)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.69217049723
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.32030782093
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.945706229288
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.90)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.2614399233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-0.199474959377
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.25)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.77110056923
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.76)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.119073593607
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded -0.13)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.845520883902
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.83)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
-0.133171816293
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.15)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 344
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.6041843129
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.96628951052
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.97)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.62727003256
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.91335330577
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.804850349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.22487140142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.313715138691
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.32)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26171802662
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.33749448598
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.33)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.32686741067
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.33)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.50078641601
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.49)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.41036820942
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.955460476967
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 345
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.04160329155
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.62859459309
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58523009665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35178276508
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.62743353498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.61)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.67445153314
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.21415516172
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.23)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.19141970265
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3899908766
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.39)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.995487615225
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.27823523835
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-10.2834773313
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.60)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.47205294644
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.51)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 346
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.30782466199
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.29)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.909744295219
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.93)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.50989388477
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.46)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.18153412969
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0142839847
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.20735667634
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.24)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 347
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.98878390258
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.00)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.76848201146
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.82)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.19191076143
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.39145568978
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.44)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.94319776031
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.52939191483
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.3977880499
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.38)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.80190752732
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.85)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.81698650173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.83)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.57546000263
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.56)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.7247456096
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.74)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.761755598596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.75)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.28589076827
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.6181615242
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.63)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.37556443221
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.40)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.46899576745
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.52)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.17730995078
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.17)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.21457386531
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.22)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.99780943365
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.99)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.27692276142
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.41415449569
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.45)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.15866320135
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.14)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
2.00337688639
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.01)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
2.04302169298
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.449199670395
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.43)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
2.09896893338
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
1.0540757134
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.05)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 348
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.87271763593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.00689463911
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.59387125675
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21985173593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.80664097626
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.79)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.76405546321
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.44401053237
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.10072271248
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.12)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.40294937237
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.923930864303
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.112682611109
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.07)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.94007809624
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.94)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.93636686367
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.93)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.22767946547
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.23)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.04031119628
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.01)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.50749636716
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.51)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 349
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.97737700411
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80337444978
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.84)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.857763803755
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.87)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.90566567197
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-18.7138201846
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.29)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.4985862614
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.50)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.35337550938
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.36)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.885631096588
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.11511418814
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.05399535528
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54170659762
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.44365385854
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.55731958635
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.03269227432
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 350
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.93986548877
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.99)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09756721074
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.08)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51124744832
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.54156293566
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.39798531894
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.79016956783
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.88854604142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.88)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.83061652666
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.83)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.51788267451
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.983116905897
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.08921979587
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.04)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.04353577067
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.04)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.62476167464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.61)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.10634024363
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.11)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.04989424394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.05)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.36695275232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.61072319737
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.04981891391
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.00)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.31993486063
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.32)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 351
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.05821465272
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.78431440859
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.83)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.53871597732
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.99355614628
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.43966276915
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.42)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.19837472096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.39494653133
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.24027807221
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.26)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.173872543369
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.09715843172
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.09)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.24379694136
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.22)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.24124384496
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.20)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.1896817279
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.15)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 352
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.812574651362
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.83)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.47766328423
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.50)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.14105054603
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.12)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.79520197318
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.82)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.95318230588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8577511006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92560296588
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26889545773
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.26)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.58962944499
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.53248059417
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.51)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.03079394274
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.98)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.95465441728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.98)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 353
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-10.2695397488
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.59)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87682749945
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85755999995
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.50274114168
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.44352032229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.04732291288
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.00762413016
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.51709661679
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.56)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.4768149005
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.26357127178
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.22)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.75511974436
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.77)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 354
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.16479030786
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.13)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.47647718791
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.87352364787
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.88)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.61902289778
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.417223345816
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.43)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.01350289232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.10632158468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.06)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.15641858222
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.19)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.69089896295
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.47851615351
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.31481212277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.46402889503
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.35331368474
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.36515807663
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.33)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 355
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.67820586274
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.0497736448
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.0786109486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.49572797878
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.31947819206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.56427243567
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.0340105540889
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.09623094698
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.62004252819
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.5916403418
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 356
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.01686679539
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.18312558815
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53567988243
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.54)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01660598174
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.08818950429
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.08)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.12535575911
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.12)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.945093432
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.939953091075
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.91)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03554439467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.01)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.938779359078
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.93)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.66446361504
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.63)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.55077896424
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.47441023432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.91089767539
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.89)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.699099647455
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.68)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.38884225339
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.42)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.75797492281
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.76)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.705658319191
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.69)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 357
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16429897553
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.62123183563
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.91360021017
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43108662136
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.43)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.31484042036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.993839390037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.07626538497
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.20881592832
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.587005988828
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.62)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.71470359129
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.72)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 358
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.88349155969
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.17050807738
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.18463493911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.07536654872
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.81041409042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.713968626433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.69)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.18346916224
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.21)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 359
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.44651711011
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.25906293348
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.59392148642
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.09468115115
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.11)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.13688889964
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.09)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.937971658889
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.92)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.11982986198
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.09)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.15438860218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.19)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.27576043251
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.47913987767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.46)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.93062710136
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.88)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.74569731841
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.79)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 360
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5131053293
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.534450408186
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.51)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.28050006246
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.32)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.06057829693
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.05)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.9066055793
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.91)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.39542949355
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.56995392233
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.61)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.82297909732
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71713740841
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.39897034038
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.38)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 361
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.6112227519
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.07542987769
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.10968910205
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.388468868267
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.37)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34316361847
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.944824790402
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.94)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.44232597841
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.39237687334
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.21550309599
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.01894784608
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.02)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.37182214355
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.40)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.70567222116
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.89049491919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.86)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.74963028097
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.44926390647
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.46)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 362
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.12531142192
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.12)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.58914920889
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.57)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.92179736155
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.92)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.14877378734
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51854360532
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.53)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.28321275968
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.37294444111
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.39)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.642293871502
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.64)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.658159622323
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.928608696991
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.42960243626
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.43)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.514724946
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.53)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.04346322249
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.08)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.963705974459
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.97)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.02153012859
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.00)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.59474079683
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.58)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.40776441988
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.79717328377
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.79)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.0151912212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.02)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.492538371635
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.49)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 363
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.182766586186
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.697413502738
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.72)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.382865966656
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15076777462
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.16)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79684169363
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.68383996803
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.71)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.954824609206
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.95)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.23836547849
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.80758190462
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.77)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.12903553954
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.13)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.42215677546
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.43)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.384417501966
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.34)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.41769968043
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.37)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.81230847
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.84)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.0654646615445
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded -0.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.53284445082
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.52)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 364
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.39952150603
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.57183070414
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02452166206
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.96429630242
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.95)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11502088913
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.12)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.835580991539
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.53024678477
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.52)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-20.0671614864
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.69)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.135596365053
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.11)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.31695920153
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.69530166227
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.66)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.28564626426
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.31)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.16393609751
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.124325065795
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.18)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.96454105583
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.99)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.36480466281
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.37)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.15781484572
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.18)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.739047482861
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.72)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.802308433986
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.83)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.441396610396
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 365
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.15744283838
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.59971256736
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.59)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.57160838438
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.59)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09562416081
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.68084309089
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.58241251049
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.59)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 366
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.36487557828
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.94407664602
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.57160339267
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.77594045499
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.12257323863
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.52946145602
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.12041340394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.63409265281
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.62)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.883194247367
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.84)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.27844719369
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 367
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.86825118745
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0178271088055
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.01)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.89958591259
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.78181022357
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.967918828698
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.96)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.056782543
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.02)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.65387973911
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.65103309966
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.64)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 368
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.236608694831
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.20)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35141521477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.38)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.79478862701
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.79)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.99814264678
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.10486818292
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.33054152482
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.31)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.11315365122
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.13)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.23328449382
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.26)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.400761169629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.36)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.82683016045
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.85)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0526335110215
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.02)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.27069394701
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.24)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.5842314292
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.61)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.30963429418
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.33)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.64778538793
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.67)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.90038422728
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.90)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 369
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.29346274637
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.84889258044
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.73075024576
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.77)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33977133129
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.68654860431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66645531992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.18466884169
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.91034281317
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.95)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.27133197364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.52599277314
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.53)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 370
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.9723681165
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.94384782264
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.98)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.87832258797
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35995327447
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04841108923
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.03)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 371
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.59792068586
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.62)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24266296862
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.23)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.04473615663
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.36079591075
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.89345297677
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.86)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 372
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.98638852282
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.68175146214
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.13985156514
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.57572585043
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.43088691694
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.6601474841
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52190940212
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.56)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.59557392434
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20731373985
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.20)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.97147750204
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.0618920378
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.5522011055
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.56324211169
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.46390270262
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.65218021054
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.78839303713
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.77)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.43992969877
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.47)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.935000607121
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.91)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.1743883197
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.20)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.11005509383
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.11)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-0.304292546358
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded -0.33)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.758694840911
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.78)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.86278716831
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.89)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.06403856171
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.03)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.888499605418
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 373
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.92807661762
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.97)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.43618695127
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.67003292149
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09011765471
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.01389548911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66467773767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.68)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.0976734734
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.54909518598
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.75523715689
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.81)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.42435006175
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.39)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.13460717751
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.13)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.72560150317
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.74)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.70326172151
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.70)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.65585755504
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.65)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.919705995916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.89)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.60604231081
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.62)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.385916141615
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.36)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.26838152072
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.79606528444
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.74299493336
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.61689070683
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.90295717569
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.93)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.709949699574
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.67)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.4127018411
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.38)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 374
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.23458704423
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.86603990904
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.90)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.78732542575
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.00678357548
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.98)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04466063153
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.19827637406
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.12173594543
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.59356862433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.66139570619
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.1391785237
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.15776949948
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.64860599979
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.64)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.99491787033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.00)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.82385078493
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.79)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 375
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.176281927963
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.13)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.90773840879
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.237074768
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.03146477838
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.00)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.23901124973
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.21)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.9118699824
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.94)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.87397853813
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08708481327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.10)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.54135926689
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.0297333117
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 376
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.80978160388
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.84)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65684223631
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.313369994344
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.58328490491
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36664687817
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.47330183694
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.37416622959
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.11052656807
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.09)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71918783784
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.8609607789
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.89)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.63214979522
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.99155430826
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.02)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.723827627284
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.70)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 377
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.3409967537
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.83880680861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49595540328
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79931954953
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65448212691
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.96173322431
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.03078164291
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.72883340908
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.12415855091
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.96462931784
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.27744862939
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.83718894658
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.54945334371
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.58)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.48185911702
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-3.99875243927
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.12)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.40740464177
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.45)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.020910234
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.865972232903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.83)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.589239841246
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.55)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.85723925845
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 378
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.60286373499
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.60)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.02334518045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.27844509524
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.66268962427
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53884128158
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.13867227303
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.40149216751
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.01940971992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.28942773698
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.47396641103
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.38692656396
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.67629643028
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.01380454909
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.01)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.00409207519
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.00)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.51144135324
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.52)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.650005309834
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.61)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.0766430490793
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.12)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.54087151374
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.55)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.862334017619
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.87)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.255670919153
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.19)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 379
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.10217804433
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.10)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.39276244268
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17093170692
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.16)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.34803529211
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19923423353
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2648757774
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.0674239023248
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.07)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.1104599936
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.10)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.03851597195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.01774600885
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.67111349262
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.5588784985
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.07803168169
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.39668405942
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.42211678488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.19644871568
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.41880383723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.41)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.04751900734
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.05)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.774861113794
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.72)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.41213041416
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.79690314087
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.1466802181
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 380
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5102397021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.67661762483
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.06223626542
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.06516375174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.74533700672
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.24853914871
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.96859300633
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.68954750746
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.52169831368
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.56485395952
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.60)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.0959060441
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.586395303015
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.57)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.73751872093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.79)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.45106343259
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.64963981474
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.70)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.07145764225
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.06)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 381
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.19898190412
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.05129634891
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.08)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.64644246608
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.63)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.5862677256
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.58)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51450688178
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.44445406058
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.43)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.38393133271
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.9351974876
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.23472408916
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.25)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.89076076995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56050756243
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.4035924428
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.54621232146
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.62)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.23558249159
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.97924780879
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.77935767148
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.79)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.05346639153
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 382
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.15799695224
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.79553541702
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.9544220236
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.26619784455
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.48334616814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.96720299517
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.94)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 383
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22866159419
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.23)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.08121450545
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49194033185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.52)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31953930644
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.36)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.05238886635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.07)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 384
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.85395782984
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-10.3490690197
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02720896477
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.65880576909
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.55641162721
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.72461345101
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.73)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.74486217733
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.65275033998
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.65)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 385
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.00280319395
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-5.76851795804
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.393250933716
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.36)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.46958394397
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.94972369039
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.93)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.26596281026
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23722669003
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.05663845551
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.54055770552
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.0687854411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.7602072904
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.76)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.890983186036
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.92)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.20620186996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.21)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.40444033623
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.39)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.34665899009
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.38)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 386
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.797102616979
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.82)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.42048286197
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.26869817008
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.84225731424
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.74414140888
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.77)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.60738939313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.75028439933
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.80)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.02344260286
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.98)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.02932441785
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.77634526985
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.059273441568
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.943098237358
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.34178644589
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.35)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 387
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.6508307423
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.85)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.14119693322
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.14)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.33631817288
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.46012164772
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65968018026
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08642274326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.11)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.83684736378
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.90)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.11229680022
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 1.11)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.72873779379
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03031172885
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.04)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.62150551743
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.61)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.0726860310015
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.02)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.30901859707
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.31)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.4338060751
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.44)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.67469829519
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.4977997657
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.50)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.904217483995
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.92)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.25642669779
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.26937183665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.25)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.10639081764
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.751971983365
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.74)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.85911700287
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.85)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.11316242394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.09)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.259277957854
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.23)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
-10.1909382816
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.51)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.53281918715
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.50)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
1.10021780244
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.10)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 388
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.04269803936
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.04)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.99513641382
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.97)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56492825173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.88151089311
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.89)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.53430396922
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.29983620467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82815750263
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.85)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.0525930133
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.54116754846
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.55)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.23505456279
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.19)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.83877677796
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
69% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 389
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.93617143021
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71835256955
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.77)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.03296386145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.70324037454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.96779684414
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.962518133332
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.94)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.26753402517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.35699874963
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.43982733457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.44)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.13994215894
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.72165567252
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.9185716161
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.92)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 390
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.24818356604
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.65802457941
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.30432633144
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.20333513292
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04897343531
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.29218338668
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.27)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.294787998571
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.6316574384
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.66757118571
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.20739058402
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.47745951516
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.34378443829
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.38)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.53284211325
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.3554201219
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.35)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.443959337815
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.41)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.25805776123
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.53414117828
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.51)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.27301675176
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.01555399421
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.6260022361
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 391
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.70628040839
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7462149002
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.79)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.30963955703
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.34017317912
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.38)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 392
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.66646881441
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41257646894
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.38520220139
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.63615339705
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.58354643995
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.81787081414
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.86)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 393
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.382716523278
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95030715659
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.64622225629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.13231719721
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58142676504
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.62)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.964915652157
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.33188472464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41334725841
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.45)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.955878534433
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.92)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.949219991246
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.92)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 394
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.57289694666
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09585472929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 1.10)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03644229843
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.420167721857
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.43)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.93026001269
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.91)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.34165238632
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.35)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.67347198486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.96451622459
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.76548958008
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.470481684691
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.55603058237
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.3218408358
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.32)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.19982681297
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.19)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.12705558668
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.13)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 395
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.74450628832
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22772619243
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.18)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.64649883534
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.91720460672
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.00781012854575
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded -0.01)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.17406317726
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.13)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.60038103123
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.12927807365
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.81744665897
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.84)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03960494903
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-8.76348392328
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.03)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.572321013355
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.860593226761
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.85)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.788489513125
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.79)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.14487275077
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.55352671959
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.31822174147
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.44104457121
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.48)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.26163539974
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.30)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.822091661371
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.77)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.28252197613
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.26)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.62220085021
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.62)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.854550480258
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.81)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.33950668421
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 396
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.96944677895
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.82805496837
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.63378194468
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.15756172338
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.57423356047
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.14113406284
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.16)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38207889493
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.79997474248
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71193691719
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.72)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.47471183194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.51)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.959647265659
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.65607205104
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.58465945985
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.55)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.10350305451
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.59197677464
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.59)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.66028533552
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.605762968901
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.57)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.8879361987
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.86)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.77831315393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 397
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.0715872552
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.09)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.33915559105
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.32)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.07578499429
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.9353577446
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.94)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 398
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.13630392298
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.16)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.3750931152
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.42)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46558995535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.48)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.78296384521
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.15385639778
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.83171443057
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.70742457998
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.69)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.49242762318
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.5231567052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.84563476587
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.80191398597
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.0974847882
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.130229138908
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.09)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.99940491568
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.98)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.758460074722
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.07406980916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.11)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.30288858718
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.749807731948
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.73)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 399
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.73400143078
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.76971009416
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.77)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.1346708083
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.14)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29748038548
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34965554852
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 400
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.73713773155
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7841850306
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.33278345467
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5146119738
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06567921677
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.47971883223
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.50)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 401
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.08548421929
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.08)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15042829542
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.15)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.09983589789
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.81278917276
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58953863225
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.93713365815
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.97)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.34263434715
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.38)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.64377011206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.89622572964
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.72131012581
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.72)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 402
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-9.43824066288
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.78003541973
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.78)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.36015709121
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.00714329064
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.39733781632
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.50925844699
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.54)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.761105638774
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.77)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.51516023686
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07554563949
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.35097833727
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.35)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.84603974884
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.2942747849
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.29)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.52550331784
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.57)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.26859170425
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.30)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.20787282596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.986788037087
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.94)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.98453127261
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.99028424017
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.664697461506
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.870704986111
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.86)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 403
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.85458835912
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.86)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.46061916066
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.29484820244
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.406294766
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53716907568
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.85382905005
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.90)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.22745904893
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.539807640442
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.51)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.87876066728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.472630317
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.44697183882
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.48)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 404
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.67646549969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.01225465371
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.32062527383
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.69334557542
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.17850045436
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.80870662788
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.71313916962
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.75)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.07431679566
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.40119016754
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.40)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.817360777919
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.80)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 405
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.152833251654
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.11)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.90150271145
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.93)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.47456084974
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.92001958237
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.32728040829
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46088791341
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.45)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.44320881208
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.49)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.86982227214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.66972327173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.70)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.10545943228
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.97625589313
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.310649119746
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.27)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.972804637658
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.97)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.67753927793
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.68)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.96862230681
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.98)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.12601563662
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.12)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.0884148432
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.07)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.72787093216
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.75)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 406
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.47930524499
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.778900337419
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.77)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.52367680227
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.96580931108
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.65532464638
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.68)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.749929921816
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00269985919
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.98)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.4570059134
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.28691021049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.94979853277
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.73960816885
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.98748700545
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.77543245132
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.795791217196
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.80)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.89336441056
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.89)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.04876348334
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.07)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.36764551911
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.37)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.24605067358
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.26)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.13876116293
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.14)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.676099183028
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.72)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 407
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.62678406818
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.65)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.63933577249
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42060557749
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43326344739
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.42)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.13957370814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.13)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 408
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65579595143
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.77537604163
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85364745735
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.08995903504
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.10)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53097024863
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.54)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.43490540561
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.43)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.886370679984
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.88)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.616578999057
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.58)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.955513296673
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.95)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-5.02327849467
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.18)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.57290432717
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.58)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.16011198989
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.13)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.29632792471
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.27)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.57448849027
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.54)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.728710068726
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.66)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.983849386316
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.30489113329
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.33)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.19160758287
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.22)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 409
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.13338402619
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.13)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11012034737
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.67999576527
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.72)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79299457389
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.84)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.22953940302
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.36763324428
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.40)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.35220637648
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.75267656547
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.79)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 410
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.75457401232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.78)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41289019871
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.39)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85895162702
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.89)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31317384136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.32)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.69586476796
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.76404152785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.75)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.79861975385
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.58246271321
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.58)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.17089551116
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.13)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.28785827015
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.53281458335
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.02168009007
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.98)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.18251607454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.57481848618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.59)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.76400492548
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.77)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.02067162748
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.44416457105
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 411
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.21689967293
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.99872303085
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66665584522
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.8096390525
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.17832796409
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.48043801714
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.51)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.76164983175
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.77)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.85643256331
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.908822053227
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.90)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.47967292807
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56184254218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.814132631555
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.78)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.72129022299
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.70)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 412
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.86298259339
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.87)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.21889555854
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51648855628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.14650241
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.19076337681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.3614848239
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.59043667357
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.36303072847
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.81502259334
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.86)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.18099257665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.16351299047
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.99474384359
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.98)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.224206409233
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.21)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.55542519587
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.57)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.59974816197
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.30546423149
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.59575944917
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.61)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.32546442941
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.36)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.23548491268
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.28)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.68051109712
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.65)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.27854450195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.28)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.58294965802
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.59)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.943652520823
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
2.27316624238
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.30)
31% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 413
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37167977058
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.30139161143
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.69353878125
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.26052966183
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.27)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11553915229
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.10)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.5047937522
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.51)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 414
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.14664437963
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.13)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.34903231854
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.62627233708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.76627807942
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.07736433434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.36077732023
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.39239345769
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.04144542323
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.40467330187
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.72624232277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.83520911343
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.36531797451
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 415
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.83496900174
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.85)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.40067801519
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.46018558316
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.27067024063
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87490772889
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.89184906341
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.0740786591394
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.35907870717
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.39)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.22158855056
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.19)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.33961824094
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.39)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.78637246746
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.78)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53527467653
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.55)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.30394277281
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.699570050147
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.68)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.169696146191
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.13)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.40533740927
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.42)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.29162701995
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.28)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-18.9080585193
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.49)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.05576168518
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.06)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
-0.42130943825
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded -0.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 416
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.190598616322
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.16)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7748816949
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82046975682
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.82)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.54472039685
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.46193352131
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.26371107689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24922740398
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.28)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.69058140226
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.96367417467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.97)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.17711594631
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.48953595152
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.56249399018
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.57)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 417
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.501143537
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.51)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.78582959582
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.83)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.44440382187
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.34252179804
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.35)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.88261338239
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.94)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.64408454839
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.57880957027
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.56)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08823420154
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.07)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 418
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.84905076368
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.85)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.688766608592
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.66)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.9733144751
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.85325663318
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.86)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.30756511677
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.33)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.227121686898
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.22)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.29794130377
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.31)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.18105690078
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.15)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.16589603286
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.42785813296
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.90650720185
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.874354153438
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.78832931855
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.78)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.901712481119
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.85)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.66937866555
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.29196399652
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 419
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.09012193022
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.11)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.3419836241
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.32)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.26168266826
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.86099707594
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.90)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 420
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.77690005626
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.42764773299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.85014198724
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79679397246
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84959484371
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.12265267593
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.58167844032
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.9676024532
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.08473849241
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.81958726375
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.37386256679
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.04771823354
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.33203122376
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.33)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.06739524546
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.01937730317
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.761155974174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.75)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.08036443949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 421
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.28532811088
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.64889994012
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.06944830504
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.31522764399
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36113422215
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.976852206491
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.02897451369
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.06)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.810488917301
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.79)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60936276674
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.84322610563
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.82)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.19132406811
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.20)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.35493487173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.97371792529
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.38006430202
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.38)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.19310887843
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.9118936723
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.92)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.02881587991
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.05)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.05015721001
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.497646832203
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.48)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.526360171106
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.49)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 422
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-39.6165847762
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.84)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.96666023839
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.94)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.68111239645
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.70)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.25842370766
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.27)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.38489499286
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.21153632565
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.20)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.67370108733
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.69)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.184451389
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.21)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00910566739
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.04)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.903275149248
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.92)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54682822231
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.59)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.84890465484
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.63857997435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.64)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 423
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.0987616263
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.10)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.60544613501
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.53757628323
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.47932514777
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.990067482168
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.96)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.0193455912
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.71168133104
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.75)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37439566122
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.36)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.87769888239
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.57201660284
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.59)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.24505430141
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.33779641839
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.3045928573
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.42106455657
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.55899241376
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.60)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.39747677265
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.14994894293
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.41289491309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 424
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.1742969647
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.16)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.1262722974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51760689608
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02262704067
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.76929920019
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.109909694781
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.07)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.51548866972
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.53)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.87569250997
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.89)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26733412526
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.32)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.15422459293
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.18)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.941216905651
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 0.90)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.76097949302
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.74502519153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.76)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.45639308771
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.45)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 425
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.09163728514
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.11)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81928014325
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.83)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.73579147711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.76)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.965408465
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.64871501186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64582771221
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38800773104
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.90369359879
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.92)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26043433998
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
74% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 426
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.00454894188
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.756828285028
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.663847428531
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.19424454752
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.75390541429
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.77)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.70317137983
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.06291668389
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.929986651081
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.938116913872
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.00294540184
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.01)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.52924342848
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.54)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.519677507383
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.50)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.18170832481
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.25783064995
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.21)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 427
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.36769745017
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.41892752618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.86013174815
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.90)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.71279200654
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.80562645545
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.48061387466
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.52)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.387552105739
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.40)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.0706849707728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.06)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.38762383107
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.25148278129
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54568292655
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.60316602402
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.66726177967
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.66)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.22927220976
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.88153819594
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 428
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.26342731422
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71505912792
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.1097134121
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.50332028494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.472147408882
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.99977579864
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.03)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52840725231
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.49)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.19919608323
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64807910115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.48442385176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.72257881175
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.70)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.63459195838
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.54881623171
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.54)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 429
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.607324631322
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.62)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26160954749
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.06542774006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.540534081121
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.54)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36314050468
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.75629526147
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.47176421404
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.50)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.31016090302
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.23684832288
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.7051417306
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-18.4354227591
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.01)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.72244283284
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.70)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.34828260861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.704509279309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.64)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 430
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.96572795883
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.02636329253
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.6640228435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.70)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.50087227331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.62785259835
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.73189076216
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.72)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62005959956
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.51472740886
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.49)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 431
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.71560332133
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-9.32625801056
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.02621740431
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.17223903158
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.33028273176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.42695324807
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-19.0063118982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.59)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.55559507465
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.56)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.25731390663
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.24)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.02724836777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.348795289203
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.32)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-0.100363264473
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded -0.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.718595196591
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.73)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.03607481156
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.62166355802
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.21305922829
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.64398331732
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.62)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.73976389274
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.79)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.722020513963
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.70)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.2663853218
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.11730335131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.697055025512
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.68)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.818827455385
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.79)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.707400214848
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.68)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 432
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.35634493141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.68966590237
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.86269082961
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.12110515511
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.11)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.88773653653
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.92)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.74056319741
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.42147963395
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.48641191683
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.86439503479
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.84)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.988993571268
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.98)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.7258884316
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.72)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 433
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.50435334818
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7747638141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.3791597577
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.74058956731
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.29520379192
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.87071522242
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.89)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.11396780733
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.13)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.48687135245
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.2061905189
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.23)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.19063559273
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.16)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 434
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.57754116574
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.36815472728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.38)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.74493377793
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.76)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.56371493076
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.24883216418
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.83172362857
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.82)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.842266985471
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.55950337311
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.54)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.820175310816
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.4826888824
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.52391750717
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.39573486184
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.5748498798
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.59)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 435
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.79916665848
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.82)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7188337442
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.72)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.34378373181
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.30)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.62926081653
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.967026525373
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.55502194271
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.56286665903
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.55)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.03115450582
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.06)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16770255777
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.013908988747
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.01)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.29134236957
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.25)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.42052575016
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.42)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.06615953965
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.08)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.5688053451
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.7164207575
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 436
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.615861145554
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.63)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.90024934983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.91)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.90372517057
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.91)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.76530177299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06199030571
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.32201953463
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.35)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.45163505319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.86932085825
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.6657692504
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.69)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.40801652267
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.44)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.36626701458
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.39)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.64332904538
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.63)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.994935243741
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.95)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.941294072969
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 437
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.98606982666
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.521414880845
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.6949607349
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-8.75528216262
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.03)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.04119852586
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.05)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46511618197
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34017465233
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48172120149
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.18498957171
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.20)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.42142860882
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.43)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.31426283524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.33)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.23744987969
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.12256163178
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.11)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.777859341507
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.75)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.08499885486
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.0470812277031
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.09)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.46581324499
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.44)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.15179662428
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.17)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.06503976157
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.06)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.48655948503
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 438
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22876359191
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07905788516
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.18956872337
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.43034780171
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.4356783992
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.41)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.16647421488
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.81325542455
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.87940145116
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.31248975609
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.32)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.8317190911
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.87)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.89306023379
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.50938149991
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.55)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 439
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.51885878978
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.52)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.01506088196
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.70924884683
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.69)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.4311200556
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.63250440379
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.62)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.59381958711
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.05085994671
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.38417107349
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.982003755997
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.97)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 440
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.35416590381
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80849832248
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.82)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.91776875722
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.94)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.17926151071
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.19)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.21753227687
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.24)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.14842198384
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.12)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.46584483325
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.84434333986
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.83)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.149850665819
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.14)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.72171225421
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.73)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.10422923011
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.08)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.932381894707
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.40453068464
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.43)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.33463801331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.29)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.84088440411
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.81)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.49821569671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.52)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.34837189184
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.37)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.61573147151
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.69502506335
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.71)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 441
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.43751449379
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09287315566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71745837116
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.89149306859
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.4136587186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30052175964
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.47915369745
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.52)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.47407271802
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.48)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77597231369
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.76)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.02426403743
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.00)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.52154803935
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.57)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.31067617726
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.28)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.1621551912
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 442
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37578410507
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.57597375843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.27260375481
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.81095813282
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.48260763138
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64454888515
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.95394709658
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.41375830492
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77991955453
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.77)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 443
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.926797049949
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.92)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.36770310536
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.36)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.74722532297
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5433589552
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08126095592
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.67461067608
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.96613557368
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.98)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.21067737587
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 444
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.75593355908
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.09878722605
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.11)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53943887521
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.57)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02954969887
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.01)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.84181752214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79295615932
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.78)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.46183191513
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.45)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.27857801079
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.26)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 445
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.04764054178
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.06)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.23294918154
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.18)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.00734068394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.99)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.66903916752
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79755275939
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.23045453454
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.85160359462
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.84)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.983202763786
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.93)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 446
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.31353010463
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.4175873635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.46)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.87386193763
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58426452293
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.88850617154
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32039373203
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.74375263597
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88235642406
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.87)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 447
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.03079555422
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.30035396668
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58455023127
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.78994698995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.35375346645
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61881007145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.15758298215
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.31920243018
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.34)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.35291153187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.36)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 448
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.06867841831
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.10)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.42895164964
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.44)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.88715706343
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.94)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.30795340942
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.31)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.70766074653
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.99246364168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.00)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.58235057112
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.62)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.76979553267
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.82)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20019053354
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.4499918957
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.49)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.34311465045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.34)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.68933000526
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.48653620442
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.969730328563
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.92)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-38.8763998137
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.08)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.90500160625
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.01456197087
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.00)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.60532290532
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.60)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.44741875109
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.706746882084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.68)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.97241095805
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.00)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.89464194637
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.90)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 449
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.60950984579
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.66)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80239484468
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.07672829301
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.21766303287
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.53224152838
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.81893820436
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.76974272665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.76)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.42967562595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.42)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.10097350797
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.21930233848
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.21)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.44926563957
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 450
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.11397281899
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.3583695724
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.8482070538
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.24980049437
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.173260153532
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.1075871045
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.76675649403
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.81)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.65537912578
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.66)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.958454258187
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.94)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.72774581438
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.73)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.8716916652
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.88)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.58524080413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.57161848316
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.753239997989
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.73)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.80314605002
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.78)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.89546677369
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.88)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.06565079535
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.02)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.12841610711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.13)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.77320902518
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.76)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.643430883579
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.60)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 451
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.21922428609
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.90811076289
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.96)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.27254277497
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.23549520136
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.12772616151
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.09)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32654123413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.51861340894
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.51)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.374206211359
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.37)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.44240682477
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.43762069391
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.79365878859
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.82)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.14539439061
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.20783917049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.20)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.05921498551
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.21843586667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.49129654817
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.50)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.56115777099
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.54)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.62642368917
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.64)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 452
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.77065501242
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.50489424154
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.51)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.47942608661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.88521967354
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.93890533142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.04601621348
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.93333110687
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.623365476281
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.59)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.13632899949
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.10)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.43109112006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.42)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.993535066905
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.98)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.75967903599
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.78)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.31277736027
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.32)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.53931923601
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.57)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 453
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.453899521675
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.44)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.43311782073
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4145592153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.88716031899
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.20832975444
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.20)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.19132270521
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.79362609666
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.56497802515
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.57)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.71472782642
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.920860563
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.11831245664
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.13)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.02386992801
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.56692597049
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.58)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.44292525642
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.45)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.99855898854
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.930533993812
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.90)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.746887771806
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.76)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.33319636134
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.33)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.662391921672
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.65)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.53523903269
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.57)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
-0.0312273626482
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded -0.08)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.82189829728
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.48337271668
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.45)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.44637402754
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.224780680024
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.23)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 454
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.52159403985
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.53)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.62939992041
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.20443090295
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.45297788766
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0552798534
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.397983172
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.38)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.32230480589
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.1814591384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.47885087253
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.49)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.24278587018
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.17115553792
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.12)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 455
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.837457566742
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.80)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26153637711
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.25)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.09375704093
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.10)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.47625631764
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04208802277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.92169558439
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73604470783
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.0335681704
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.24969834047
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.36352265397
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.54657018935
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.57)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.56577759405
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.60)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.60788275237
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.61)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.9510773846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.20251892486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.9674375252
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.60954566169
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.63638983124
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.66)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.02154245719
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.00632117131
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 456
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.69790848235
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.68)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.31438044971
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.38134390857
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31699189201
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1098181475
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.70108398182
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.66569879327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.68)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.59532787387
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.82419024249
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.63884731394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.79462407321
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.79)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 457
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.21978895401
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.73117988547
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.76)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.2180632855
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.81586922947
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.4883794927
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.40775975548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.45)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.94066027708
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.98)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.44997615914
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3124962878
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.35)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.351659631929
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.36)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0849955667618
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.10)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.0971276341413
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.07)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.33751925534
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.31)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.57644654498
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.59)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.50685207072
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.51)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.86158999945
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.81)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.45654104082
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.672736726956
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.66)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.68636530264
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.69)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 458
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.648922532709
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.64)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.508867374905
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.49)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.63086436627
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.65)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20401652144
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.15)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51245319905
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46518600101
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.15556069478
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.54953029465
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20856724947
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.72943663949
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.78)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.19299409407
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.658462538983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.63)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.36269467708
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.35)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 459
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.63926127158
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.68)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11501866557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.11)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.10310552924
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5540723271
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65354763961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.44686027963
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28249195706
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.70945844549
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.151837812
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.62178773851
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.63)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.06924675488
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.05)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.70263626429
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.68)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 460
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.02404368485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.77421276677
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31519351076
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.17455030932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.59672754415
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.09979127161
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.11)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.66059365554
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.67)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.70682961643
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.18810328521
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.18)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.53530445592
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.53)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.05877472594
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.984047674551
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.93)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.04384554409
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.978155044524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.96)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 461
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.09683283748
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.04374124157
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.68090782638
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.621351745
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.31431905567
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46427901609
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.18901272262
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.18)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.991490136814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26706268614
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.08683817711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.07)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.5156057399
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.49)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.17770148658
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.60562580033
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.62)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.36475967684
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.32)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.166519244622
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.13)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.927545516545
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.47188122872
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.47)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.93045135145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.96)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.31596392396
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.30)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.15905396174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.34719088875
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.55154452472
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 462
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.91433452301
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.94)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.189304827651
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.65662067131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.82535915991
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.7661614884
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.76)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.41426239357
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.50763038891
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.50)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.203659499692
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.20)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.61065706935
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.65)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.52326341171
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.55)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-20.1744046392
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.80)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.10871801038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.4555358057
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.43835919151
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.43)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.59003432378
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.07947311332
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.427321588
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.41129680925
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.41)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.536427353
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.858785161442
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 463
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.94431461643
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.97)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29303372271
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.29)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.04815292993
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.01)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.52308492363
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.53)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 464
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.3078755663
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29319244294
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.29)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.774669007138
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.79)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29838836487
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.27)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.95896508125
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.97)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.25972933197
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.67997560237
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.971949961515
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.81830443042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.81)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.5346232991
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.58)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.866074653281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.86)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 465
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.97943934685
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.98)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.48484364394
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.79238040518
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.74846257202
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.75)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.0735202341885
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.25861794219
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.28)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.75420141861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.2627668201
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.37942578093
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.40)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.61570386715
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.66)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.22766895303
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.64079624713
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.45368727372
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 466
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.490076244573
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.8507056616
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.09402716674
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.11)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.30069379714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.18717631698
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.776941869
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38008725454
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 467
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.52383826898
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.02802134755
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.02)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4785607135
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.90575301142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58353846064
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.89520409843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.04966412665
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.00)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.16843818469
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20124888196
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.21)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.65871840605
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.23360829516
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.822753311828
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.78)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 468
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20166360769
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.18)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09110940805
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.09)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.22683592382
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79202462366
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.81)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.57176223715
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.58)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.76821807468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.62147060093
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.64)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.40404123652
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.38)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07886080241
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.47617090624
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 469
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.41106088578
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.45)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2462539458
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.44116899397
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.01801995171
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71526846732
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27354599071
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.510938355498
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.73837540108
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.79)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.25312393191
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.25)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.56962874725
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.55)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.34553565008
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.32)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53679776059
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.52)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.22140488391
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.914468000506
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-39.0581337131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.27)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.48615949038
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.44292884475
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.402509916696
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.41)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.57707598606
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.47815818813
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.52)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.45313449368
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.45)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.76636368543
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.11071857456
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.14)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.69433918535
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.67)
31% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 470
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.52609183589
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.53)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.34570552131
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 0.32)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.464952533446
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.3293396621
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.32)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.92527870898
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.24868692077
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.6930956485
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.63200996715
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.95468656706
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.94)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.758184200251
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.845610106939
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.84)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.85312715588
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.84)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 471
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.77442431467
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09790138625
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.3942490681
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.82308345177
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03277694016
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.93500141695
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.4753549357
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 472
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.03131106323
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.01)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.49308109574
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.73449746629
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.77)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.33175997403
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.34)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04096224319
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.99)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.49931739007
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.50066058397
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.55)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.41844569171
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.34481756929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.18162043342
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.16)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.75867927323
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.77)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.31331596691
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.30)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 473
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.75282866102
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.93471546772
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.94)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.46912081274
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.32857045094
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.75875401087
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.22017231843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.7464386814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.949520166654
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.16953286579
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.17)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.01005471468
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.99)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 474
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54752650459
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.51)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70091746466
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.71)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.56074666993
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.97632310069
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.01)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85003549583
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.91)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 475
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.81876540612
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.85)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.61729920535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.68565200168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02484989965
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.63839366475
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.56005329726
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.26606526588
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.27)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.50294462613
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 476
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54885887833
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.56)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.77604107569
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59375395236
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02343527404
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.02125224602
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.25939149023
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13055279673
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.09)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.09054814628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.35386662743
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.60869437022
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.58230323452
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.70262208924
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.80656380309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.83)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.915807930781
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.87)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.33409127454
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.35)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.03816639016
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.02)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.65296561998
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.64)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.730852704999
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.71)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.24426068094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.22)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.78422212543
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.76)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 477
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.46171061988
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.46)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.98816849142
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.33461436383
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.44605051245
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.43)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.28604607016
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.13469883832
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.16)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.64666237614
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.64)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.12822729377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.11)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.979754928755
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.42293664007
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.46)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.4035131461
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.986708681549
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.47344482443
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.37570911059
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 478
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.34541266692
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.29208740897
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.44832306552
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.57820787477
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.85239579261
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.3417704624
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.34)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.43935486182
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.43)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.96158527938
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.96)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.54175233896
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.54)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.33739466443
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.36)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.19671711666
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.18)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 479
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.200677375926
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.21)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.6580756788
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.67)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.31113217114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39473599724
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.66040564042
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.19236158785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.19)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.553002926024
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.178188852955
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.17)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.2584445599
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.22)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.56755439741
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.61)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.52171067807
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.55892530282
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.57)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.95849411926
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.94)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.49959474372
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.51)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.19766161311
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.19)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.588339216866
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.59)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.7862206765
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.81)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.824196410054
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.81)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.08258146534
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.09)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.14943817172
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.17)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.17708298759
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.20)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.930937719716
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.92)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.60205832409
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.1522690444
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.15)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.96742152576
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.96)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.87286707359
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.90)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
-0.0800099370437
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded -0.13)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.765009759401
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.74)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 480
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.27874955553
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.03380401056
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.04)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.83606946761
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.20337016629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.21)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 481
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.36028492023
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.32)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.25655678102
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.61103997923
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39074930628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.19032253981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.33461378357
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75791105393
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.99599656315
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.00)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.50463413045
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.69446101175
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.70)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.34966151949
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.33)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.824437790718
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.80)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.41413797236
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.41)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.3060041515
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.32)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.06876715682
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.875282242581
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.84)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.715965282337
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.70)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.839798359939
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.84)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.456215424357
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.42)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.842655001398
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.83)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 482
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.33811583843
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07865677623
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.86932094922
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26136154646
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.30)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.59480040861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.64289660231
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.17396487609
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.13231340266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.13)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.85649762769
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.2593710297
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.28)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.923081554119
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.89)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.23189856047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.22)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 483
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11126372265
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.11)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.45967372074
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.34727860324
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.36)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.80330754508
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.56721066962
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.41532000432
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.35479741002
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.38)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.39760614995
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.34199496647
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.16347221451
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.14)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.69784454362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.89664756886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.87)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.12404281475
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.8483710046
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.82)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 484
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.74278476909
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.02468316639
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.94577143486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.7226604645
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.47360243992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.43)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.57116224556
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.55)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.80442026926
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.83)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41788588581
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.45)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.995052042184
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.00)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.22878380063
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.27019178418
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.31)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 485
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-4.19752352939
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-19.6448442511
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.203297102693
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.67123015917
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.09016304684
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.10)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.10037189467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.12)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.09050435221
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.06)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.68248312551
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.72)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06047866447
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.7788402046
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.80)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.909837669512
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.87)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.07014036731
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.26871164033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.84985959335
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.909631333725
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.93)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.98972269265
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.98)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.57742687302
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.653953873119
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.60)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 486
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.03453145063
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.50129659743
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.34030893087
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.40529284082
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.63354889654
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.38945398761
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.36067018312
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.02588578046
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.836295104
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.84)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.363312648903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.32)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.15548311864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.14)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.19949209324
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-9.45602891774
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.75)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.844218080507
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.80)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.31449394043
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.35)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.21925257424
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.81004111905
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.145765862254
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.09)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-3.89150355007
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.01)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.07383665717
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.09)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 487
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.74741008973
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22789143949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.23)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.55529144434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.981545695216
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51652578544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.9966370001
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.00)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 488
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.86759769007
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.26012091093
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.31610621573
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.06583629749
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.30906760902
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.91382084689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23410089886
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.20)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.525678307793
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.54)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.77820148696
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.53970623845
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.59)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.24638414403
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.18345870512
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.17)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.42630438601
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.9446179721
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.93)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.2759153732
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.48325826334
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 489
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.35390276946
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26727922102
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.27125763865
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.52205932741
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.65471436403
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.36152148402
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.60900351116
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62274372323
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.67)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.365848469036
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.34)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.00475171293
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.49672075296
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.2175293717
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.22)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.37310068696
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0592257673923
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.07)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.24252759315
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.18407674348
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.73135220564
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.28925828365
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.25)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.24839676137
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.28)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.173547596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.94208027444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.82086845865
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.70592855664
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.68)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 490
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.59551687641
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.52640593925
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17834912964
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.14)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.07871006791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.66912829422
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.51564695941
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.55)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.26638568501
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.25)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 491
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.54882001277
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.56)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.14997596474
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.77045230476
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.91987235363
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.8508453763
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.5180833528
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.57)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45224716813
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.45)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 492
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.07049431175
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.04)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.31248513585
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.75243593849
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.03459979168
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.24878108591
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.95767846828
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.93)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28524603346
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37645655723
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.19455911991
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.41394884628
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.56737326854
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.56)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.4304333358
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.993987865494
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.78807305888
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.41381290811
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.45)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.01933425319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.70171610123
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.68)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.5420317215
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 493
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.818352282605
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.81)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.01383096251
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.00)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71621375217
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.63052849979
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.62)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.68237724834
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.71)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8602822561
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.72217053038
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.73)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.41640919955
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.43)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44217393768
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.42)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.39399221269
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.67370488618
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.20396090671
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 494
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.03637196593
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.34052133918
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.7557811694
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.77)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11464404784
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87232497778
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.47432703473
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.6132447737
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.53138003053
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.98777454768
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.2665753595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.29)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.24945049805
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.23)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.60135602815
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.90379155361
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.91)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 495
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.69041210821
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.77)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.97811118858
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.98)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.06584416963
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.20672132173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.49076376747
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.21162728992
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.18)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 496
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.18718198464
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.263066522352
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.24)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17598932718
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11324775722
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.49178846173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80517807227
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.03675309118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.63499527665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.67527711706
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.992698122597
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.94)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.33041932576
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.35)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.56947762362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.53)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.00806999242
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.04)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 497
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.141198871998
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.13)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11101350411
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.77762332048
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.79)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.42711729482
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.41)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.16594070601
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.61021135284
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.04518680823
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.984862280141
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.97)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.18724343163
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.43640328037
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.44)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.21705428061
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.25)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.36168511341
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.36)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.04914054419
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.08)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.922150583185
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.95)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.803144081224
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.81)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.739618982141
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.69)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.57866799534
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.62)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.196081015669
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.19)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.961142880676
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.92)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.18558315509
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.18)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.740451072768
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.74)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.84217360051
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.84)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.630346320091
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.60)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 498
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.49860371237
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.51)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.86084400355
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.87)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4251720974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26415781905
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.61009174426
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.96616219323
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43371177393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.94064610981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.17318489669
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.17)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.84891182919
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.91)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.03596801822
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.06)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.81101477245
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.80)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.54194155547
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.60390572504
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.59)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.72280154102
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.083032317735
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.10)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.16749509877
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.17)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.88630681378
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.89)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.71388195541
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.72)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.70579251555
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.51478679821
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.8271542938
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.796830479749
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.919858608056
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.91)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.76116620764
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.79)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.35355475873
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.34)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.691939747326
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.64)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.610465282711
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.56)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
1.84255929553
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.85)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
2.12043562546
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

learned value
0.590551880963
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.57)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

learned value
1.20649888168
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

learned value
1.73305521546
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.74)
6% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 499
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.44730138175
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.05614640072
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.12)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71254520682
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.77294985756
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.77)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.14083783599
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.14)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80833758113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.73137827331
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.78)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.71563284782
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.80)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.30559176262
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.29)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.70958929231
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.76)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.35487819944
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.74941817264
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.76)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-0.0543207882858
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.09)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.15604879362
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.13)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.373044340025
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.36)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.917461378895
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.91)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.75351234197
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.76)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.72081681426
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.72)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.32938788036
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.38)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.30387913315
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.29)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.79477820057
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 0.77)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 500
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.33683796217
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-4.66456588527
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.19881504619
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51046312617
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72215405756
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.592273605409
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.59)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.54153259813
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.57)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 501
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5566852539
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.36381061349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.93862559566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.98)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.83169068138
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.88)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.15805558026
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.92435386655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.84815118195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.59350855362
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.29948330416
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.29)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.88616863022
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.64973529373
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.66)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.19895328603
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.23)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.07096344074
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.10)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.51333065465
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.55)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 502
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.45154244241
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.3998967856
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.42)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43564348107
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.40)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60522920003
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.53189679942
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.6561208578
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.761050599757
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.12797848217
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.12)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.58991223584
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.80613445135
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.38147064513
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.40)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.66034256806
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.27564498418
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.97981993294
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.590056087664
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.58)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.74750292164
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.76)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.08848808334
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.07)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.83237072898
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.81)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.4369168093
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.49)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.00045036498
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.96)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 503
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11161164134
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.38588480499
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.37)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51539247949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.57997633155
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.37867175871
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.55838481422
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.52)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.73233213565
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.73)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.31511812075
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.45443024581
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.44240508425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54199711655
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.97022000983
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.99)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 504
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11670862208
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.48038754444
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.46)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.38468279304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.58785801491
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.73815123185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.77)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.41194851587
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.297630092
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.71165783524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.33044845977
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.30886029993
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.2821474638
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-0.0722142303814
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded -0.12)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.30995387546
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.49725801439
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.52)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.927238995785
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.89)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.273882394817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.27)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.16947899166
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.16)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.713908575796
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.66)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.416323828084
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.36)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.34695747351
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 505
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.04159388945
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67402999088
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.69)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66477484624
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93751192979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.19921920646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.77031360784
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.77)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.59635110037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.12057547688
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.13)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.76636044019
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.78)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 506
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.24192875596
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.26)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.26362397427
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60675149992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.52290445968
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.97025342808
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.57481228975
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 507
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.70381310505
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.82801285261
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.86)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.30011751567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.4094811657
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.14097870514
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.70226012391
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.70)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45369666536
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.48)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.36799667952
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.58186935193
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.4496752257
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.47)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.1014499182
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.803382852605
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.76)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.33745511107
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.677790724529
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.70)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.663671484492
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.60)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-19.8238755984
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.44)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.31983291275
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.37)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.514218720772
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.46)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.59994379994
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.879791131232
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.82)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 508
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.80185157995
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.86357057787
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.84)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.27739462691
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43281891406
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.90647187992
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.92)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.24135475315
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.5767496761
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.54)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.72820247685
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.79)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16550613169
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.28052811434
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.04713820091
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.18434381438
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.16)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.868937120087
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.87)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.99602081326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 509
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.77004650537
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.0359926922
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.84409842184
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51247437507
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.81954182946
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.23096514727
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.68787561748
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.57268063435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.59)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.45360496265
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.66645654333
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.02678291804
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.01)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.44792559378
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.46)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.4307004887
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.83485832091
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.83592819276
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.81)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.40807656639
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.61996251409
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.63)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 510
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.08189578681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15672774147
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.16)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.11147822934
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.08)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20531663199
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.40839154869
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.15793380099
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.13)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.32963320323
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.480523757244
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.49)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.01462314399
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.96)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.829861420659
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.82)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.50233517843
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.52133099171
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.54)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 511
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.926531996741
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.92)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.85334899965
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.89)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.78754009902
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.0768654555
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.05)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 512
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.5510596433
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.58715266798
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.58)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.01688169108
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.99)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5947748063
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.57)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 513
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.21877579601
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.38786911382
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.40)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22858519344
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.92204527701
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.93)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.64210195015
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.63)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.31013212261
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.2671315605
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.30)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.25246322832
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.30)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.83196063582
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.13860403646
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.12)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.2634712611
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.30)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.73633976572
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 514
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.47452030995
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.48)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.3397311366
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.39)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.97925228931
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.61888118396
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.04526224842
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.03)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.50478398934
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.50)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.29920164394
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.32)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.5148317112
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.39186688361
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.515676152738
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.51)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.49516716287
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.925405540107
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.94)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.64423447139
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.24568556854
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.23)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.42774256405
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.43)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.396249076624
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.40)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.45137645274
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.00216817723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 0.97)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.23426884603
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.48550224011
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.49)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.07601228029
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.658082662296
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.61)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.544506717425
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.52)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 515
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.91655420463
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.56112090042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56978753986
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.31446711132
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.01908586066
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27730311801
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.30)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.71663924466
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.73756952397
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 516
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.42739537334
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.24643854037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.20)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31765283111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09392178981
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34072601327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.12358823394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.49984849374
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.65291100544
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.63)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 517
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.2574515529
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.25)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.79443728326
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.36059802703
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.91775664898
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.93)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.05843680732
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.56481741331
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.6560905511
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.72)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.43219424335
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.44)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.45036697086
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.42)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07543510342
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.04)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.987495928846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.96)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.38361535403
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.966818269287
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.66820129464
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.67)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.52246389996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.53)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.1640831437
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.371621115
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.42274055756
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.46)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.49079397823
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.18945735402
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.17)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.62453481309
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.63)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.620445028765
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.61)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.747453167785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.70)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 518
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.8739917714
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.93)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95704439442
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.47642204277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.84705559417
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.86)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.83737487854
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72927419261
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45456138658
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.48)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.44417029681
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.0764040118
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.08)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.52766294068
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.53)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.877877682574
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.88)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.68656199158
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.38956451241
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.39)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 519
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.344923781913
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.74429535027
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.03462996035
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.01846574632
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53327863739
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.3379149178
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.40409901391
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.24089358167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.25)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 520
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.88864704403
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.93)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23207511524
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42303231597
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.3865545225
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.74038304048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46678187063
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.1005008058
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.09)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.07075453194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.24801698628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.60303506512
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.87751720542
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.48262179131
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.50)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.10341179705
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.10)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.40642672385
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.37)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.41246717282
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.44)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.777231023906
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.76)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.47539327843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.52)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 521
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.2144381206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.65458082511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.17094949179
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.24066120571
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.98986333491
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.97)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.00146768021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62742463832
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.69991707919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.72)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.04881921778
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.64516640851
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 522
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.93193429481
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.96)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.68818140516
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.73327330176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.89510952531
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.94)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.22955569996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.27)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80587510058
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.26561873049
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.23)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.28560170814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.29)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.93073729841
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.88)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.28112807169
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.29)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.40054579363
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.37)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 523
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.972243034371
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40074246148
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.45)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.30080132204
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.26)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.19898013649
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.81869138681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.0213777304
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.54031839836
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.55)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.34611359657
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.05850224141
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.877532850856
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.9347456584
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 524
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.40108771984
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.45)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.434478429
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.20419007798
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93170300849
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.94)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.17073926309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.12)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11610208194
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.13)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.5023912843
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.50)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.317255679498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.29)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.65894590575
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.67)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.66765080499
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.26962892931
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.26)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 525
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.87265906965
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-5.24340094354
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.84001069173
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29293372187
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.48556534736
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.52)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75329324812
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.946394582423
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.95)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.5651421054
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.60)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.18723545516
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.08257320867
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-38.4725925335
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.66)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.02951297324
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.00)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 526
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.00886839735
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80201965774
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.83)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94525082497
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.93)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.27862313554
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.34129754806
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.17717773773
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.14)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 527
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.73718854032
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.79558054886
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.82)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.54193874347
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.52)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.2294275159
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.21)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83732758192
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.81)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 528
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.64118544294
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.66)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.26948194589
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.24)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.99110829034
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.97)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.15580115883
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.15)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.63897272278
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.66)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64238763313
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.63)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.41968262389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.46)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.49527364673
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.48)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.51361478422
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.50)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.70760170602
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.74)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 529
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.08342896655
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.05)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.73238224074
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.28916583185
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.106029814292
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.10)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.76649816301
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.77)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.44093846237
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.46)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.58460137493
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.40535838733
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.41)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16480088867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.12119351875
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.13)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.19008159301
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.15186503091
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.8741042338
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.90)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 530
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.36535201045
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.40)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.6748128917
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.90872025212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.95)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.3292294942
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.33)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.251012287801
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.24)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27540599573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.00717457159
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.17416800147
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.21)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.24862328972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.55253119038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.56)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.02387137037
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.75363262362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.80)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.53666891134
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.53)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.07233522929
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 531
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.64489383334
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29340181788
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66543266594
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.48951925735
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.964346368849
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.95)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.24555059124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.25)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.70368852814
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.72)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.29772750791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.28)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.66336696494
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.22315272472
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.05660439789
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.996438352124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.02)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.292463011704
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.41410727734
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.41)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.21118136801
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.23)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.12798269597
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.32128072902
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.36)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.791549178515
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.73)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.318168824239
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.24)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.871647824567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.86)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 532
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.60730529104
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.975655057845
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.63577520943
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.92494298734
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58769258102
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.38946533419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.22926590227
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.5449130748
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.56)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 533
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.70889484747
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.75)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.02333384155
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.05)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.17017621025
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.20)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.58232734756
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.63)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.73351747382
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.15519938311
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.44659031245
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.46)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.949047572815
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.93)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.46318185631
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.15508241495
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.943672429445
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.91)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.75094690352
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.77)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.19830791133
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.18)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.12363077621
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.08)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.05117678016
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.08)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 534
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.26694239892
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.31)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.72002627067
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.82560493711
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.00316912395
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6163561173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.75292301728
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34317467116
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.41753625273
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.36397984047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.35)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.53996789228
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.55)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.41290157324
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.01791603595
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.10415718884
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.54408958103
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.58)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.7839901627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.79)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 535
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.3838789992
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80095398764
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51987692278
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.52)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.70209056769
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.17036373312
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61401790926
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.62)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.21533567865
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.902016186053
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.88)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.50344632105
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.5148270893
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.89595518626
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.16014552358
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.19)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 536
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.76432261025
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.45029756962
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.69914286644
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.27746038966
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.20280788987
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.08071241999
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.52721052176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.76696920656
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.78)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26362067899
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.64896755346
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.55859077194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.55)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.97296359204
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.97)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.802095647048
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.76)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.838179078315
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.82)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 537
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.26991959635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.00502290117
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.99)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.16416466556
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.19844705608
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.15452534517
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.17)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.38129128105
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.42)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13445330909
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.11)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 538
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.12054618791
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.10)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.66179126335
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.63)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.24564977504
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.06500157434
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.06)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.44574828167
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.45)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.84303591826
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.74914602453
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.76)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.80799836055
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.81)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-4.77444392054
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.92)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.95910255321
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.62131249014
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.66)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.30830945877
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.25211436353
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.27128115421
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.31)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.0511972154416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded -0.08)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.37730032272
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.39)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.445806514714
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.46)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.21527328248
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.25)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.259957084412
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.23)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.45078225183
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 539
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.19818047957
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.31431578177
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.40961678761
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.58216401114
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.70331512788
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.87311782216
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.28127040246
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.25)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.16571711872
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.13)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64303361024
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.43267550433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.46)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4092976284
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.36)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.00711087117
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.00)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.54947566887
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.57)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.853722665973
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.84)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 540
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.470330469308
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.46)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.61688473551
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.62)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.29627639173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.30)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.21280417158
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.57011217255
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.61)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.51344149185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13309438134
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.16)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.59477243879
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.57)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.44142528854
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.47)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.10923119758
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.08)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.11431813651
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.09)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.401543033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.19786624627
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.29182996155
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 541
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.57753337447
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.59)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.85012750017
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.90)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.37813250039
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.38)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39348437013
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.41)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19866363885
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.269130622636
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.10581378397
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.09)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.2746009582
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.24)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.69303935626
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.20439805666
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.20)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.22491219092
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.29)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 542
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.35741015904
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.12667481252
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.49084831812
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.49)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.14227517011
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.10)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.85990054289
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.88)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11548207883
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.29960625089
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.34)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26797910384
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.27)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.07767500576
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.41165731533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.97484697496
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.55924423332
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.32046154837
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.35)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.10008609272
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.12)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 543
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.45928631225
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.47)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.63026736686
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.7258537066
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.73)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21848974809
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.4719361
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.997003008952
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.19047746251
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.970738973448
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.92)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.12570489512
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.12)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 544
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.85238963423
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.44576096322
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.2335288758
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.21)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.1826970171
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.21)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.72479297561
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.56460900486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.60)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73037030554
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.62589785103
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.66)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.24713421642
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.25)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.85294188416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0955167868094
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.05)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.57371530523
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.49129550306
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.52)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.09609279986
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.10)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.11991802925
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.09)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 545
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.27326387229
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.24)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.28753141906
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.26)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.14093019206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.09)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.7147356482
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.68)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.93277143572
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.94)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.7868392262
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.84)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.33974553238
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.35)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 546
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.64437934529
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.61335575593
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.12333962706
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.27703569534
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.31)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.74044182776
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.78)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.76249133777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.59957771969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48941932716
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.6855159793
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.08889395696
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.60364505323
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.61)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.56162702284
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.06676836152
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.29506652827
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.43745338172
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.43)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.08481745975
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.52150543414
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.52)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.56684106739
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.95683808093
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.94)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.77353532936
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.74)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 547
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.06143029574
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.05)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.96425216321
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.95)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.79770871103
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36134058444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.80294655553
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08124058785
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82159235462
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 548
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.05269524136
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.03586987737
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.68810264718
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43916643302
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.43)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.77051564155
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.92917978124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.96)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.60450462375
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.78852316792
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.52046328741
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.51)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.9988450928
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.97)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.29315469848
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.27)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.45705663032
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.05527501492
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.46478906984
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 549
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.6182330721
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.62)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.50444798068
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.50)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.20413151445
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.20)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.0881811269
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.39953441184
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.41)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.39521265359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.0614184213
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.42250339725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.43)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.49726297154
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.42963620766
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.36212407539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.27783740377
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.89726002544
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.87)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.94153255458
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.49985237202
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.72611153484
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.03011385209
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.0268873947
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.99)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.37573834665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.2945389488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.28)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 550
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20806857978
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.27836879803
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.33)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.4196079601
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.48923175008
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.8007597274
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.02340843012
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23842237381
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.25)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41009767958
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 2.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.10914679588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.90165996804
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.94454249506
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.96)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.71347153852
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.76)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.12099616317
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.11)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.54856095043
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.57)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.893575826832
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.86)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.48680922427
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.50782139387
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.47)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.48456499248
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.51)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 551
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.0570173848
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11165217465
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59043532021
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.31458196812
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.67401713957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.40439913463
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.05395834011
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.05125843091
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.05448353891
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.05)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 552
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.532174489276
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.469483170162
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.46)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.87700622774
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.86)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.40371546673
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.41)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.70445955872
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.71)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.10549263981
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.91305819585
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.91)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.43657146935
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.46209725115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.80592685998
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.09280686999
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.55428412995
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.57)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.61123266117
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 553
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.15241496689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.55171045096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46485037218
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.45)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93489218114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.96)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.26169769773
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79336711617
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86584804982
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.90)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.11374060109
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.14)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.21106415224
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.22812025922
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.23)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 554
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.7703510926
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.80)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.17216644328
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.15)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.40243551565
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.47599892162
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.7682098676
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.78)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.41237819761
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 555
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65098787968
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.14606303204
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.92102061488
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.03681887779
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.98)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.37132710327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.39)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.47780804627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.60554845404
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.58)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 556
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.68417581921
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.69)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.13705751993
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.825944919565
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.81)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43221129261
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06974301787
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.49259449762
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.49)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.83793791107
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.86)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.77709616674
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.617349817916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.60)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
-0.0670806647148
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.12)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.36556053586
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.38)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 557
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.65786207448
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87072269923
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.45227339357
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.53424126121
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.24445249662
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.82365186302
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.39593776728
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.44)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.84823516495
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.87050563413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.919778433257
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.89)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.64265723944
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.69)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.18919241705
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 558
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.63524396382
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65911398241
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.65)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.63509858582
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.39417862387
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.44)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.52229529476
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.53)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.4407428039
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.55184132173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.69917502234
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.68)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.48372084429
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.45)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.52636125851
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.51)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.8684076389
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.41449656772
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.41)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.89756488173
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.91)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.31943074966
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.35)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 559
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.766763875916
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.77)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.76148693178
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61713820745
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79051577533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.46908428137
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.35158590913
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.35)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.19800704176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.73782346704
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.15015100095
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.15)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 560
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.17277242742
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.03494224999
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.85527144111
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15284405478
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03238073056
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72209363403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.1238955692
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.14)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.03501017469
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.01)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16673860979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.47922382124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.27890812999
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.27)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.542980836
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.64745144383
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.229149073521
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.20)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.91016104634
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.94)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.01897504838
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.05)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.55748299104
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.07406122325
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.06)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 561
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.78495143819
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.82)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.37890505972
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.68464622536
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.11944704691
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.10)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.68407878525
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-19.2520337542
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.85)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.0477370299247
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded -0.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.89575331862
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.88)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71725671624
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.26444939399
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.28)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.91143033977
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.89)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.51119487289
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.2602896727
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.737620446365
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.69)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.27871795846
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.30)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 562
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.796644646207
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.77)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74490824192
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.73)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31136823852
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.48714449522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.49)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.77497067994
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.75)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.33095233499
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.55325331441
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.06300749002
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.03)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 563
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.95130969723
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.78109816927
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.76)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.98412139673
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.4971301409
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.96202524375
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.90909106211
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.455226818155
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.46)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.45433063488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77721858736
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.37236422102
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.40)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.76076726687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.5909278382
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.65724466467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.97116461139
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.99)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 564
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.21094744446
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.91587255703
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31879794677
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.45837037109
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.02294034576
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11781693295
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65449154966
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.39042594526
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.40)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.28141189289
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.93206294825
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.94)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 565
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11354957681
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.10)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83915953875
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.01298316119
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.85255110026
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.64607471024
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.00144156258
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.956362345835
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.27674934419
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.26)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.71313081115
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.24316611596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.880304543834
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.84)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 566
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.2447418622
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41361506042
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59782511649
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.45286212021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.45)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 567
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.71019981527
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.40816245305
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.40)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.50493510686
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.32400988003
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.59060344119
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.63)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.77670877452
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.79069020145
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.0895642188374
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.37946522324
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.41)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.235399345129
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.19)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.986808070941
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.96)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.71234095028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.2753586919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.1655069372
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.15)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.65234710446
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.66)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.24335654612
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.23)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.01046980297
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.20423318194
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.24)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.833926335032
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.81)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.315706446049
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.27)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 568
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.93693310423
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.96)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.570126766849
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.58)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.22896220937
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.23023506176
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.78959594484
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.79)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.41437998673
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.21422724601
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-0.0307690392408
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.09)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.2585031436
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.29)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.34250945181
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.38)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.31599687051
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.16620192698
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.79623552317
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.06272558193
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.09)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.94867342079
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.95)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.721858683117
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.70)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.09284213209
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.06)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.69207858717
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.65)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.75050283609
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.77)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.272866021296
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.22)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 569
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-39.0604549956
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.27)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.57982931985
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.60)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31819367577
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.31)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43116757238
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.46511443142
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.47)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.3987975733
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.69658593461
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.993974710275
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.09840369719
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 570
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.47343295477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.27569619449
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49610643037
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.8663632886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.87)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.77589000841
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.78)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32667369764
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.4694602752
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.50609122978
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26566601315
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.25)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.01522526197
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.35975906503
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.32)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.8355841695
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.83)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 571
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.84362267018
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.35456275141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42350969249
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.23920876925
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.482855483561
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.997001842896
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.36930953408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.71278276577
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.63780919902
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.46190945822
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.66325047794
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.72)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.13142549897
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.87915474587
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.26122137953
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.25)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.90175728177
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.93)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.04817551827
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.02)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.551736179444
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 0.54)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.26917616546
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.26)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.335081295991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.33)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.905500174805
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 572
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.34651727283
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.020390398
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.52134611959
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.7229728268
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.46442227142
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75845253607
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64519140068
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.76149331837
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.972008743385
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.96)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.21587239096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.24)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 573
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.51156466091
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35974476098
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.40176874381
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.48133745779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.63440817803
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.65)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.94955620344
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.00)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.25829493862
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.26)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.10340775242
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.64739185009
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.67)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.948083684085
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.92)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.87952473051
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.68164981936
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.22533497999
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.22)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.52512656116
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.53)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 574
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.50762427197
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.55)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.93355354804
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.96)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.44198587713
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60085746688
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.45737111031
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.44)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.73155594812
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.77)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.16853236361
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.12)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88218938404
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03567000369
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07023222889
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 575
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72589052234
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.85363488142
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.11679885167
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.10)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.12050154993
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.45727772839
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.88359331804
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.86)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.82023168043
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.87)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 576
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.50569825031
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.54)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07042300749
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.09)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.16826350961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.604510378579
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.59)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.47277876498
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.48)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.69827194879
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.70)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38847227221
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.95856385861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.96)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.55336951727
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.75248317399
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.72)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56235617848
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.53)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.73088299949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
66% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 577
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.32152614137
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.10899872329
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.10)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.62794048114
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.80474626036
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.32436440721
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.87579589083
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.88)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.5397121406
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.53)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41003482194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00077295051
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.03)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.91836106602
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.394171686972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.36)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.25444196533
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.992031207517
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.34710255563
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.33)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.42856824029
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 0.40)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.63323780772
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.65)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.05536271004
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.09)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 578
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.05265699708
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.08)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.96403092671
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.08465101502
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.07)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.66566394862
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.13282592846
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.65347673943
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.2226808621
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.92809872552
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.29840056759
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.687079077736
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.9091633169
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.92)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.26805881551
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.02237388516
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.853703717722
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 0.79)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 579
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.50023806312
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.48)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.65044699095
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.64124947957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15841436433
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11498967932
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.37478674721
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.31868747568
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.56772804395
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.59)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00370622942
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.23227569467
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.24)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.0846952284171
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded -0.15)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.247293523763
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.22)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.27249155295
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.30)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.46108146311
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.43)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.14418232561
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.12)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.47171039569
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.52)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.93593324673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.96)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.18288542623
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.30400011831
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.30)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 580
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.8686055034
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.86)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25620502241
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.23)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60315017321
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.59)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.78117267595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.79)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.56550723571
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.57)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.1774738198
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.34792299648
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.13740957289
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.15)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.32982136685
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.59970935006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.64)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 581
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.6826796225
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.71)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.57633011941
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.54)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46401984929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.44)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.54262714896
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.93735610695
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.95)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.7244522434
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.73)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.74884884603
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.72)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.16512582209
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.20)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.6614165821
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.6375639019
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.10140374174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.5989287003
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.54592534562
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.53)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.16838646558
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.26503091067
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.28)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.36962296371
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.38)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.05524614336
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.04)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.5102402913
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.55)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.88216542825
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.90)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.930412037654
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.89)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 582
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.14090413448
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.19262619083
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.23)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.78769379491
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.79)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.61760951872
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.60)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.30333615913
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.27)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30128908252
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.74688938141
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.78)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.49883362515
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.49)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.61192555688
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.01057671918
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.98)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.80485679426
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.83)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.47171357074
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.87623032479
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.87)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.40113000327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.48040221115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.51)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.97735150301
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.68897701318
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.68)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.20872175896
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.23)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.59173173351
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.60)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.60240922437
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.59)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.739889550822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.70)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.99488947438
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.98)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 583
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.10379877962
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.11)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.44691125838
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.91279866155
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.9464249029
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.424202017316
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.41)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.05841208501
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.16158241432
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.30150172688
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.27)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.37992082072
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.21982175273
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.19)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.94741517801
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.02071647205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.03)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.933966990703
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.92)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.46370742897
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.45)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.34311044198
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.36198370635
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.31978169001
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.86730071374
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.85)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.56054379734
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.20708496217
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.55116294667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.56)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.11394795361
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 584
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.21242492568
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.28835360613
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.31)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.18046716276
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.49959538567
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.90158685703
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.17223640293
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.17)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.7005357815
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.72)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.28727980939
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.28)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.68776538096
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.72)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.53586038235
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00126930194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.99)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.75393241483
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.70285777911
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.67)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.10398504398
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.13)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.772055924603
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.76)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.00569575854
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.00)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 585
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.81642797727
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.82)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70755176662
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.71)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.46695399709
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.59320457149
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.37086675027
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.45617037178
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.48)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.61622797916
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
77% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 586
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58380499907
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.0511552779
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.04)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59695602751
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.94492029344
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.95)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36454577878
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.09678539476
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.09)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.6883535193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.71)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 587
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.32854669417
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.5196217974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42767559477
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.46)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.59528840992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.92486349605
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.319508677877
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.27)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45453515121
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.42)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.40704781926
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.40)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.83321843063
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.73836543931
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.18492370254
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.21)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.38505999075
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.11691956227
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.027688024
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.99)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.55098341563
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.61983850819
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.693934589772
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.70)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
-0.386903530431
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded -0.43)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.55875552565
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.31791507019
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.28)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 588
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.76050831403
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2557666815
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.26289460488
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.57448242327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.54)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.02632365419
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.0624026916792
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.05)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45016830712
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 589
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.27868876616
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.33756803068
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.34)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.10537855118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15424897606
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.15866386086
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.45647434184
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.13034193666
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.79755246342
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.81)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.56335488186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.7371070043
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.38181251332
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.53796759887
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.46401735461
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.47)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.659945020337
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.63)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.647656268822
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.62)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.88079158963
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.88)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.73557468545
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.71)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.20581731962
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.21)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-0.610675225786
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.64)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.286517401334
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 590
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.8706209409
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.79900007584
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.52521363019
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.31985820696
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11999613557
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.10)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.20014657505
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.69190965912
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.73)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.12371297346
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.58925355999
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.71927591339
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.74)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.66498430084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.65)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.5627764403
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.847485610153
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.79)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.931520827627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.89)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 591
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.47105258783
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.43987871969
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.47)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.48347839955
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43051755753
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.78908842826
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.57352298177
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.59)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.6404933883
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.35277626974
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.36)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.84233782203
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.87)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 592
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.88691965099
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07369372981
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.43015302917
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.82819985143
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.58815526066
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.59)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46878431753
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.43)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 593
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.9987413681
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.435758869306
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22111521802
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.20393439418
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.38096393517
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.22626452382
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 594
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16464377997
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.16)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.91650411293
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.89)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.14839815584
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.15)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.33510762178
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.77920355564
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.88176613975
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.92)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.8283625129
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.53262362344
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.05663680949
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.06)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.58613148724
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
71% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 595
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.357853139456
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.948168205529
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.98)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56588459369
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.6068155974
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.75261524145
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.49541245538
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.51)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.85752321604
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.87)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.46103462779
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26410382264
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.09074350242
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.10)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.87668441758
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.92384907605
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.91)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.09781720348
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.05)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.25412522109
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.3916636771
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.42)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.94133030168
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.94)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
-0.0359619695064
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded -0.09)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.9492959306
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.94)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.60741161122
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.61)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.861628287394
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.83)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 596
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.45067112174
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.76760804743
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58474637298
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.30805274298
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19433011507
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.07146154435
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.08)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.02381155454
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.40979860802
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.67643441576
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.34843697404
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.32)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.22545360898
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.22)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.57831590219
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.59)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.96910078148
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.96)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.26051320181
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.24)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.72465864646
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.73)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 597
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.90735693315
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.93)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.74411362517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.99111499004
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.00092927126
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.22592913441
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.20)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.12242004811
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 598
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.91681163185
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.98)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.47099548991
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.51870336246
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.37642567761
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.55545786113
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.56)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.78798950836
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.30543311411
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.249402545508
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.24)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.2028008186
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.15)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.68154951165
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.13626046955
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.16)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 599
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.60915395373
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.65)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08369155774
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.74216115967
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51524010876
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.28979518157
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.73382428276
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.75)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92247139385
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.34668216094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.36)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00591273173
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.99)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.71033399073
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.70)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.74516131761
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.80049007689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.68471049715
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.61021553699
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.64)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.60396456236
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 600
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.76880175234
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.06670987277
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.94398950742
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88673772523
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.680497693955
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.67)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.06074277124
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24950768733
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.62793496414
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.66)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.45950212724
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.12857332667
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.10)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.817263509529
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.81)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.51846519982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 601
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72527374674
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.75)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.45834197017
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.26602395161
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.23)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.91301109722
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.96)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.66586547638
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.33409693042
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.33)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.58191623434
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.60)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.9918074809
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.99)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.35292444475
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.38)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.56749411702
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.65909452718
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.71802256641
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.72)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.06761399465
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.07)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.57000017995
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.56)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.22151286957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.63790297582
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.63)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.14948004945
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.91927811267
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.94)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.30527645029
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.28)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.21544975679
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.22)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.33893122777
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.795596082567
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.77)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 602
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.232731381656
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.58323543647
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.73696037962
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72214247112
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.25043928073
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.978669304368
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.53200853838
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.53)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52576734348
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.52)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.76511043103
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.77)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.882634115343
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.84)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4338169459
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.41)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
-39.1169164378
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.67661257484
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.97115161477
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.80345621681
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.82)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.537922277477
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.50)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.61432027182
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 603
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.06542704756
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 1.10)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.69740557559
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.23422204359
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.21)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.51090726544
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.0367962145
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.03)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.77918812697
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92299831254
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.97)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.9841329083
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.01)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.50591769986
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.957086231454
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.93)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.15279342115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.16)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.82161665973
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.80)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 604
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.31225272269
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.36806870097
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22113314352
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.30599415175
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.35)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.6675555441
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.5798109001
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.60)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.6981292094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.65975206006
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.33156273096
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.14934901593
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.15)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 605
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.89559809561
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.87)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.15011002612
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.10)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25128652051
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.85334737159
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.88)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.2617502454
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.27)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27141943602
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.30)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.4488367317
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.44)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.59159588325
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.58)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.02770827226
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07872248452
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.06057885242
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 606
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.7284061147
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.30194273517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.32)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82706016096
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72307041812
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.71)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.6528121092
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.98919801618
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64081229022
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 607
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.18188305365
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.16)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.67253714596
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.82398524076
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.19976723528
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.37606135255
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.60678490823
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.0941803204
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.09)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.67357871777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.22956392764
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.5126446648
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 608
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.67403144536
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.52499138392
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.66443230139
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.41678954302
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.57140236756
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.55)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.37087471043
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.36)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.19382387338
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62780423302
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.28801826366
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.918305041662
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.87)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 609
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.72427408561
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.76)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.35121945874
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.34)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.30184521262
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.50105914497
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.53)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.44852052167
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.18190348209
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.13)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.87752353517
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.89)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.56365142462
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.62903813244
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.39005063173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.73496693399
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53207893807
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.01437890358
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.96)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0254732724223
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.02)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.60078092139
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.61)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.26238760255
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.24)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.15541593618
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.13)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.23890297776
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.20932106172
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.752058837015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.70)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.35787903014
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.29430545888
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.30)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 610
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.132959485158
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.09)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.40725365702
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.41)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11643072429
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.11)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02445928628
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.05)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36922160471
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.74090887731
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.75)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.77826186406
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.33575266581
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.37)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.19502178068
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.06126362703
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.05)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.57864140664
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.963751768743
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 611
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.35276441795
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.01621830484
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.01895966219
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.49954939753
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.358455944052
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.32)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.53881053003
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.51)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65611227989
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.53312262291
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.0173541983
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.40941517786
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.54917439565
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.57)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.24323031268
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.25)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.32129216524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.36)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.49872468247
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.14819024317
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.18)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.18457242787
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.76409255033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.77)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.937675447567
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.89)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 612
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0776936958535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.03)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.417881534934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.40)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.47202701866
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43474953968
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.46)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.28066299486
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.62426151377
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.760774459118
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.05783679064
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.76546225605
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.78847877298
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0883675551133
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.05)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.08617294233
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.11)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.67056083907
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.80250849513
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.81)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 613
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58962590835
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.56)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.09585503038
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.10)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.36386290047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.1124549575
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-4.53924647281
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.68)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.02350031142
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.35767745034
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 614
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.3301649766
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.33)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.13022440745
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.09)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.31120086024
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.17957142981
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.86556689824
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.89)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.71580065842
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.73)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52668174661
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.54)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.60076662752
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.64)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 615
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.278820030123
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.28)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.77295948301
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.79)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.07178300581
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.44285777796
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.03435691226
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.0994960069082
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.09)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73165423212
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.30747727682
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.32)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.93132362549
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.96)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.83691669751
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.03711557389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.70978758489
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.656676701124
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.66)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.0528131591096
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded -0.05)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.404747808976
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.42)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.86728599145
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.87)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.10615389131
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.13)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.10702738633
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.07)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.33303039339
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.838617767019
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.83)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.47948258379
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.44)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.92157819968
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.94)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 616
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37970085805
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.42)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29441872365
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.45853142114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.172001078
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.6581847988
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.64695663322
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.98932747219
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.98)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.97112215149
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.95)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 617
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.46311691342
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.46829539217
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.23275164404
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.42123704632
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.19929591717
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.18)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.56404812803
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.754450583495
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.35904009681
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.35)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 618
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.364939798619
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.2536361223
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.14574869925
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.27093541196
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.39186771807
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.41596451259
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.74522318326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.00833813564
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.72162671613
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.0677550959
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.67505412237
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 619
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.64192716979
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.810455703375
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.83)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.49568152413
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.47)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.4051849676
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.49869638327
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.31596998049
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.35)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 620
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.26631429432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.28744329359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.76616180517
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.78)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26779450165
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.80075019641
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.14562751683
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62743381262
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.81200310989
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.82)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.28793604163
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.30)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 621
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0177956290944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.01)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.20151814616
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.18)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.40857697928
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.45)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.8056361148
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.2893798116
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.32350465614
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.93449641387
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.7725231495
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.78)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.90180922558
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.91)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.74482229028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.79)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.40214401749
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.44)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 622
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.71160968014
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.68)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23016145416
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.24)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.0477485466
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.71902926269
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.37233343512
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.03875395299
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.14545554271
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.17)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.31613153341
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.7718461352
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.48867858116
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.45)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.36543060443
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.35)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.20542950298
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.50399675723
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.49)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.85012404904
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.83)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 623
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.813455339285
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.832990485024
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02595393597
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.02)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.82448688901
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.87)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.57962622422
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.797163700878
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.80)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.24511371739
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.20)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.33796444645
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.34)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06709510375
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.426173656187
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.43)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.66385559779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.72)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.983499131229
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.97)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.45766533388
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.50)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.27395050039
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.31)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.06945135306
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.07)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.967519855652
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.92)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.10463684625
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.10)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 624
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.0167731835958
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74971195617
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.75)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.0815388869
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.55455998811
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.977875283249
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.71851441654
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.73)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62423984433
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.64)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.04521331289
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.55332403007
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.61270784688
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 2.63)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 625
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22493859565
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.23)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.95785828161
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.98)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.37848517503
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.38)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.80131857987
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.9519374748
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.97)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.59231635429
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.89154252432
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.90)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.6810573934
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.16687485813
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.03884449196
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.71866860518
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.54758105171
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 626
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.601946207322
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.58)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25266775592
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.25)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.39959169355
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.9009988397
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.92)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.76584872274
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80134354901
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.84)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.66990765325
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2638204294
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.25)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 627
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.66901803658
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.69)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83212614791
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.83)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.41523914069
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.37)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.168692755
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.13)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06259416503
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.07)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.58027377504
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.41830496811
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.41)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.55589902777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.57)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.79756670783
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.78)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.53785850501
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.51)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 628
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.63159411298
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.26195180664
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.69795197212
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.22589074141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.32045500945
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.05982649087
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.80601784105
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.55136898551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.34016550548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03377585366
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.03)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 629
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.63232556666
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.35659097973
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.03662260758
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.59158173971
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.6865984186
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.81869376411
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.61296004596
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.6593095288
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.69)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.02896642694
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 630
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.73884172863
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.51288849281
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.50)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.47321082885
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.50)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35785437586
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.50168055192
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.16257679205
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.15)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.88711367551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.89)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.68910294229
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.67)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.96609703969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.961729580417
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.00924230095
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.30770890613
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.33)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.84429456201
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.84)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.24308498855
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.24)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.48266399981
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.45428115689
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.56447074796
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.60)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.742047867612
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.72)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.188731336457
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.19)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.39841715507
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.862787492786
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.82)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
-0.376975317503
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.41)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
-0.454169352074
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.48)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.393183189399
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.34)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.946032808867
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 631
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.53648950298
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.58)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.16323474934
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53798060436
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.79864343353
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.90785994191
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.77690199524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.36616018528
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.44192148034
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.43)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.75578694746
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.80)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.67569555379
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 632
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.48576578088
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.52)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.914896703682
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.93)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.15665059592
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.16)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.70021776461
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.06185704661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.44822440982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.42820053218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.75103107553
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.76)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.44812727558
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.936912577532
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.01671972409
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.01)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.55558152832
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
66% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 633
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.62767027207
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.61)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.21273588178
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.19)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80116862799
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.2383381791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.97785155595
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.01)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.10926206652
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.30983535094
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.32)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.11055242142
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.13)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.99765125021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.99)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 634
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.06373791958
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.07)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24096405033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54907118363
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.43067180713
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.55434780635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.53839201259
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.54)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38751407611
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.36)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.58000028475
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.42139905837
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.43)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.61471644827
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.05801134598
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.6099264514
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 635
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.19378510416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.21)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.69485623985
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.72)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.448352637
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.77570405096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.66336299691
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.67)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30938602364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.12162695774
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.888278090361
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.86)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 636
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.02012176511
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.00)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.1773086177
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.92110508422
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.89)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.06215009382
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.05)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.61542551206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 637
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.98953200683
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.97)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.63968455252
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.9955380222
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.98)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.25716484149
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.64432039051
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.65)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.46571802439
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.07060543001
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.04)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.92195123581
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.93)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.48217920908
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.75670721619
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.77)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.69947093268
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.73)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.53794825027
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.04012118831
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.02)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.5632868738
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.56)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 638
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.77714866059
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.82)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.52288386566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.05942395416
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.55734589948
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65179031979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.31020021974
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.36895717264
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.17666400137
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.08274932717
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.04)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.03009519724
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.01)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.103208296783
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.06)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.92426335626
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.94)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-0.200935449259
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded -0.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.744916227106
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.72)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.21524696263
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.25)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.0656350352
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.05)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 639
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.98755435939
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.00)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41640948703
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.39)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25716229963
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.23)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.88095354774
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.37047643283
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30614325087
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.40269569194
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.15468071139
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.16)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 640
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.27393019767
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.30)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.48251175421
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.50)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.90594380243
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.90)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.89744356362
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.93)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84595698132
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.85)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.45100573911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.41)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.28319657846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.27)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.47531567824
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.52)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 641
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.83292664509
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 1.87)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.68835544025
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.91036339419
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.63827209769
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.76900127477
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.78)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.7795989678
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92894829252
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.938783729364
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.37424701345
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.36)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.58710915816
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.63)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 642
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.4458467874
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.5477877858
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.26648875371
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.32)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.04650004522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.00)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.05118327779
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.87646852566
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.89)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.69295195411
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.96334075544
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.97)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.98569927114
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.01)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 643
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.27576376112
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.25)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.42449422
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.41)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82406757248
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.76523681262
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.77)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.26949926098
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.29255944488
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.12129176037
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.14)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.945591455996
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.96)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.06151345286
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.09)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.52273179717
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42414299836
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.44)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.56630759877
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.57)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 644
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.5647980179
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.55)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.1138926855
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.32527682717
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.38421697953
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.40)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.82244340517
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.14743734451
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.01865278572
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.01)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.77426266131
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.67492415049
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.0641010240527
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.04)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-5.54803635631
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.72)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.51467475136
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.80703400424
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.89034519419
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.88)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.4210333176
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.09478112212
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.08)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.13383024904
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.15)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.09745922229
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.07)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.06647210801
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.05)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.64711072949
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 645
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.91946616065
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.93)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.4622130638
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.39606611343
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.6983200795
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69185922359
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.72)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.68991133105
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.41762750671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.43)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.7503463003
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.74)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60157226328
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.86296264246
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.85)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 646
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.44848645418
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.47)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.08465451762
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.775245969056
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.77)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.52147017173
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.0482683566245
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.00)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.53482582643
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.994707170226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.96)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.50136459432
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.46)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.35158010313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.34)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.439406576287
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.41)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.02906937823
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.81074036497
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.81)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.6108565015
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.60)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.39854522621
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.44)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.982829600875
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.452697340524
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.44)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.13610265105
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.13)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.472252091402
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.46)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.21987977165
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.23)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.31979092674
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.32)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.34138304321
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
-0.174441472073
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.24)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
0.897177892425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.88)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.976061714539
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.04755452305
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
1.06356235591
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.06)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
0.803590751327
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.76)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
1.05977558864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
0.643447878823
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.62)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
0.0425092986756
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.01)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 647
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.15981713038
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.12843718097
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.17868250383
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.90697692219
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.20001489619
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.23)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.71507505366
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.72)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.27734677425
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.28)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.05857395455
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.51280222205
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.49)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.90100459433
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.70832666903
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.60472152515
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 648
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.83488537096
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.15843582439
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.51837018744
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.72485085664
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51172193783
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.42171171133
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 649
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.72957823147
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.11666877237
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.36622076578
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.34)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02990059133
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.76380381351
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.39527789406
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.37)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92086812263
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.977646678129
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.0538798664
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.03)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.17056090429
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.16)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.997159423635
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.97)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.72484709841
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.28945865392
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.4837406082
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 650
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.915732836
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.8864063596
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.70831785598
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.70)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.00689880285
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.02)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.50676309652
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.74473085823
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.206004781383
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.21)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.07616656979
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.06)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.896689490138
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.90)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.17044643494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.16)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.19069550587
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.17)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.28442134826
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.76649843892
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.716729185347
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.18413861115
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.18)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.60530858002
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.61)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.553168010241
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.48)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.08509686718
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.632419673168
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.58)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.77154871505
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.76)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 651
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.49405845491
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.9797399138
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.77056003867
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.80240422617
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84622302549
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.66816000693
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.69)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.32561643188
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.38)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 652
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22604310887
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.48056827395
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.56072317573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.53)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.35426191591
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36847339834
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61217916584
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.0336460688325
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.00)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52769380744
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.53)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.21383387758
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.20)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.62290478224
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.63)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.30883927937
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.30)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.4717961674
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.73547910232
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.75)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.737647991305
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.69)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 653
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.24622430442
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.22)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.10626026859
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.06)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.51626734458
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.34786259566
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.55550688309
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.49477855603
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.52)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.31124343668
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.88192170208
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.89)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.886075210113
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.88)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.81732274046
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.47059435736
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.55)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 654
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.82098503089
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.83)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08435419106
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.10)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.44371149502
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.42)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.04381172427
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.97355046936
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80003369975
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.83340930756
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.81)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.81309780537
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.72176711956
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.33810272853
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.57580300306
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.57)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.65620507165
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.12479848378
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.32828236965
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.33)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.70530067758
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.72)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 655
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.50132367817
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.47)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.72692697219
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.5223257784
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.52)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15060345948
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.73298869687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.18118359533
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.55352804303
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.00108376958
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.03)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.24101581972
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.24)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.50014533071
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.06084198622
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.04)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.82755256856
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.80)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.70669958657
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.71)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.31935149673
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.31)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.85515842091
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.84)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.89097548653
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.89)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.992493901958
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.98)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.28846045488
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.28)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 656
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.49735751837
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.54)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.44285063154
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.46973387457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.58762546069
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.59)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69245445237
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.40981845471
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.41)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.15791629464
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.16)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.06388296734
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.09435366195
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.97048000915
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.98)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.71900604248
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.73)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.08553044965
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.05)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.53429151742
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.54)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.841141178977
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.83)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
-0.352287285026
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded -0.39)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.19633131226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.20)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.00919278346
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.01)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.912754684883
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.87)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 657
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.0343980314
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.17077632776
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.18238382643
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.66015857293
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.91783590997
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.93)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.0836478887426
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.03)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.46461120409
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.25807601393
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.22)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.94084970891
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.92)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.62776265179
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.7411192967
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.45610889389
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.48)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.41205861871
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.42)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.941620261806
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.94)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.5664625978
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.26143549127
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.29)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.45788005304
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.46471699305
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.49)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.2102102512
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.11602026822
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.15)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.714216863
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.72)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.40722017567
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 658
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.22824613506
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.4759622381
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.49)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.16977818282
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.51481704876
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.68792809992
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.13206424966
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 659
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.76917735813
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80656590602
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.84)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.20267437389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.56226325522
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.60)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.77254623333
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.78)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.78727438077
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.19430054975
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.17)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.83906638502
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.3949920484
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.17422636496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.10250358603
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.52178694501
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.53330059284
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.53)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.829562278465
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.82)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.26914446823
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.34033672032
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.29533483083
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.28)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.759837047938
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.74)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
-0.127884314708
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.16)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.3933142887
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.36)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 660
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.286931519862
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.28)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.09796189256
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.09)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.01964411624
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.23718944959
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.92808435452
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.91)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.344412174814
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 0.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.94306619244
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.96)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
-0.0559305313843
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.06)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.60558911274
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.64)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.455248364496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.42)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.59308710808
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.59)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.875794215938
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.85)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.827969388758
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.81)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.20086855608
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.22)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.78751361786
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.81)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.52476443197
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.51)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.910313427453
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.91)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.09940091448
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.07)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.32330422507
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.03490519463
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.05)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.5190160966
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.51)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.693364967978
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.68)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 661
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.86125740935
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.89)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.687302328993
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.53953586167
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.8511464017
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.40984145699
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.43)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.12483451776
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.12)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.51965264436
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52960957277
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.90459123933
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.88)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.10995466343
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.10)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.09312699558
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.11115864421
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.11)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 662
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.36869842908
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.36105905816
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.79481449837
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.60500234893
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.81450857567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.61574407006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62302024875
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.06209902744
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.22707016393
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.19108987706
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.17)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.15338564462
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.17)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.57871733807
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.952714564146
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.96)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 663
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.01670951467
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.06482595244
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.71730277503
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.7743628781
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.53664849931
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.971874576269
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.54217171871
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.54)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.760814942265
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.78)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.57841160486
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.58)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.866184841985
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.45597585907
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.49)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.22823579355
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.22)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.14595028419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.11)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.06711831214
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.04)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.10082160284
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.13)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.850547996886
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.85)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.9312081162
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.72156674309
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.74)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 664
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.79389095709
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.10668833231
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.09)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.30940763562
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.83290276128
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.1047472319
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.37214184055
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.46681510065
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.48)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.38710141642
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.15691602351
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.04630880958
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.03)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.03256203221
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.06)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.27999287778
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 665
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37076991463
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.38)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.74260268035
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.77)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.07656518844
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.11)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.42314575586
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.41)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.38421708815
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.38)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72791062827
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.75)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62471228783
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.63)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.98446587362
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.99)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.896927392608
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 0.91)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.60220268253
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.61)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.01109441209
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.04854326124
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
-38.9509984209
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.16)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.15226667222
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.14)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.37517481832
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.38)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.03107655012
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.04)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.38383537121
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.38)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.0486271334425
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.00)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.47044383785
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.45)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.672423565058
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.65)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
2.56894055752
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.57)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 666
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.79837504927
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.85)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.30759475738
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.01741648225
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.01)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.92432139686
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 667
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.691308231233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.66)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.52079973524
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.30226038782
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.42687415532
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.27106574561
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.27)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07224378427
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.16580479095
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.68554861595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 668
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.18793941482
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.19)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.62469531511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.85752511238
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.24722946442
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.35465487278
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.39)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.44070406626
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.45)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.07971307552
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.76916968995
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.74)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 669
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.1297941555
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.55910030175
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.34241199779
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.7311197032
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.05088052495
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.81610390266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.421598190976
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.360083054777
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-0.107780438237
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.12)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.70361272478
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.73)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.6536624625
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.98857758998
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.97)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.34741650843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.48158157043
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.46)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.52354367618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.1420869674
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.12)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.25014134258
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.22)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.2083371233
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 670
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.90769617231
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.97)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.38423799275
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.34)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.4784423595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.64471330648
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.0300830483
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.04)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.2685139533
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.26)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.2895837565
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.26)
77% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 671
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.66248783393
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 1.71)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08664190214
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.16255836661
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.03309314777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.16112643892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.98893952113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.5741217006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.54)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.80212359462
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 672
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68083467636
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.94725028765
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.99)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.20547135752
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.23)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.08538878425
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.35313737246
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.8428579894
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.24261208827
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.27)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.31430790842
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.34)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.77564439738
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.21427883306
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.10357124506
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.07)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.81162371006
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.46516884906
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.25316318815
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.26)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 673
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.07761633579
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.04)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.91286824515
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.0980222798
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.04)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-0.0287804868974
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.03)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.551337604429
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.57)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.73816384858
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.47955571337
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.48)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.20321161011
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26888411603
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.24)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 674
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.01126727501
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.93424761843
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.24833130169
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.27279803037
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06471517144
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.00877187941
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.00)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 675
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.57491618151
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.61)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.79383630149
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61906158927
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.5858816672
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.82597211446
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.84)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.36276304064
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.39)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.14563601591
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.66605876124
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.8762850648
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 0.86)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.8175869966
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 1.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.26899600691
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.24)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.73372522923
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.34002877571
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.39)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.50372056252
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.50)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.03765282096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.04)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.72832642508
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.43303395505
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.45)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.21308822567
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.17)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.05632928279
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.02338148439
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.24661841623
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.11844626662
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.14)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.39121996123
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.39)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
1.36292562146
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.34)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.01721525732
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 676
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.415132450028
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.44017057877
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.88340180721
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.94)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.67809296183
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.962891306494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.94)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.26075422714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64354120052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2879832345
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.31)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.78542953682
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.64616288886
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.39555543201
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.43)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.47759701211
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 677
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.22798150056
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80857886253
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80574776447
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.86)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29013413478
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 678
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.50138816528
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
-38.8281694088
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.96800249481
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.94)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.73209948614
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.73)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.98130390428
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.96)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.17237436605
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.97693490078
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.99)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26411019384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.23)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 679
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.06794295769
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.17092499761
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.07524560566
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.41678776173
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.90256933709
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.90)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.55031754686
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.55)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62418775593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.62)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.47091646437
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.41507450972
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.38)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.52228109713
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.4278338435
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.26900838591
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.28)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 680
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.04718821081
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.06)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.430899419003
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.43)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.34254285102
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.14527132664
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.26803597046
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.9135986031
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.90)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.09914809891
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.13)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.84745717693
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.87)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.82096972176
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.82)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.19255019597
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.30801623459
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.34)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.877503995487
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.87)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.06512630903
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.814408056021
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.81)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.20273424911
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.23)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.14731805078
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.47081623651
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.60602519767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.59)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 681
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.49432341463
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.48)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.78119723917
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.83)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.07837133877
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.06862585441
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.04)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.31134611629
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.31)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.79567144178
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.11604433244
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.10)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.76593495063
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.78)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.53067215434
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.64600430384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.69)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 682
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.18931513876
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 1.23)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.0519069377
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.33732898175
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.31)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.19010592143
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72474750266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 683
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.20110309589
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.41456512708
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.33918192808
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.31)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.75718518413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.78)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.01851994846
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.52224310015
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.49)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 684
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.53707500326
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.55)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.63731375408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.63)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.63230919331
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.7244292796
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.72)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.42636648714
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.42)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.90823516061
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.92)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.18627473069
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.85180299944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.88)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.73268009226
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.74)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.41046558275
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.37)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0281046948717
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.03)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.00892772945
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 0.97)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.83161747502
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.83)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.32234520052
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.33)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.64234216701
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.63869642134
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.24171989115
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.22)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.38592618408
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.39)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.31277118982
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.35)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.60768473208
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.59)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.75127539352
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.76)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.34731435348
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.31)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.89292450422
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.91)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
2.4395774026
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.44)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
2.07141889196
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.05)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

learned value
0.897702530251
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.87)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

learned value
-0.136044578473
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.19)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

learned value
0.984963931727
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

learned value
1.80998384708
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.78)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

learned value
1.50434416932
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.51)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

learned value
0.743960165708
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 0.72)
11% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 685
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.252697471848
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.70339613058
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.587245604553
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.56)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.46059549647
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.59170370463
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.57)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.17836884406
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.63342394
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.37056994928
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
-20.0771957408
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.70)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.05909077884
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.04)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.18845929045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.22)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.272332718511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.27)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.7423616744
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.71)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.06000339931
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.725641457609
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.68)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.851252604445
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 0.85)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 686
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.35223752072
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.39)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.14765577025
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.27195932662
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.14384885951
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.09)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87657020073
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.89)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.04023003047
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.05)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.931934597339
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.47327493541
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.68099664839
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.69)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.09705662264
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.13761967189
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.11)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.25499555157
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.64903369102
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.67)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.31910642469
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.65335069641
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.70)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.72702365773
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.74)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 687
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.0021227122
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.64554913983
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.69)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.55483953143
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.64065446424
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87087785777
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.85)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.89437203929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.35641619271
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.86307477979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.92)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20467310409
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.02054746202
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0232243659212
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.01)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.69031674634
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.70)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.45240739615
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.47)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.13692705055
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.13)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.87037704266
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.87)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.73605762999
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.74)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.16753601723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.15)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.79227692128
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.78)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.94108740112
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.97)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.26655014797
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.25)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.54040039052
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.52)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
2.24255353548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.27)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 688
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.99344481872
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 3.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.71042597756
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.221750325937
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.20)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72793197913
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.76165018402
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.78)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.16510057224
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.16)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 689
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.68308153938
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.56239562989
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.16208043895
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
-5.21596850644
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.78997066634
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.81)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.03119286838
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.51422358137
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.51)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 690
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.21377274143
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.17)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.52237025575
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.54)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.07566265484
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.04)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.557980309216
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.53)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.74764373097
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.59705976986
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.18044499799
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.21)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26341625218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.27)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.00967643735
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.8603838043
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.83)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.05046818706
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.03)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 691
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54202142457
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.51)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43396643524
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.05753170695
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.79544717088
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.53569160085
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.06721798788
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.04)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 692
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.10111314172
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.11)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.09116117514
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.04)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.75507032245
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.18291778653
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.18)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.61186873104
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.75838369181
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.429805351609
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.44)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.172739553404
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.13)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.8646093113
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.67948192234
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.35643179257
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.34)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.71031402169
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.67126874242
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.45279141742
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.06920157202
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.62326069983
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.252861476
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 693
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.81753833163
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.82)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.46619290397
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.48264259328
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.42201686485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.45978403071
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.7888743593
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.76783687451
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.58082538963
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.60)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.18595452993
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.21)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.62070296322
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.58)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.29126886047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.25)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.129404914497
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.10)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.07764543861
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.09)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.72263046091
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.73)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 694
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.38184526311
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.86721329616
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.92)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.69031529055
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.73)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.86814090138
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.9979977682
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.665295885664
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.65)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.09396668065
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.10)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 695
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.67282580403
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.15098685417
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.12)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.25197830283
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.26)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.16847181552
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.81733633352
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.84)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.32925083192
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.36248816535
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.40)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 696
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.51975293685
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.92858590021
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.93)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.54986974569
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.87372972307
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.90)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.40564054238
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.821551416607
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.72351881416
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.67889032654
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.73)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.022044291275
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded -0.03)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.454203773632
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.47)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00651765008
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.99)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.48159094054
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.01037872557
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.04)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.78199444048
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.77)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.4777446961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.98103540718
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.99)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.23366338822
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.06666464641
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.06)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.53919212897
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.52)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
0.717371329655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.66)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 697
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-19.8586568086
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.24935515835
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.71677248438
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.55696076296
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.61)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.48123708318
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.21768509358
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.69313583213
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.68)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.80245846807
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.80)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.476136224
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.75211829014
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.24103196629
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.28)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 698
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.4472338241
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.47)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.40823381734
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.43)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.10678190001
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.63828330916
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.65)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.84649951794
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.89)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51872226295
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.09158667264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.11)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.54829639204
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.59)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.65507827346
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.65)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.00852341648
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.03)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.63742130356
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.80954426021
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.81)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.26528025475
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.26)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 699
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.07070250411
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.07)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22074841612
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.19)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.71818942004
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.04711525264
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.25172847656
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.95440274501
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.97)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.20428751076
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.19)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.06157497006
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.05)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.515878479102
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.52)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.1218523719
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.282230265856
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 0.25)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.918381555516
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.91)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.1384720965
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.15)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.849235264298
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.84)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.817995302123
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.78)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.56742808415
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.55)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.40487282045
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.39)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.02671143242
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.02)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.47146512426
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.47)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.08702009264
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.10)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 700
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
-38.7926270156
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.99)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.45181728554
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.44)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82427421424
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.80)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29543031259
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.28)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.40184013617
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.9850153653
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.98)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.03168981237
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.05)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.430097627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.33190933738
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.33)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.44176252554
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.42)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.65986079677
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.15034423398
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.15)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.18983112427
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.19)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 701
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.434803472749
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.43)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.61518933797
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80505432421
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.54086035092
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.92287989231
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.95)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.30863304866
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.52219110559
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.57)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.45357683468
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.16357005613
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.35763167747
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00060677589
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.03)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.34238952387
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.36)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.30123495208
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.29)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.74711710435
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.77)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 702
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.62120067622
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.55060311192
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.58447265029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.59)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.63233794084
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.61887991728
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30166958647
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.08733567378
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52905692821
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.55)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.914368266391
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.90)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.77167607853
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.77)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.67081993177
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.69)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 703
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.59661171528
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.58)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.83342160833
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.84)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80211440413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.63797916694
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.66)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.79313573311
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.78)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2927273606
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.27)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 704
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.87703127273
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.90)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.59033391029
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.55)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46149034791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.77139089912
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72482942184
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.498495063293
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.48)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.216286441347
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.21)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.09859037428
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.09)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.17058358548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.13)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 705
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.88310193287
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.89)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.70971373862
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.76)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.30790212519
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.26)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.27033052018
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.03107529224
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.23216278136
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.73382041104
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.75)
77% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 706
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.05125999578
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.08)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.674348458252
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.70)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.62769344872
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.63)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.07580594724
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.48944880767
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.46)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.62539421135
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.66)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.692820988337
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.71)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.75298734776
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.56197679522
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.94318624229
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.96)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.81686090047
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.63751940039
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.01107120341
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.23728778548
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.93251178968
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.98)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.99483007733
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.21181602566
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
51% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 707
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.782942803012
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2107089705
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.65628904665
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.66)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.59393080907
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.01954447501
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.70213876518
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.61620890371
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.61)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.42302254064
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.43)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.50070020981
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.09101529843
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.09)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.56473403799
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.54)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 708
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58186946798
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43040920291
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.8771891487
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87031298013
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.88)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.87639439611
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.21794177729
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.25)
76% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 709
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.785345864494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.62274693769
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.04298344646
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.53197202043
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.58282158179
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.73192371948
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.71)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.0183991955074
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded -0.07)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.46037017127
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.20487896818
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.18)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.736149725416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.71)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-38.1093771577
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.55670938573
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.57195211242
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.62)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.248500833567
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.25)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.84250836987
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.85)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.664047777217
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.67)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.26269768725
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.880452641864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.88)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.98383214903
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.99)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.8604508513
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.86)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 710
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.3864783994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.42)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.74381851604
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.71)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.74477957907
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.74)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.3630189384
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.79651206909
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.29834355635
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75378446963
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.962042215786
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.94)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.00848579511
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.90242526734
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.89)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.13823610255
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.13)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.96750754256
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.80991259957
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.84)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.07562741357
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.08)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 711
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.37363506325
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.81986515403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
-4.13899096343
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.86544319855
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.88)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.47403892982
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 712
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.878591109817
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.86)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.37122535227
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.05304546764
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36454915309
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.04985424867
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.63369046283
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.13913911132
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.41484437414
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.73189724318
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.74)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.63457016242
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.65)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.870489151832
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.85)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.727110131393
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 0.72)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 713
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.01298068509
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.31035019452
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.29)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.69238594971
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.35041634753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11885696637
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.08)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.84435853756
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.16349469383
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.35176235575
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.36)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.19516582036
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.06123764965
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 714
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.943728809743
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.45976208538
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.46)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.50689236779
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.50)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.94259198403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.94)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.12218239444
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.571310816909
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.57)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92792068059
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.07770193347
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.21237142282
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.994939390782
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.13941469024
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.14)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 715
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.3612895441
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.39)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.86392502076
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.88)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.38299010465
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.49135916615
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.71472092419
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.9112883145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.92)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.54922358663
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.49419423992
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.47936679896
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.47)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.12420377648
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.14)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 716
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.81848091924
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.80)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.07233277129
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.09)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.49893681707
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.86443285476
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.7422332892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.5242618653
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.80695251327
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.09127297957
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.09)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.75571748012
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.96003036407
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.95)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.47160978231
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.48)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 717
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.58211648802
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.7044471513
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.43827246667
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.40324885485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.58967430527
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.707329060503
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.71)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.10141760469
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.08)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62391913652
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.64)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.33092281898
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.28)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.11887969166
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.09)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.44154669965
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.48)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.98352061815
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.49235089971
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.62101932844
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 718
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.0901611381
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.10)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.173815605593
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.15)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.76631780539
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.79)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.93153641785
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.95)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.79755892275
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.80)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.48473598994
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.50)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43438252515
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.44)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.45845806977
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.10985029059
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.11)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.07525484882
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.06)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 719
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37643187734
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.8500772842
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.57248033365
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.689694829134
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.70)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.12182665058
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.10)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.5895209053
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.58)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.926713138256
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.52364383426
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.57)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.23781649215
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.24298114413
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.62254634588
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.15293708212
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.28980571183
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.20571701415
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.382585531663
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.38)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.82981793962
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.82)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.11830307799
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.12)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.98082431465
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.01)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.09835474348
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.10)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.05180072124
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
1.19224291319
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.794975392441
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.81)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
1.38897903348
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
0.995102758406
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.99)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
0.473790602716
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.45)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 720
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.19542294554
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.21)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.71241016245
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove forward instead of right. (rewarded 1.74)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.58528236827
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.59)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.10157598046
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.11)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.36626106133
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.14375011651
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.16)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 721
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.16949826621
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.15)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.18000742883
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.21)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.32599813154
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.31)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.37637181319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.3077792573
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.29)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.30340520746
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.61332164059
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.303813225385
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.28)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.40054559078
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.38)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.36261959984
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.35)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.59993252223
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.60)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.9903282477
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.99)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 722
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.39350436003
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.77564447652
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.03000050118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.02)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.10302166963
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.59697908195
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.60)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.87572431001
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.70534059775
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.70)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.260739466533
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded 0.24)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.45363558058
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.47)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.16553864433
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.65362931338
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.159420803848
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.13)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.62789608544
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.60)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 723
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.11949167067
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.14)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.21050891445
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.04099678004
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.09767253379
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.46790985834
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.51176691272
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.53)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.99644096276
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.99)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.40744940884
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.43)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.23105080492
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.23463458289
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.88490192136
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.87)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.65927792905
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.68)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.29337033027
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.30)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.66179343557
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.71)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 724
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.6829897165
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.70)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.91111324983
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.93)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.8462634482
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.84)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.21707893496
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83399345837
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.8315210607
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.88)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.61216159331
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 725
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.99849649509
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.69632584922
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.75)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.53468124677
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.29244584252
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.23004593796
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.15409844748
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.14)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43434296238
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.41)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.00584306257
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.98)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.91059624449
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.72839997262
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.70)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 726
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.37175018229
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.37)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.82439694319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.89)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.73419138584
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.73)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.75701845913
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.80)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.30229675403
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.80337098949
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.45972113967
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.45)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 727
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.412597665096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.37)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.724704748253
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.72)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.93620700026
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.97)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.01713634687
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.00)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.65721761022
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.66)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.06793379819
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.05)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.79918747892
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.81)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.72006608217
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.72)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.6968894535
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.67949450308
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 728
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.54395036386
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.55)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43676825651
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.45)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.35641399113
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.37)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.33790497479
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.69496916594
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.73)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.88346118993
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.92)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.980564869823
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.93)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 729
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.80436500232
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.80)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.29556981855
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.26)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.05149317498
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.74508042686
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.84751361056
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.25147214468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.53899794338
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.50896137392
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.50020799741
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.63735386143
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 730
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.70710472093
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.71)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.83381636544
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.61538518325
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.92004215118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72930183206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.58986278496
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.59)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.90136899068
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.93)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.02503675819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.00)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.03722437263
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.00)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.31216335481
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.08058401948
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.11)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 731
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.84747158521
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.85)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.46303134561
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.42)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.4673732468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.9219852884
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.92)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.41959276977
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.45)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 732
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.39197118807
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.42)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.21413464508
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.43465012543
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.44)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.87916377412
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.85)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.06340751964
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.08)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.34005134792
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.32)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 733
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.78372096986
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.87)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.25322715881
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.25)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.80324488485
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.83)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02676216485
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.16724834671
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.05907251969
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 1.03)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.4532175297
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
77% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 734
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.28847324527
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.06511611861
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.66325613239
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.8164690777
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
0.33422240557
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.38956479934
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.39)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.993496138445
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.98)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.81549474196
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.83)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.30667951201
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.54694627097
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.55)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.63952093687
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.64)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.702743270855
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 0.71)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.107931920827
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.09)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.45303129228
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.43)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.58662759895
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.59)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.53994067837
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.52)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.34395158419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.37)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.67747858957
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.64)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 735
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.09014082109
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.10)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.13347198788
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.16)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60757360183
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.60)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.66822944687
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.9092773118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.03057691621
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.02)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.82069967898
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.50045854101
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.35332961821
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.36)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.51179980815
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.51)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.72326753752
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.922178658006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.94)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.66215492945
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.40023107032
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.41)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.00946968075884
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.00)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.58724074273
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.62)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.68746013362
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.69)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.88256921849
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.88)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.771953297579
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.72)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.34323965185
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.878692494942
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.88)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
0.808061942919
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.79)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 736
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.84447048325
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.86)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.45245880061
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.49)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.85443839176
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 1.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.41126617867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.43)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.18417048475
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.31538355996
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.95040902145
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08577887344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.07)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.66842711438
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.70)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.941299445725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.91)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 737
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.059118117
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.0501949641691
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.03)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.97657029809
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.25397772673
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.88095231031
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.91)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.19881256366
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.21)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.59364115039
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.23133858789
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.25)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.11779249761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.09)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.09086583206
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.07)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.55701612368
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.29840141159
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.26)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.64559067269
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 738
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.30849848275
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.54962486752
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.57)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.86081879863
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.83)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.17550276749
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.83133910496
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.84)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.31558298118
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.23154745736
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.20)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.97724477827
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.98)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26185038396
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.6078864886
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.948604649502
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.89)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.85598985791
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.96779640145
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.97)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.1520361559
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.13)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.17770960448
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.15)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.12)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.40103602099
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.949758578852
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.93)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.22446830817
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.20)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.68988629339
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.65)
33% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 739
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.6816789263
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.12557615169
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.87704681987
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.89)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.4219790916
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.75592995389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.89404571314
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.1059871579
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.26649743889
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.2404846159
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.04828302828
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.1739697205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.890805047115
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.86)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.982791522345
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.96)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.14235379795
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.12)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.31665793906
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.34)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 740
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.87480631011
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.93)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.52024435884
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.27525425892
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.74065049765
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.05023436679
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.06)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.69798042595
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.64303568194
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.02160506218
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.997632494775
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.23370502796
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.782293672114
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.78)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.4419281254
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.771657235892
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.75)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 741
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.08628150835
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.11)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65916285689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.67)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.17480485995
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.44152300301
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.17404041769
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.12)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.34739211871
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.35)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.2055775392
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.23)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.24983167854
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.27)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.39182869035
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.28940085209
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.27)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.65894757993
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.68)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.41070448148
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.41)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 742
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.51666692729
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.52)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.46461199457
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.51)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.88481322913
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.04487097033
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.11259458247
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.11)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.29592806967
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.32)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.76821087506
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.82)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.67346102266
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.8734842308
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.88)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.71575420372
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.72)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.1640769729
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.17)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.25544134035
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.21)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.68855155735
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.20752525942
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.782885743671
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.78)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.950063471071
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.90)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.18442450005
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.18)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.758528088698
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 0.73)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 743
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.39976618116
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.82108446017
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.65594667739
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.70)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.15102686857
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.14)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 744
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.07822640883
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.11)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.46505262637
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.47)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.92710873384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.14302408739
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.16124904065
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.71019369714
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.73)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.38497055391
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.28206757205
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.85031964218
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.87)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.41642888597
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.39)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 745
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.903195524617
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.44563349071
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.23631766352
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.32291158835
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07296557177
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.05)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.02)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.79501324515
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 746
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.94386354066
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.96)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.96713021618
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.96)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.42476989568
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.42)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.45071513419
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint left. (rewarded 2.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.36459217881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.03333606874
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.49284755283
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.51)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 747
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
0.93228622113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.95)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.65694248155
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.65)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.40429129086
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.40)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.29006221903
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.74480623853
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.72)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.81296408145
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.81)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.87525962066
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.90)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.83621403113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.90)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 748
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.77298511633
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.58980315038
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.55)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.82954047207
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.80)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.13736176992
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.12)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.76190447755
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.3247730969
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.900368568688
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.06305446652
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.01)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.22996901687
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.22)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.65460732567
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.65)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.88673276613
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.88)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.2530853542
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.23)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.09660299564
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.08)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 749
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.72596573867
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.30213133725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.31)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.23532668028
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.64427180435
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.09382607929
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.22663766379
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.35268926347
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2201680096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.8662457906
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.88)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.0992143572759
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.08)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.14346092697
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.10)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.24244154795
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.25)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.913073400684
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.87)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.71887523158
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.06242753116
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.29)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 750
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.4660848799
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.37340282539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.39)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.16158575858
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.13)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.13422408344
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.14)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.00)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.77131518373
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.82)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.48102766885
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 751
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.50683942551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.21943809861
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.87549611817
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.91)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 752
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.12565793195
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.13)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.55044957699
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.52)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.59670638144
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.10315386414
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.05813714724
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.03)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.27118476742
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.54249047187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.79233261574
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.68018001012
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.74365098125
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.505422573349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.48)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.307262024343
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.406038003361
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.40)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.51193190124
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.55)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.63892793044
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.63)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.55205863263
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.59)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.32484350779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.30)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.00195422044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.95)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.798257157136
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.75)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.19)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 753
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.22)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.87287558393
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02384828961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.91684165522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.92867561945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.92)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.11346218934
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.0655234906462
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.02)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.78114883388
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.79)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 754
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.80537454739
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.2004451537
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.8779574824
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.09)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.13)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.34772431704
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.63543542688
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.48369359636
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.43323029281
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.45)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.64612380523
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.65)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 755
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
1.11077161957
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.07)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.89)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.2161614474
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.23)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.43641524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66617449231
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.65)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75532639612
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.36702113634
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.37)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 756
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.9700

/-------------------
| Step 0 Results
\-------------------

learned value
2.16905426437
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.17)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.19885410162
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove left instead of right. (rewarded 1.23)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.42396072397
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.41)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.90966234486
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.13)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75866623304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.31979722639
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.34)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.95)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 1
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.76051549863
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.08)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.43040920291
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.20)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.307262024343
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.23)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.59)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 2
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.11077161957
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.39)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.54395036386
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.36386290047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.922178658006
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.72)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.87)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.84)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 3
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.35907870717
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.88)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.571310816909
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.45)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.04828302828
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 4
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.21)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.90966234486
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.87)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.70)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.72930183206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.19)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.66)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 5
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.46829539217
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.75298734776
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.46829539217
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.57)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.34395158419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.36459217881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.22663766379
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.66617449231
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.66)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 6
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.46505262637
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.36459217881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.08)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.82439694319
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.717371329655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.35)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 7
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.878591109817
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.42396072397
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.52364383426
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.62)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.34395158419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.87)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 8
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.11346218934
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.47)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.75866623304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.96)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.30213133725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.97)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 9
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.51179980815
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.50)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.42201686485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.00)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.25147214468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.02503675819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.59)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.54)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.96)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.36)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.91)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.04)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 10
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.81848091924
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.993496138445
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.94)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.04828302828
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.81450857567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.05)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.31)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.15)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.65916285689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.47)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.98755435939
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 0.74)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 11
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.505422573349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.75592995389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.73)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.45)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.6981292094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 1.41)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.43)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.34395158419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.65)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.28)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.54395036386
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.79)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.50683942551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.64)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.65)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 12
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.23133858789
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.10)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.473790602716
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.70)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.00195422044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.11)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.85)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.00195422044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.04)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.15)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.51179980815
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.54)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.94)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.55044957699
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.42)
25% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 13
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.32291158835
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.91684165522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.65916285689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.17044643494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.76)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.60757360183
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.80)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.2201680096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.51)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.473790602716
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.04)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.206004781383
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.61)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.05907251969
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.43)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.49432341463
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.52)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.00195422044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.47)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 14
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.54249047187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.67)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.23154745736
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.20)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.717371329655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.83621403113
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.42396072397
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.95)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 15
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.91)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.994707170226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.75)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.72474750266
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.25147214468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.30213133725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.59)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.27576376112
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.14346092697
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.49)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.01)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.11346218934
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.95)
57% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 16
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.55044957699
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.42)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.50683942551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.75)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.54249047187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.11779249761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.19)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 17
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.12987672141
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.29)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.75)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.75866623304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.46059549647
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 1.34)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.75592995389
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.54)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.66)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
0.307262024343
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.01)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.6981292094
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.02)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.77)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.03)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 18
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.25)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.34395158419
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.95)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.11779249761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.44)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 19
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.32484350779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.06)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.20)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.406038003361
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.94)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.430097627
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.12)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.81)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 2.61)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.37340282539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.70)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.65916285689
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.46)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
0.269130622636
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent drove right instead of forward. (rewarded 1.25)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.26269768725
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.99)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.00195422044
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.56)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.32)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.12)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.91)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.994707170226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.65)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 20
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.14375011651
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.49)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.32)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.26648875371
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.68)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.02503675819
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.52024435884
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.56)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.39)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.77)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.40)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.57)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.43641524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.80)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
0.994707170226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.76)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 21
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.86)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.92710873384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.01107120341
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.41126617867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.505422573349
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.73)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.58)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.89)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.717371329655
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 22
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.02384828961
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.20487896818
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.78)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.33)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.53)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.50)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.43323029281
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.11)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.92)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.30213133725
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.85)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.49432341463
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.41)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.07070250411
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.20)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.66)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.72930183206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.85)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.86)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 23
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.26)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.41126617867
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.31)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.76190447755
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.92867561945
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.37)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.46)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.59)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.32)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.31979722639
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.37)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.33918192808
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.83)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.37340282539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.05)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.26)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.03)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 24
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.51)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.71)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.64427180435
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.25147214468
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.26)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.03)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 25
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.64)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.06242753116
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.06242753116
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.42201686485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.75)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.8662457906
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.19)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.26269768725
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.35643179257
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.16)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.57)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.98)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.1640769729
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.74)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.58)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 26
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.87490772889
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.57195211242
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
0.473790602716
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.05977558864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded -0.01)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.77298511633
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.78)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.55)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.92)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.05)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.54249047187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.84)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.24)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.86)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.45)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
1.37340282539
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.44)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.91684165522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.69)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.04)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 27
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.19378510416
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.98)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.52364383426
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.17)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.60757360183
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.92)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.48)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.62)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.43)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.43040920291
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.20)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.45610889389
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded -0.09)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.16905426437
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.51)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.83161747502
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.61)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
0.571310816909
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.67)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.08)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.39)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.65694248155
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.57)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.16905426437
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.27)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.24244154795
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 1.02)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.23)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.23154745736
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.35)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.38)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 28
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.11779249761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.03)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.23631766352
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.77)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.89)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.51176691272
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.71)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.994707170226
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.71)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.54249047187
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.33)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 29
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.43303395505
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 0.54)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.78114883388
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.99)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.26)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.19)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.43323029281
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.27)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.42201686485
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.25706134233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.50463413045
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.22394689326
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.29)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 30
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.22798150056
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.92710873384
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.72930183206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.74)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.72930183206
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.8315210607
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.22)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 31
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.63892793044
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.56)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
1.90966234486
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.75866623304
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.92)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.72)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.71)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.69)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.36386290047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.48)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.31596998049
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.10)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.10)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 32
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.50683942551
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 2.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.79)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.10)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.84)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.55205863263
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 2.50)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.66617449231
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.36)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 33
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.36386290047
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.1640769729
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.69)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.03530110708
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.27)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.14743734451
Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 34
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.3612895441
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.62)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.57)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.29556981855
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.03)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.43641524
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, None)
Agent followed the waypoint forward. (rewarded 1.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.68)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.35)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.87)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.88)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 35
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.64)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.73)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.87)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.0271899979
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.03)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.05907251969
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.57)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.36459217881
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
1.51176691272
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 1.18)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
1.33918192808
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.82)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.63892793044
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.43)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
2.07764543861
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.34)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.94)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
0.406038003361
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.77)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.61)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
1.11779249761
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.86)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
1.2206221683
Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 2.12)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.2201680096
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.07)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

learned value
0.406038003361
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, None)
Agent drove right instead of left. (rewarded 0.87)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

learned value
1.80436500232
Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.29)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 1.58)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

learned value
2.37656192994
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, None)
Agent followed the waypoint right. (rewarded 0.95)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

learned value
1.22024491723
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 2.12)
29% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 36
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.691308231233
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.26)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.307262024343
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.12)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.35158010313
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.05977558864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.17044643494
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.19)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
2.26031792944
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.84)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.46)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.76)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.58)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.90)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.27)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.31)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 0.64)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 37
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.28)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.81450857567
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.05)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.43323029281
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.2404846159
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.11554600753
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.65905212727
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
1.11077161957
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.95)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 38
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
0.0
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
2.06259416503
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 2.20)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.07770193347
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
2.91684165522
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.56)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 39
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
2.50020799741
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.68)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.900368568688
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.57)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.05977558864
Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.02352629932
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.88095231031
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.58)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
-0.100269244167
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.85)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.62636453141
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.47)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

learned value
1.32484350779
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.11)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

learned value
1.89847675193
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint left. (rewarded 1.53)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.45)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 0.96)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.42)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.27)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

learned value
2.1640769729
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.30)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 1.07)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

learned value
2.86841584919
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, None)
Agent properly idled at a red light. (rewarded 2.12)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

learned value
0.989868711442
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, None)
Agent followed the waypoint forward. (rewarded 1.86)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

learned value
2.78114883388
Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

learned value
2.80527273673
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.90)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 40
\-------------------------

Simulating trial. . . 
epsilon = 1.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

learned value
1.29556981855
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.08)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

learned value
0.993496138445
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.80)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

learned value
2.08809023264
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

learned value
1.05907251969
Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;, None)
Agent followed the waypoint left. (rewarded 2.84)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

learned value
1.1683340411
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

Simulation ended. . . 
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>



<div class="output_text output_subarea ">
<pre>&lt;matplotlib.figure.Figure at 0xe29b198&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>On running agent.py, I notice that the smart cab does not move at all.The driving agent is getting both positive and negative rewards.Since the vehicle is not moving, when light changes red, it gets a positive reward.This is because the smart cab is not expected to move when the light is red.However, when the light changes green and when there is no oncoming traffic, the smartcab  is expected to move.Since it is not moving, it receives a big negative reward.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Understand-the-Code">Understand the Code<a class="anchor-link" href="#Understand-the-Code">&#182;</a></h3><p>In addition to understanding the world, it is also necessary to understand the code itself that governs how the world, simulation, and so on operate. Attempting to create a driving agent would be difficult without having at least explored the <em>"hidden"</em> devices that make everything work. In the <code>/smartcab/</code> top-level directory, there are two folders: <code>/logs/</code> (which will be used later) and <code>/smartcab/</code>. Open the <code>/smartcab/</code> folder and explore each Python file included, then answer the following question.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-2">Question 2<a class="anchor-link" href="#Question-2">&#182;</a></h3><ul>
<li><em>In the </em><code>agent.py</code><em> Python file, choose three flags that can be set and explain how they change the simulation.</em></li>
<li><em>In the </em><code>environment.py</code><em> Python file, what Environment class function is called when an agent performs an action?</em></li>
<li><em>In the </em><code>simulator.py</code><em> Python file, what is the difference between the </em><code>'render_text()'</code><em> function and the </em><code>'render()'</code><em> function?</em></li>
<li><em>In the </em><code>planner.py</code><em> Python file, will the </em><code>'next_waypoint()</code><em> function consider the North-South or East-West direction first?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>In agent.py, the following 3 flags can be set.</p>
<ol>
<li>learning   - The driving agent will use Q-learning if flag is set to True.If it is set to false, Agent will not use Q-learning</li>
<li>enforce_deadline - If this flag is set to True, then a penalty is given if deadline is not met.If not, no penalty is awarded.</li>
<li>display - If this flag is set to 'true', pyGame Frame is enabled .Else it is disabled.</li>
</ol>
<p>In environment.py, the Environment class function "act" is called when an agent performs an action.</p>
<p>In simulator.py, render_text() is the non-GUI version of the simulation.It writes status messages on to the console.render() function is the GUI version of the simulation.Messages are displayed on the pyGame frame.</p>
<p>In planner.py,  the 'next_waypoint() function will consider East-West direction first.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Implement-a-Basic-Driving-Agent">Implement a Basic Driving Agent<a class="anchor-link" href="#Implement-a-Basic-Driving-Agent">&#182;</a></h2><p>The first step to creating an optimized Q-Learning driving agent is getting the agent to actually take valid actions. In this case, a valid action is one of <code>None</code>, (do nothing) <code>'left'</code> (turn left), <code>right'</code> (turn right), or <code>'forward'</code> (go forward). For your first implementation, navigate to the <code>'choose_action()'</code> agent function and make the driving agent randomly choose one of these actions. Note that you have access to several class variables that will help you write this functionality, such as <code>'self.learning'</code> and <code>'self.valid_actions'</code>. Once implemented, run the agent file and simulation briefly to confirm that your driving agent is taking a random action each time step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Basic-Agent-Simulation-Results">Basic Agent Simulation Results<a class="anchor-link" href="#Basic-Agent-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial simulation, you will need to adjust following flags:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file in <code>/logs/</code>.</li>
<li><code>'n_test'</code> - Set this to <code>'10'</code> to perform 10 testing trials.</li>
</ul>
<p>Optionally, you may disable to the visual simulation (which can make the trials go faster) by setting the <code>'display'</code> flag to <code>False</code>. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the initial simulation (there should have been 20 training trials and 10 testing trials), run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!
Run the agent.py file after setting the flags from projects/smartcab folder instead of projects/smartcab/smartcab.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_no-learning&#39; log file from the initial simulation results</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_no-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1gAAAI4CAYAAAB3HEhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-3">Question 3<a class="anchor-link" href="#Question-3">&#182;</a></h3><p>Using the visualization above that was produced from your initial simulation, provide an analysis and make several observations about the driving agent. Be sure that you are making at least one observation about each panel present in the visualization. Some things you could consider:</p>
<ul>
<li><em>How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents?</em></li>
<li><em>Given that the agent is driving randomly, does the rate of reliability make sense?</em></li>
<li><em>What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily?</em></li>
<li><em>As the number of trials increases, does the outcome of results change significantly?</em></li>
<li><em>Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>The driving agent is making bad decisions 43 to 48% of the times.These bad decisions cause accidents 5% to 7.5% of the times.</p>
<p>The rolling rate of reliability falls drastically as the number of trails increase.It gets close to zero percent towards the end.Since the agent's driving is random, this makes perfect sense.</p>
<p>The rolling average rewards per action is net negative and it continues the downward trend with increasing trails.This indicates that the agent is penalized heavily.</p>
<p>Since the actions taken are random, the probability that the agent would do an appropriate action or cause a violation or accident is same.The rolling frequency of bad actions stays within a range.This indicates that the outcome of results dont change significantly.</p>
<p>This smartcab cannot be considered reliable or safe for its passengers.Each action it takes is random and is not decided based on current circumstances.The random action increases the probability for a violation or accident.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Inform-the-Driving-Agent">Inform the Driving Agent<a class="anchor-link" href="#Inform-the-Driving-Agent">&#182;</a></h2><p>The second step to creating an optimized Q-learning driving agent is defining a set of states that the agent can occupy in the environment. Depending on the input, sensory data, and additional variables available to the driving agent, a set of states can be defined for the agent so that it can eventually <em>learn</em> what action it should take when occupying a state. The condition of <code>'if state then action'</code> for each state is called a <strong>policy</strong>, and is ultimately what the driving agent is expected to learn. Without defining states, the driving agent would never understand which action is most optimal -- or even what environmental variables and conditions it cares about!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Identify-States">Identify States<a class="anchor-link" href="#Identify-States">&#182;</a></h3><p>Inspecting the <code>'build_state()'</code> agent function shows that the driving agent is given the following data from the environment:</p>
<ul>
<li><code>'waypoint'</code>, which is the direction the <em>Smartcab</em> should drive leading to the destination, relative to the <em>Smartcab</em>'s heading.</li>
<li><code>'inputs'</code>, which is the sensor data from the <em>Smartcab</em>. It includes <ul>
<li><code>'light'</code>, the color of the light.</li>
<li><code>'left'</code>, the intended direction of travel for a vehicle to the <em>Smartcab</em>'s left. Returns <code>None</code> if no vehicle is present.</li>
<li><code>'right'</code>, the intended direction of travel for a vehicle to the <em>Smartcab</em>'s right. Returns <code>None</code> if no vehicle is present.</li>
<li><code>'oncoming'</code>, the intended direction of travel for a vehicle across the intersection from the <em>Smartcab</em>. Returns <code>None</code> if no vehicle is present.</li>
</ul>
</li>
<li><code>'deadline'</code>, which is the number of actions remaining for the <em>Smartcab</em> to reach the destination before running out of time.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-4">Question 4<a class="anchor-link" href="#Question-4">&#182;</a></h3><p><em>Which features available to the agent are most relevant for learning both <strong>safety</strong> and <strong>efficiency</strong>? Why are these features appropriate for modeling the </em>Smartcab<em> in the environment? If you did not choose some features, why are those features</em> not <em>appropriate? Please note that whatever features you eventually choose for your agent's state, must be argued for here. That is: your code in agent.py should reflect the features chosen in this answer.</em></p>
<p>NOTE: You are not allowed to engineer new features for the smartcab.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>'waypoint' and 'inputs' are relevant for both safety and efficiency.Waypoint indicates which direction the smartcab should head to.If we have to move in that direction safely, we would need the 'inputs' feature.We need the'light' feature to determine if the cab should move or stay in that place to remain safe.The 'oncoming' feature would help us determine if the cab can merge right in an intersection.'left' and 'right' features will be needed if we have to switch lanes or trying to turn to left from right lane or turn right from left lanes.</p>
<p>'deadline' feature simply indicates the number of actions that can be taken to reach the destination before the allotted time ends.The next safest action of the smartcab will be mostly dictated by current conditions.'deadline' may not help in this case.Efficiency can be ensured by setting up proper rewards for the next safest action when multiple potential paths are available.Hence 'deadline' feature may not be appropriate for modeling the environment.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-a-State-Space">Define a State Space<a class="anchor-link" href="#Define-a-State-Space">&#182;</a></h3><p>When defining a set of states that the agent can occupy, it is necessary to consider the <em>size</em> of the state space. That is to say, if you expect the driving agent to learn a <strong>policy</strong> for each state, you would need to have an optimal action for <em>every</em> state the agent can occupy. If the number of all possible states is very large, it might be the case that the driving agent never learns what to do in some states, which can lead to uninformed decisions. For example, consider a case where the following features are used to define the state of the <em>Smartcab</em>:</p>
<p><code>('is_raining', 'is_foggy', 'is_red_light', 'turn_left', 'no_traffic', 'previous_turn_left', 'time_of_day')</code>.</p>
<p>How frequently would the agent occupy a state like <code>(False, True, True, True, False, False, '3AM')</code>? Without a near-infinite amount of time for training, it's doubtful the agent would ever learn the proper action!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-5">Question 5<a class="anchor-link" href="#Question-5">&#182;</a></h3><p><em>If a state is defined using the features you've selected from <strong>Question 4</strong>, what would be the size of the state space? Given what you know about the environment and how it is simulated, do you think the driving agent could learn a policy for each possible state within a reasonable number of training trials?</em><br>
<strong>Hint:</strong> Consider the <em>combinations</em> of features to calculate the total number of states!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<table>
<thead><tr>
<th style="text-align:center">attribute</th>
<th style="text-align:center">No of possible values</th>
<th style="text-align:center">values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">waypoint</td>
<td style="text-align:center">3</td>
<td style="text-align:center">Right,left,forward</td>
</tr>
<tr>
<td style="text-align:center">light</td>
<td style="text-align:center">2</td>
<td style="text-align:center">Green,red</td>
</tr>
<tr>
<td style="text-align:center">left</td>
<td style="text-align:center">4</td>
<td style="text-align:center">None,Right,left,forward</td>
</tr>
<tr>
<td style="text-align:center">right</td>
<td style="text-align:center">4</td>
<td style="text-align:center">None,Right,left,forward</td>
</tr>
<tr>
<td style="text-align:center">oncoming</td>
<td style="text-align:center">4</td>
<td style="text-align:center">None,Right,left,forward</td>
</tr>
</tbody>
</table>
<p>The total Number of possible combinations is 3X2X4X4X4=384 states</p>
<p>Since there are 384 states, the driving agent could learn a policy for each of these states.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Update-the-Driving-Agent-State">Update the Driving Agent State<a class="anchor-link" href="#Update-the-Driving-Agent-State">&#182;</a></h3><p>For your second implementation, navigate to the <code>'build_state()'</code> agent function. With the justification you've provided in <strong>Question 4</strong>, you will now set the <code>'state'</code> variable to a tuple of all the features necessary for Q-Learning. Confirm your driving agent is updating its state by running the agent file and simulation briefly and note whether the state is displaying. If the visual simulation is used, confirm that the updated state corresponds with what is seen in the simulation.</p>
<p><strong>Note:</strong> Remember to reset simulation flags to their default setting when making this observation!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Implement-a-Q-Learning-Driving-Agent">Implement a Q-Learning Driving Agent<a class="anchor-link" href="#Implement-a-Q-Learning-Driving-Agent">&#182;</a></h2><p>The third step to creating an optimized Q-Learning agent is to begin implementing the functionality of Q-Learning itself. The concept of Q-Learning is fairly straightforward: For every state the agent visits, create an entry in the Q-table for all state-action pairs available. Then, when the agent encounters a state and performs an action, update the Q-value associated with that state-action pair based on the reward received and the iterative update rule implemented. Of course, additional benefits come from Q-Learning, such that we can have the agent choose the <em>best</em> action for each state based on the Q-values of each state-action pair possible. For this project, you will be implementing a <em>decaying,</em> $\epsilon$<em>-greedy</em> Q-learning algorithm with <em>no</em> discount factor. Follow the implementation instructions under each <strong>TODO</strong> in the agent functions.</p>
<p>Note that the agent attribute <code>self.Q</code> is a dictionary: This is how the Q-table will be formed. Each state will be a key of the <code>self.Q</code> dictionary, and each value will then be another dictionary that holds the <em>action</em> and <em>Q-value</em>. Here is an example:</p>

<pre><code>{ 'state-1': { 
    'action-1' : Qvalue-1,
    'action-2' : Qvalue-2,
     ...
   },
  'state-2': {
    'action-1' : Qvalue-1,
     ...
   },
   ...
}</code></pre>
<p>Furthermore, note that you are expected to use a <em>decaying</em> $\epsilon$ <em>(exploration) factor</em>. Hence, as the number of trials increases, $\epsilon$ should decrease towards 0. This is because the agent is expected to learn from its behavior and begin acting on its learned behavior. Additionally, The agent will be tested on what it has learned after $\epsilon$ has passed a certain threshold (the default threshold is 0.05). For the initial Q-Learning implementation, you will be implementing a linear decaying function for $\epsilon$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Q-Learning-Simulation-Results">Q-Learning Simulation Results<a class="anchor-link" href="#Q-Learning-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file and the Q-table as a <code>.txt</code> file in <code>/logs/</code>.</li>
<li><code>'n_test'</code> - Set this to <code>'10'</code> to perform 10 testing trials.</li>
<li><code>'learning'</code> - Set this to <code>'True'</code> to tell the driving agent to use your Q-Learning implementation.</li>
</ul>
<p>In addition, use the following decay function for $\epsilon$:</p>
$$ \epsilon_{t+1} = \epsilon_{t} - 0.05, \hspace{10px}\textrm{for trial number } t$$<p>If you have difficulty getting your implementation to work, try setting the <code>'verbose'</code> flag to <code>True</code> to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the initial Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_default-learning&#39; file from the default Q-Learning simulation</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_default-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1gAAAI4CAYAAAB3HEhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xd4FVX6wPHvm1CCdOkCCigg7YYOIr2pCBHEQhDQxQK6
iK6iRlEDLKKr+NNVdxdZVlEWA4o0xbK0CIioQToECwZpIoQuNcn7+2Mm13uTm0JIuCnv53nuk8yc
mTPvzC1nzsyZc0RVMcYYY4wxxhhz4UKCHYAxxhhjjDHGFBZWwTLGGGOMMcaYXGIVLGOMMcYYY4zJ
JVbBMsYYY4wxxphcYhUsY4wxxhhjjMklVsEyxhhjjDHGmFxiFawiSkR+EpFrsrFcmIioiNTKgxiu
F5EffaZ/FZGO7v/jReSN3N7mhRKRkSKy5ALWXyYit+dmTCb7RKSpiGwUkRMict9F2N4aERlyEbZT
0t2ny/J6W8bkJ1aWmWARkVki8nSw47hQVn7kDatgBYmIjBKROBE5IyLTA6T3EJF4ETkpIstF5IoM
8rnT/WKcEJFTIpLiM30ko+2r6pWq+lUu7McaETntbu+AiLwvIlUuNF9VjVbVUReaT1o+hezvbsy7
ReRvIiJ5sK0XRGSa7zxV7a6qs3N5O2n36YSI/Jqb2yhEngQWqWoZVZ2aNjHN5/mo+91rlJcBuZV2
FZGbzmMdv4qbqp5x92lv3kRpTGBWlmUur8qyVCISKiJ7RGRdXm3jYnMrqCfd92KfiEwTkVLBjis/
svIj/7IKVvDsBSYCb6VNEJHKwFzgGeBSIA4IeFKuqu+4X4wyQD/gl9RpVa0QIO9iubgPqe5xt98Q
qAq8kAfbyG0N3Zh7An8C8vwuw0XQ0Oe9rx5ogTx6/wuSK4AtWSyT+nmuBHwDvJ3HMd0JHAKG5fF2
jMkLVpYFV0+gDNBURJrlxQaCVG70dt+L1kAHYEwQYgCCX25msX0rP/Ipq2AFiarOVdX5QGKA5JuB
Lar6gaqeBsYB4SJydU625V4NGiMiW4BjPvNSmzBcKyJfi8gREdkrIq/k5AdFVQ8BC4HmPtsuJSL/
cK9C7RaRl0SkeDZi9t79EZGrRSRJRP7k5nFARB7zWbaMiLznxr9ZRJ4Un+YaWcQcD6xJE/OlIvKu
e4x2iUi0iAT8rojIv9yYjonINyLS3p3fH3gESL0q+407f42IDBGRS9z5V/nkVdO9clvRnR4gTnO2
IyKyUkQaZ2ef0sR3vYj8KCLPiMh+4F9Z5S0ibUVkg4gcF5H/ishccZtBSJomkpKm2Y37fr/qHrdf
ReR1ESmZJpan3Pdwj4jc4ZNXaRF5zV33qIh8ISLFRGSpiNybZr+2i8gNGezzQBHZ6u7bEhGp785f
DVwDTHOP/eWZHTtVTcI5GfQ9Npl+V0TkRhH5wU3/vyzeHkSkAdAOGAH0FZFL06Tf6r5Px918e4jI
y0Abn/14OcD7cKn7nTggIj+LyOMizl1a9z1c6h7rI+I0serps817RSTB3eYOEbk1q/0wRZeVZVnG
nNdl2Z3AHGCx+39qXneKyKo0sTwpIu/77E9Wv9XeckNEqojIp27Mh0RkgYjU8Mm7voisdn83PhOR
N8WnBYeIdPJ5b74TkWuzOnYAqroHWEL69yKj2L8WkRvd/3u4v4s93OkbRWSNz3sR6+7LARF5R0TK
+mwj0GfNr2wESmQUt/s7u8w9DsfEKZM6+6RneJ7hs+4/ROQwEJXBNvJd+WH+YBWs/KkJsCF1QlV/
B3505+fU7UAvnKvyaZ0DRrlpnXCuHt5zvhsQpzlFf5xYU40HPEAzoBXQFXj8fPMGQnGuZF0F9AGe
E5F6btpEoArO3YkbgaHnEXMTnJNu35hnAkeBekBbnH3KKM+vcPatErAA+EBEirsnHP8HpF6Vbeu7
kqqexCnAI31mDwI+V9XD4lTU/olzd60SMAOYn5OTBaAOUByoDYzOLG9xmmEsAN7EueL8KRBxHtv6
P6AWzjFpCDTAv3C4AhDgMpzP3BQRKeOmvQZcjfPjfynwNKDAO/jcYRSRdkA54H9pNy7OFdzpwAM4
V6C/ABaISDFV7QB8i3uVWlV/yWxH3AJ7ME4FPFWG3xX3ZON94FGcz+MBnM9sZu4EVqnqHOAXfD4P
bmE8FXgIKA/0AHap6qNp9uPRAPlOwXnP6+J87+939yVVZ5y7CZWAN4DUE8CKwEtAD1UtC3QENmex
D8ZkxMqy9HKtLBORcm6cM93XHSIS6ibPA1qK/4WkwcB77v9Z/VbXwafcwDlfnAJcjvO7AvCKG4fg
/PYtxzn2L+D/m10HmA+M5Y/f9vnu702m3Ph74/9eZBb7FzjvDUAXYAfO713q9Bc++UwAqvvkMzbN
5r2ftRyWjZ1xPv+px2S++55B1ucZnYH1QGXg5Qzyz1flh0lDVe0VxBfOD+r0NPP+A7yQZt6XwF1Z
5NUTSAgw/1dgcIB5HTPIJwqIcf8PwznJrZXBsmuA33Gu8CjOF/cyn/Q9QHef6ZuAePf/64EfA8WE
82M0zf3/ajfvyj7LbgT6u//vBbr4pI3yzTdNvKn7c9SNW3FOyIu76Ve484v7rPMn4FP3/5HAkgzy
FuAkTlM9v31Ic7yGuP/3Bbb6pK0FbnP/fxsYm2bdnUC7LPbpiPt60ecYp92fDPPGKch+TpP2HfB0
oP33/XwAxYCzQE2f9G7ANp9YjgIhPunHcK5MFsc5OWoYYP9Ku8td7k6/AfxfBu/Bc8C7PtOhOBWd
9mmPfxaf5yPuvhwCOmWyvO935T4gNs22f8toezgnLL8AI93p8cDXPunvAM9nEucQn2nf96EkkAzU
80l/CPjM5z3c7JN2qbtuBaCiu+83AWEZ7be97JX2hZVlF7Usc9PvceMKwfmd/B24wSd9DvC4+38z
4DDOXZfs/Fb7lRsBtt0e2Of+3wA4BZRMs+3U/Y4G/p1m/S+A2zPI+1fguPtS4DOgrJuWVew3At+4
/8e6xyjWnf4a6JPBNgcBX2X0WSOLsjFAfiMDLL8RuJXsnWd8n8V3JN+VH5nFWxRfdgcrfzqBc4Xe
V3nguIhcLn88+HviPPLclVGCiDR2b/3vF5FjwLM4V02ya4SqlgNa4lwNuszNV9zpnT7L7gRqnkfe
qZJV9aDP9EmgjHtLvTr++5fhvvpoApTFabd8LXCJO/8KnB+bA+7t7yPA34FqgTIRp8nFdhE5ilN4
hZH9Y/c5UE1EwkWkIVAf+MgnjqdSY3DjqELmx66JqlZwX75XVn9V1XM+05nlfRmwO02+O8mey3Aq
Slt88p2Pcycp1QFVTfGZPonz/EANnILzp7SZqnPVey7O1dniOFcVZ2QSw06fdZNxTkDO5zM3Qp1n
PsKAW4CPxG3SlMV35TJ8Pns+285IN5zP7gfu9EygrfzRfKo2AY5HNlTnj8I3VdrvnW8nKCfdv2VU
9TBwB84V619FZKH4NGM15jxZWZZebpZldwKzVDXF/Z1cgE8zQZy7Val3NQYDc1T1LNn7rfYrN0Sk
rIi8JSK/uMf2f/j/9h1Q1TMZxH4FMCRNmdPaXS8jN6hzF7030BTnRD51W5nFvgqnGWplnLtS7wAN
3elwNx0RuUxEPhCnqfoxnLswaT8rvvuQk7Ix0PKXkb3zjKze+3xXfuRgW4WaVbDypy04PwSA82wK
cCVOW3bfB3/P5wOtmaT9G+dKzJVu4TIB527MeVHVdcCLwOvutOJ8Ea/wWexyMj/pPN9tpgD7ca68
pKqd3XVVdQbOVaUn3dm7cE4KKvpUVsqpasu064tIL+BBYADO1f9Lca7ipR67zI45buE1B6cAHAzM
U9VTPnE86xNDBVW9RFXnZmff0m4qzXRmee/D/1iC856l+p0/KqPg/Bin2gck4XyOUvMtr6qBmvKk
5V03g/R3cE78rwf2u5+1QPbi83lzm8vUJAefOffzsQzneKW2Mc/su7IPn8+ee8KU2QnYnTi/wVvE
6fVxBc57lXqCtIuMj0dmn61fgRT837dsf+9UdZGq9sA5EfgF97k9Y3LAyrLsb/O8yjIRuRKnCe/d
4jzH8ytOq4ibRKS8u9gnQF1xekIdxB/NA7PzW532OEe5sbVxj21v/H/7qrjNqgPFvgvnbpZvmVNa
VV/J5JA4QaguxnkW9m/ZiV1Vj+I0a34EWOuWs3Hu9GZVPebm8xJOedbU3Z97SP9Z8T0GWZWNgQRa
fi/ZO8/I9PyBfFp+mD9YBStI3OddwnCaEYWK85Bh6vM183B6BBroLhMNbFCnQ4a8UBY4qqon3GeS
7s1qhUxMA64Skevc6RggWkQqiUhVnDbO/72wcNN5HxgrIuXd9tr3n+f6zwN/FpFKqvozzu3zF90r
diHiPLzbMcB6ZXGatR3AaXYxAeeqVKr9OIVbZgX8ezgFXyR/FH7gtJ1+UERai6OMiESIyCUBczk/
meW9AggT50HWYiISifPcQar1QAsRaeIu/2xqgluQvQX8XUQqu3nXdiuimXLXfdddt5o4XQ93lD+e
J4jFOd7PuctlZDYwQEQ6u3e7onAevo/L1pFJQ5x27PX5o+fBzL4rC4E2ItLX3fZj/HHVNW2+ZXA6
ALgLp4lk6msMzpXeEJzv0gh3X0LcY9nAzWI/Ttv9dNyryPOASeJ0HHIlThOPLL934nS0cqP73p7B
OQlIyWI1U4RZWZarzqcsG4bzfM/V/PH70RDn9+42AHU6FpmH83xrcdznj3L4W10W527FEfdukO/4
T98D24GnRaS4+7t5vU/6O8Ct4nSyECpOJxU9RCRgb7cBvIxTcWyUzdi/wGlemfq8VWya6dT9OQEc
c4/1I1nEkFXZGEhtn+WH4FQ6/3ee5xnp5Nfyw/izClbwPI1ztyMK52HQU+48VPUAMBDnZPIwzgOQ
g/Iwlr8A94jTTOMfZNCNbna4d2DewOmWF5wT8K04J6jrcdrfv3hB0ab3NM5x2onz4On7OCeH2aKq
cfxxhQucyk4FIB7nGZzZBG4i+BHOj+5POA/SHsSpbKWahXO355A4PdgFsgLnxKQ8Tk9JqTF9idNM
602cZ2K+x7nLldVVrSxllrf7/g3A6STiME579o981t2E8/6txDk+sWmyfxjnCl0czvNWn+E8zJ0d
o3GO5Tqck4S/4l5RdK8gz8Bp2jkzk33bCNzt7tsBnAd7b1KnR8DsSu1d6QROIfWoqi530zL8rqjq
Ppzv6avutquRccXuFpzPVoyq/pr6wqn8lsN51mMlTnv3f+Icy6X8cUX0FWCYiBwWkUDfpxHu353A
Mnc/MjxuPkJxfpN+xXkP2uCcmBiTESvLck+2yjL3ot0w4B++vx/ub9BU0jcT7AnMTtM8+3x/qyfj
NKFLxGlm90lqgvv7fLu7ncPAUzhN18646TtwPgfjccrJnTgn7dk6B1VnfKZZ/FGpyyr2L3AqUCsy
mAbn/ezorj8P+DCLGDItGzOwAmiB81s/FrjZvcMG2T/PCCS/lh/GhzjfC2MKDxH5C3C9ql6X5cIm
SyIyC6dpxcQgx3EfTicg1iWsMabQK8hlmYgsANao6vPBjiUYRGQkcIuVV0WX3cEyBZ5767u9exu8
Cc6VsXnBjsvkHnGe3bgf5wqdMcYUOgW5LBORdiJSx429H04TwQXBjsuYYMnTCpY4g9VtF2fAunQD
pYnIHeIMgrZJnAHqfB+GTXDnrxeROJ/5L4lIvLvePBGp4M7vJSJr3XXWikj3vNw3k6+UxGmTfRyn
qcAsbFyGQkNEInC6O/8Rp1MQY4wpjApyWVYLp+ngcZwOJIar6tbghmRM8ORZE0H34fTvcQYp240z
pkSk7xdORDrgjF1wWERuAMapajs3LQForf7dmSIivYFlqpokIn8DUNUnRKQFTu9ie0WkKc6ArTnp
QtUYY4wxxhhjciQv72C1xRkgb4c64y7MwhmYz0tVV6sz7go4Paqk7dIyHVX9n88D6951VHWd+yAk
OA+hlhL/LkONMcYYY4wxJk8Vy3qRHKuJ/0Bpu4F2mSx/N06vOakUWCIiycCbqhro2YvhBO4laCDw
nfoPegd4H5S/D6B06dKtrr766rSLGGOMyQfWrl17UFWrBDuO3Fa5cmWtU6dOsMMwxhiTRm6VO3lZ
wco2EemGU8HyHQOgo6ruccebWCwi8aq6wmedsTiDzc1Mk1cTnAHpegfalltRmwrQunVrjYvL0fA4
xhhj8piI7Ax2DHmhTp06WNljjDH5T26VO3nZRHAP/iN51yLASNAi4sF5iPMmVU1Mna+qe9y/v+H0
otPWZ527cEYsv0N9HiITkVrussNU9afc3BljjDHGGGOMyUpeVrC+BeqLSF0RKYEzuOBC3wXc0bPn
AkNV9Xuf+aVFpGzq/zh3oza709cDjwMRqnrSZ50KwCIgyh1I1RhjjMlzWfWYa4wxpmjJsyaCbi9/
o4DPgVDgLVXd4g6+hqpOwRlJuxLwT2dgcpJUtTXOaNbz3HnFgPdU9TM36zdwujJd7KavUdWRwCic
kbyfFZFn3WV7u3fAjDHGmFzn9pj7D3x6zBWRhdZFtTHGFF151k17QWDPYBmTv507d47du3dz+vTp
YIdi8lBYWBi1atWiePHifvNFZK170S3fEpFrcIYYuc6dfhJAVZ/PaJ2yZctqq1at/ObddtttPPDA
A5w8eZI+ffqkW+euu+7irrvu4uDBg9xyyy3p0u+//35uv/12du3axdChQ9OlP/roo/Tr14/t27cz
YsSIdOlPP/00PXv2ZP369Tz88MPp0idNmkSHDh1YvXo1Tz31VLr0V199lebNm7NkyRImTpyYLv3N
N9+kYcOGfPTRR7z88svp0mfMmEHt2rWZPXs2//rXv9Klz5kzh8qVKzN9+nSmT5+eLv2TTz7hkksu
4Z///Cfvv/9+uvTY2FgAJk+ezMcff+yXVqpUKT791Olj669//StLly71S69UqRIffvghAE8++SRf
ffWVX3qtWrX473//C8DDDz/M+vXr/dIbNGjA1KlOP1333Xcf33//vV968+bNefXVVwEYMmQIu3fv
9ku/5ppreP555+M0cOBAEhMT/dJ79OjBM888A8ANN9zAqVOn/NL79u3LmDFjAOjatStp2WfPPntg
n73Uz94XX3yRK+VOvujkwhhjAtm9ezdly5alTp06uHesTSGjqiQmJrJ7927q1q0b7HByIls95vr2
YFuypI0gYowxhZndwbI7WMbkW9u2bePqq6+2ylUhp6rEx8fTqFEjv/kF5A7WLcD1qnqPOz0UaKeq
ozJax8oeY4zJn3Kr3MnLTi6MMeaCWeWq8Cvg73G2esw1xhhTdFgFyxhjjMm5LHvMNcYYU7RYBcsY
YwJITEykefPmNG/enOrVq1OzZk3v9NmzZ9Mtf+jQIaZMmZJlvklJSVSoUCHg/NDQUO82WrVqxZo1
a84r5qefftr7wHIgTZs2ZciQIVnms2PHDmbNmuWd/vrrr/nLX/5yXrEUFaqahNOL7efANuB9Vd0S
3KiMMcYEk3VyYYwxAVSqVMnbI9O4ceMoU6aMtzekQFIrWCNHjszxNsuWLevd5qJFixg7dmy6XqVy
atOmTRQrVozly5dz6tQpSpUqleGyqRWsQYMGAdCuXTvatUvXb4NxqeonwCfBjsMYY0z+UKTvYJ06
dYp+/fpx/PjxYIdijClAXnzxRZo2bUrTpk15/fXXAYiKimL79u00b96cqKgojh07Rvfu3WnZsiUe
jydd97xZOXbsGBUrVvT+n1FeEyZMoEGDBnTs2JEffvghw/xiYmIYNmwY3bt356OPPvLO//777+ne
vTvh4eG0bNmShIQEoqKiWL58Oc2bN+e1115jyZIl9O/fH4CDBw8SERGBx+OhQ4cObN68GXDunt19
99106dKFevXq8Y9//AOA48ePc8MNNxAeHk7Tpk2ZM2fOeR0HY4wxpsBR1SL7Cg0NVUB79uypZ86c
UWNM/rJ161a/6ejoaAUU0Ojo6HTLP/LII970yZMnp0u/9957velvvvlmtuOIjo7Wl156SVVV16xZ
ox6PR0+ePKnHjh3Tq6++Wjdu3Kg//PCDhoeHe9c5e/asHj16VFVV9+/fr1dddZWqqp47d07Lly+f
bhvnzp3TkJAQDQ8P14YNG2r58uX1u+++yzSvr7/+2hvLkSNHtE6dOvrKK68E3Icrr7xSd+/erYsW
LdL+/ft757ds2VIXLlyoqqqnTp3S33//XRcvXqw33XSTdxnf6ZEjR+rEiRNVVfXzzz/XVq1aqarq
2LFjtWPHjnrmzBndv3+/XnrppZqUlKSzZs3SkSNHevM6cuRIwPjSvteqqkCc5oOyIrdfqcfMGGNM
/pJb5U6RvoOVnJwMwDfffEN8fHyQozHGFASrVq1i4MCBlCpVirJly9K/f39WrlyZbjlVJSoqCo/H
Q+/evdm1axcHDx7MNO/UJoLx8fF8/PHHDBs2LNO8VqxY4Y2lfPny9OvXL2C+a9asoWbNmtSsWZNe
vXrxzTffcPToUQ4fPszBgwe964WFhXHJJZdkuf+pAzr27t2bvXv38vvvvwPOoJIlSpSgatWqXHrp
pRw4cACPx8Nnn31GVFQUX375JeXLl8/8ABtjjDEFXJGuYF122WVUr16dFStW4PF4gh2OMaYQeffd
dzl69Cjfffcd69evp3Llypw+fTrb63fs2JG9e/dy6NChC84rJiaGzZs3U6dOHerXr8+xY8eYO3du
TnYrU74D6IaGhpKUlESjRo2Ii4ujSZMmREVFMWnSpFzfrjHGGJOfFOkKVo0aNdi0aRPh4eHBDsUY
kw3jxo3z3n4fN25cuvSXX37Zm/7oo4+mS586dao3/b777stRDJ06dWLevHmcOnWKEydOsGDBAjp1
6kTZsmX9nuc8evQoVatWpVixYixevJg9e85vaKQtW7YQEhJCxYoVM8yrc+fOzJs3j9OnT3Ps2LGA
z3mlpKQwZ84ctm7dSkJCAgkJCcydO5eYmBgqVqxIlSpVvM9knT59mpMnT6bbl7T7P3PmTACWLFlC
zZo1KV26dIb7sWfPHsqUKcPQoUN59NFH+e67787rOBhjjDEFTZHvRbBy5crp5qlqQR/40hiTR9q2
bUtkZCRt2rQB4P7776dZs2YAtGrVimbNmnHjjTfyyCOP0K9fP5o1a0bbtm2pX79+lnkfP36c5s2b
e6ffffddRIShQ4cGzKtt27YMGDAAj8dDtWrVaNu2bbo8ly9fTt26dalWrZp3Xrdu3RgyZAj79+9n
5syZjBgxgrFjx1KiRAk+/PBDWrRoQXJyMuHh4dx99900btzYu+6ECRMYPnw4Ho+HMmXK8Pbbb2e6
Txs2bCAqKoqQkBBKlCiRra7sjTHGmIJMnOe5iqbWrVtrXFyc37yDBw9y88038/TTT9O7d+8gRWaM
Adi2bRuNGjUKdhjmIgj0XovIWlVtHaSQ8kygsscYY0zw5Va5U6SbCKa1c+dOOnbsyMqVK7n55pux
AtAYY4wxxhhzPqyC5SM0NJSTJ08CcPLkSe+An8YYY4wxxhiTHVbB8lGrVi0+++wzqlevzuzZs7nn
nnuCHZIxxhhjjDGmACnynVyk1bhxY3766acsx4IxxhhTsIhIKFANn7JPVX8JXkTGGGMKI6tgBRCo
cqWqJCUlUbx48SBEZIwx5kKIyINANLAfSHFnK2CDIBpjjMlVVsHKhnPnzjFixAjOnDnDjBkzCAmx
lpXGGFPAPAQ0VNXEYAdijDGmcLOaQhbOnj1L//79efvtt3nvvfd4/PHHgx2SMeYiEhGGDBninU5K
SqJKlSr07dsXgIULF/LCCy/k+nb/9Kc/8eabb/rNmz9/PjfccAMAHTp0yHT9hIQEmjZtmuUy7733
nnc6Li6O0aNH5zDifG8XcDTYQRhjjCn8rIKVheLFi3P55Zd7pw8fPkxKSkomaxhjCpPSpUuzefNm
Tp06BcDixYupWbOmNz0iIoKoqKgL3k5SUpLfdGRkJLNmzfKbN2vWLCIjIwFYvXr1BW8zbQWrdevW
vPbaaxecbz61A4gVkSdF5JHUV7CDMsYYU/hYBSsLIsIbb7zBgAEDGDt2LNOmTbMmgsYUMX369GHR
okUAxMTEeCs5ANOnT2fUqFEA3HXXXYwePZoOHTpQr1495syZAzjPcD722GM0bdqUZs2aMXv2bABi
Y2Pp1KkTERERNG7c2G+bPXr0ID4+nn379gHw+++/s2TJEvr37w9AmTJlMs3bV0JCAp06daJly5a0
bNnSWzmLiopi5cqVNG/enFdeeYXY2FjvnblDhw7Rv39/PB4P7du3Z+PGjQCMGzeO4cOH07VrV+rV
q1eQKmS/AIuBEkBZn1eOicitIrJFRFJEpNANiGyMMSZn7BmsbAgNDeWDDz4gNDQ02KEYU7Qt6Zp+
3uW3QYMHIOkkxPZJn17vLud1+iCsusU/rWdstjY7aNAgJkyYQN++fdm4cSPDhw9n5cqVAZfdt28f
q1atIj4+noiICG655Rbmzp3L+vXr2bBhAwcPHqRNmzZ07twZgO+++47NmzdTt25dv3xCQ0MZOHAg
77//Pg899BAfffQRXbt2pVy5cn7LZZZ3qqpVq7J48WLCwsL44YcfiIyMJC4ujhdeeIHJkyfz8ccf
A06FL1V0dDQtWrRg/vz5LFu2jGHDhnnHBoyPj2f58uUcP36chg0bcv/99+f7DoBUdTyAiJRxp0/k
QrabgZuBN7Na0BhjTNFht2KyKaPK1bFjxy5yJMaYi83j8ZCQkEBMTAx9+gSoxPno378/ISEhNG7c
mP379wOwatUqIiMjCQ0NpVq1anTp0oVvv/0WgLZt26arXKXybSbo2zzQV2Z5pzp37hz33nsvzZo1
49Zbb2Xr1q1Z7vOqVasYOnQoAN27dycxMdH7e3fjjTdSsmRJKleuTNWqVb37mZ+JSFMRWQdsAbaI
yFoRaXIhearqNlXdnjsRGmOMKSzsDtYFeP3115k0aRIrV67kqquuCnY4xhR+md1xKnZJ5ulhlbN9
xyqQiIgIxowZQ2xsLImJGXdEV7JkSe//qpplvqVLl84wrUOHDuzbt48NGzawevXqdM9kZdcrr7xC
tWrV2LDtgfZ7AAAgAElEQVRhAykpKYSFheUon1S++xgaGpru+bF8airwiKouBxCRrsC/gcx7C8kl
InIfcB/g91yvMcaYwsfuYOXQc889x+jRo/n111+57rrrCsQVXGNMzg0fPpzo6GiaNWt23ut26tSJ
2bNnk5yczIEDB1ixYgVt27bNcj0R4fbbb+fOO+/khhtuCFgxyk7eR48epUaNGoSEhDBjxgySk5MB
KFu2LMePH88w5pkzZwJO08HKlSuna55YwJROrVwBqGoskHHt1iUiS0Rkc4DXTeezcVWdqqqtVbV1
lSpVzj96Y4wxBYZVsHKoW7du3pOdqlWrUqyY3Qw0pjCrVatWjrswHzBgAB6Ph/DwcLp3786LL75I
9erVs7VuZGQkGzZsCNg8MLt5P/DAA7zzzjuEh4cTHx/vvWvm8XgIDQ0lPDycV155xW+dcePGsXbt
WjweD1FRUbzzzjs52PN8ZYeIPCMiddzX0zg9C2ZKVXuqatMArwUXIWZjjDEFkGSnCUuOMxe5Hvg7
EApMU9UX0qTfATwBCHAcuF9VN4hIbeBdoBqgwFRV/XuadR8FJgNVVPWgiPQCXsDpIeos8JiqLsss
vtatW2tcXFyO92/hwoW8/fbbzJw5k0suuSTH+RhjAtu2bRuNGjUKdhjmIgj0XovIWlXNld75RKQi
MB7o6M5aCYxT1cO5kHcsMEZVs1WgXGjZY4wxJm/kVrmTZ7ddRCQU+AfQC9gNfCsiC1XV9+nqn4Eu
qnpYRG7AaSPfDkgCHlXV70SkLLBWRBanrutWwHrjdLub6iDQT1X3ikhT4HOgJnkoIiKCfv36ISJ5
uRljjDEXyK1I5eooyiIyAHgdqAIsEpH1qnpdbm7DGGNMwZOX7draAj+q6g4AEZkF3AR4K1iq6jtS
5hqgljt/H7DP/f+4iGzDqSylrvsK8DiwwCevdT55bQFKiUhJVT2Ty/vlJ1Dlas+ePX4DkRpjjAkO
EXlVVR8WkY9wWkT4UdWInOatqvOAeRcSnzHGmMInLytYNYFdPtO7ce5OZeRu4NO0M0WkDtAC+Nqd
vgnY4zYlzCivgcB3gSpXed2T07Jly+jfvz/R0dE8+uijuZ6/McaY8zLD/Ts5qFEYY4wpMvJFzwwi
0g2ngtUxzfwywIfAw6p6TEQuAZ7CaR6YUV5NgL9ltIyqTsVpikjr1q1z9QG0FStWcP3113Pu3DnG
jBlDjRo1GDx4cG5uwhhjzHlQ1bXuv80DPMv7EPDFxY/KGGNMYZaXvQjuAWr7TNdy5/kREQ8wDbhJ
VRN95hfHqVzNVNW57uwrgbrABhFJcPP8TkSqu+vUwmmuMUxVf8r1PcpC27Ztad++PQA1a9bE4/Fc
7BCMMcYEdmeAeXdd7CCMMcYUfnl5B+tboL6I1MWpWA0C/G7niMjlwFxgqKp+7zNfgP8A21T1/1Ln
q+omoKrPcglAa7cXwQrAIiBKVb/Ms73KRFhYGAsWLOC+++7j5ZdftsEkjTEmyEQkEqfsqSsiC32S
ygKHghOVMcaYwizP7mCpahIwCqc3v23A+6q6RURGishId7FngUrAP0VkvYik9lt7LTAU6O7OXy8i
fbLY5CjgKuBZn3WqZrFOrqtYsSIffPCBVa6MKSREhCFDhnink5KSqFKlCn379s10vbi4uByPmwVQ
r149tm/f7jfv4Ycf5m9/+1u28p4+fTqjRo3KdJnY2FhWr/6jr6EpU6bw7rvv5jjmfGo18DIQ7/5N
fT0KWI9/xhhjcl2ePoOlqp8An6SZN8Xn/3uAewKstwpnbKys8q/j8/9EYOIFhJunDhw4QEpKCtWq
VQt2KMaY81C6dGk2b97MqVOnKFWqFIsXL85WL6GtW7emdevsD6WRlJTkN2D5oEGDmDVrFtHR0QCk
pKQwZ84cvvzyS6644orzyjsjsbGxlClThg4dOgAwcuTILNYoeFR1J7DTHXdxr6qeBhCRUjjNzBOC
GJ4xxphCKC+fwTKuHTt20KFDB/r06cPx48eDHY4x5jz16dOHRYsWARATE0NkZKQ37ZtvvuGaa66h
RYsWdOjQwXvXKTY21nuX69ChQ/Tv3x+Px0P79u3ZuHEjAOPGjWPo0KFce+21DB061G+bkZGRzJ49
2zu9YsUKrrjiCq644ops5e3ro48+ol27drRo0YKePXuyf/9+EhISmDJlCq+88grNmzdn5cqVjBs3
jsmTnc721q9fT/v27fF4PAwYMIDDh53xeLt27coTTzxB27ZtadCgAStXrsyVY3wRvA+k+EwnAx8E
KRZjjDGFWL7oRbAw+/333+ncuTN79jj9e9x66618+umnNjixMefrvTz6zgzOujPRQYMGMWHCBPr2
7cvGjRsZPny4t2Jx9dVXs3LlSooVK8aSJUt46qmn+PDDD/3Wj46OpkWLFsyfP59ly5YxbNgw1q9f
D8DWrVtZtWoVpUqV8lunWbNmhISEsGHDBsLDw5k1a5ZfxS47eafq2LEja9asQUSYNm0aL774Ii+/
/DIjR46kTJkyjBkzBoClS5d61xk2bBivv/46Xbp04dlnn2X8+PG8+uqrgHO37ZtvvuGTTz5h/Pjx
LFmyJMtjmA8UU9WzqROqelZESgQzIGOMMYWT3cHKY6VLl2bcuHEAlCxZkhEjRljl6jwdPnyYu+++
mxo1arBmzZpgh2OKII/HQ0JCAjExMfTp4/846NGjR7n11ltp2rQpf/nLX9iyZUu69VetWuW9Q9W9
e3cSExM5duwYABEREekqV6kiIyOZNWsWSUlJzJ8/n1tvvfW88k61e/durrvuOpo1a8ZLL70UMMa0
+3TkyBG6dOkCwJ133smKFSu86TfffDMArVq1IiEhIdO88pEDIuIdVNgdU/FgEOMxxhhTSNkdrIvg
nnvu4ciRI7Rr145OnToFO5wCZfny5QwbNozdu3czevRo2rXLbKxqU6hl405TXoqIiGDMmDHExsaS
mOgdUYJnnnmGbt26MW/ePBISEujatet55Vu6dOkM0wYNGkTv3r3p0qULHo8nx89wPvjggzzyyCNE
REQQGxvrveiTUyVLlgQgNDSUpKSkC8rrIhoJzBSRN3Ce8d0FDAtuSMYYYwoju4N1kYwZM8YqVzmw
efNmdu/eTe3atencuTMnT54MdkimiBo+fDjR0dE0a9bMb/7Ro0e9nV5Mnz494LqdOnVi5syZgPNs
VuXKlSlXrlyW27zyyiupXLkyUVFRAZsHZjdv3xjfeecd7/yyZcsGfC60fPnyVKxY0dsMcsaMGd67
WQWVqv6kqu2BxkAjVe0A2EOxxhhjcp1VsILo7NmzrFq1Kthh5Gt//vOfue666zh58iQhISF+V/tV
laNHjwYxOlOU1KpVK2DX6I8//jhPPvkkLVq0SHc3J7U58Lhx41i7di0ej4eoqCi/Sk5WIiMjiY+P
9zbLSys7eY8bN45bb72VVq1aUblyZe/8fv36MW/ePG8nF77eeecdHnvsMTweD+vXr+fZZ5/Ndsz5
XDHgdhFZCqwLdjDGGGMKH1ENbrObYGrdurXGxcVlvWAeOHHiBAMHDmTZsmUsWrSI3r17ByWO/CQl
JYUzZ86kex7l119/RVWpUaOG3/xJkybx73//m88//5wGDRpczFDNRbJt2zYaNWoU7DBy5MMPP2Th
woXnVZkqygK91yKyVlUvuD96t0v2m3AGHG6BM8hwf2CFqqZktm5eCGbZY4wxJmO5Ve7YHawgeeCB
B/jf//5HUlISAwcO5Oeffw52SEG1e/duevfuzYgRI9KlVa9ePV3l6q233mLs2LEkJCRw7bXXEh8f
f7FCNSZLCxcuZOzYsQE/z+biEpH3gO+BXsDrQB3gsKrGBqNyZYwxpvCzClaQTJo0iVq1agHO81l1
6tQJbkBBtHfvXjweD0uXLmXGjBl+Y/9kpFq1at47XeHh4dStWzevwzQm2yIiIoiPj/cO4GuCqjFw
GNgGbFPVZCBXmm6IyEsiEi8iG0VknohUyI18jTHGFGxWwQqSWrVq8fnnnzNt2jSio6OLdNftl112
Gf369QOcZ1Z++umnLNe58cYbWbZsGb169WLu3LneXs2MMcaXqjYHbsNpFrhERFYBZUUkZ10y+lsM
NFVVD85dsidzIU9jjDEFnHXTHkSNGzemcePG6eavXbuWyy+/nCpVqgQhquB4/fXX+eWXX5gwYUK2
e1ts3749//vf//I4MmNMQaeq8UA0EC0irYBI4FsR2e32JpjTfH1/gNYAt1xYpMYYYwoDu4OVzyQn
JxMZGUmdOnV47LHHOHz4cLBDylVnz55l2rRppKT4P/pQrlw5li9fnitd2X/88cc89thj6bZhjDGq
ulZVxwBXAFG5mPVw4NOMEkXkPhGJE5G4AwcO5OJmjTHG5DdWwcpnZs+ezQ8//MDJkyeZNm0aISGF
5y3atm0b7du359577+X111/Pk2189dVX3HbbbUyePJlhw4Zx9uzZPNmOMaZgU8eKrJYTkSUisjnA
6yafZcYCScDMTLY3VVVbq2rrotQ6wRhjiqLCc/ZeSFSpUgWPxwPAQw89RPny5YMcUe6ZMWMG69Y5
w8488cQT/Pjjj7m+jWnTpnHq1CkAVq9ezZEjR3J9G6ZoERGGDBninU5KSqJKlSr07dsXcHoMfOGF
F/Js++vXr0dE+Oyzz3KcR0adbdx1113MmTMnx3F98sknOY6poFDVnqraNMBrAYCI3AX0Be7Qojzu
iTHGGC+rYOUzvXr1Yt26dcyfP5+HHnooXfrkyZMZMWIECQkJFz+4CxQdHU3z5s0pWbIkL7zwAvXq
1cv1bUydOpURI0ZQpUoVPv/8c6pWrZrr2zBFS+nSpdm8ebO34r548WJq1qzpTY+IiCAq6sJbmqUd
pDhVTEwMHTt2JCYmJsd5r169OsfrZqQgVbBEJEREbsuDfK8HHgciVPVkbudvjDGmYLIKVj4UEhLC
TTfdRMWKFf3mnzhxghdeeIGpU6dSv359Vq1aFaQIsyftxdySJUsSExNDXFwcDz/8cJ40fwwNDeVf
//oX69ato379+rmevyma+vTpw6JFiwCnwhMZGelNmz59OqNGjQKcO0KjR4+mQ4cO1KtXz3t3SFV5
7LHHaNq0Kc2aNfMORRAbG0unTp2IiIgI2OGNqvLBBx8wffp0Fi9ezOnTp71p7777Lh6Ph/DwcIYO
HQrA/v37GTBgAOHh4YSHh3srVmXKlPHmN2rUKBo2bEjPnj357bffvPmtXbuWLl260KpVK6677jr2
7dsHQNeuXXniiSdo27YtDRo0YOXKlZw9e5Znn32W2bNn07x582wNrRBM7nhXj+dB1m/g9E64WETW
i8iUPNiGMcaYAsZ6ESxA5s6dS2JiIgC1a9emXbt2QY4osOPHj/Pwww9TtWpVnn/+eb+0q6++Os+3
LyJ+dxhSbdu2jVOnTtGyZcs8j8Hkka5d08+77TZ44AE4eRL69EmfftddzuvgQbglTSdvsbHZ2uyg
QYOYMGECffv2ZePGjQwfPpyVK1cGXHbfvn2sWrWK+Ph4IiIiuOWWW5g7dy7r169nw4YNHDx4kDZt
2tC5c2cAvvvuOzZv3hxwLLfVq1dTt25drrzySrp27cqiRYsYOHAgW7ZsYeLEiaxevZrKlStz6NAh
AEaPHk2XLl2YN28eycnJnDhxwi+/efPmsX37drZu3cr+/ftp3Lgxw4cP59y5czz44IMsWLCAKlWq
MHv2bMaOHctbb70FOHfXvvnmGz755BPGjx/PkiVLmDBhAnFxcbzxxhvZOob5wBIRGQPMBn5Pnamq
h3KaoapelRuBGWOMKVysglWADB06lNq1azN+/HiGDBlC8eLF/dL37t3L4cOHadKkSZAihD179tC5
c2d27NiBiHDDDTd4TySDaffu3Vx33XUcPnyY+fPn06NHj2CHZAoQj8dDQkICMTEx9AlUifPRv39/
QkJCaNy4Mfv37wdg1apVREZGEhoaSrVq1ejSpQvffvst5cqVo23bthkOlB0TE8OgQYMAp5L37rvv
MnDgQJYtW8att95K5cqVAbj00ksBWLZsGe+++y7g3M1N+wznihUrvHFcdtlldO/eHYDt27ezefNm
evXqBTi9mdaoUcO73s033wxAq1atCmTzZNft7t8/+8xTIPfbKhtjjCnSrIJVgIgI3bp1o1u3buma
3wFMnDiRKVOmMHDgQCZMmECjRo0ueow1atSgbt267NixA1Vl4cKFQa9gqSqDBw9m165dANx22238
/PPPlCtXLqhxmRzI7I7TJZdknl65crbvWAUSERHBmDFjiI2N9d5JDsR30Ovs9HlQunTpgPOTk5P5
8MMPWbBgAc899xyqSmJiIsePHz//4LOgqjRp0oSvvvoqYHrqPoWGhmb4rFh+p6qBa7HGGGNMLrNn
sAooEfGb3rNnD//5z39QVebMmcPu3buDEldISAjTp0+nbt26zJw5k8mTJwclDl8iwj//+U9q1qxJ
sWLFiImJscqVOW/Dhw8nOjqaZs2anfe6nTp1Yvbs2SQnJ3PgwAFWrFhB27ZtM11n6dKleDwedu3a
RUJCAjt37mTgwIHMmzeP7t2788EHH3greqlNBHv06MG//vUvwKmgHT161C/Pzp07e+PYt28fy5cv
B6Bhw4YcOHDAW8E6d+4cW7ZsyTS+smXL5kllL6+IyCUi8rSITHWn64tI32DHZYwxpvCxClYhcfr0
aa6//noA2rdvT8+ePfN8m6rqPUHzVatWLb7//nsGDx6c5zFkV9OmTVm9ejVz5syhd+/ewQ7HFEC1
atVi9OjROVp3wIAB3g4punfvzosvvkj16tUzXScmJoYBAwb4zRs4cCAxMTE0adKEsWPH0qVLF8LD
w3nkkUcA+Pvf/87y5ctp1qwZrVq1YuvWreniqF+/Po0bN2bYsGFcc801AJQoUYI5c+bwxBNPEB4e
TvPmzbPsebBbt25s3bq1QHRy4XobOAuk9lm/B5gYvHCMMcYUVlKUh+1o3bq1xsXFBTuMXLVu3TqS
kpJo06aN3/y4uDiefvppnn322QzHxDkf+/fv55577uHjjz9mzpw5DBw48ILzDJZjx45RqlSpdM+0
meDbtm1bUJq6mosv0HstImtVtXVu5C8icaraWkTWqWoLd94GVQ3PjfzPR2Ese4wxpjDIrXLH7mAV
Mi1atEhXuQL461//yueff861117Lc889d8HbGT9+PB9//DEA9913H3v37r3gPIPh1KlT9O3bl/79
+/P7779nvYIxpqA6KyKlcDq2QESuBM4ENyRjjDGFkVWwioBdu3Z5x/ABp5ezCzVp0iRq164NOL0b
ph2zqyBITk5m8ODBrFy5kk8++YSePXty7ty5YIdljMkb0cBnQG0RmQksJW/GxjLGGFPEWS+CRUDt
2rXZunUrkyZN4syZM+m6cVdVvvjiC7p06ZKu84yMVKhQgZkzZ3L69Glv184FTUhICE2aNGH+/PkA
3HLLLdZM0JhCSlUXi8h3QHtAgIdU9WCQwzLGGFMIWQWriGjQoAHTp08nJSUlXdrChQvp378/bdq0
YcKECd7OMsAZYHTSpElUrVqVkSNH+q3XqVOnPI87L4kIEydOpHr16vzyyy88+uijwQ7JGJO3ugAd
cZoJFgfmBTccY4wxhVGeNhEUketFZLuI/CgiUQHSrxaRr0TkjIiMSZP2kIhsFpEtIvJwmrQHRSTe
TXvRnddLRNaKyCb3b/e83LeCKiTE/y1XVSZMmADAt99+y9KlS71pv/76K506dSI6Opq//OUvbNu2
7aLGerGMGjWKF198MdhhGGPykIj8ExgJbAI2AyNE5B/BjcoYY0xhlGcVLBEJBf4B3AA0BiJFpHGa
xQ4Bo4HJadZtCtwLtAXCgb4icpWb1g24CQhX1SY+6x4E+qlqM+BOYEZe7Fdhc/r0aa699lpKlixJ
WFiY312cChUqeDt+OH36NK+99lqwwrzoVJWHHnqIuXPnBjsUY0zu6A5cp6pvq+rbQB93njHGGJOr
8vIOVlvgR1XdoapngVk4FSMvVf1NVb8F0vYs0Aj4WlVPqmoS8AVws5t2P/CCqp5JzcP9u05VU7uy
2wKUEpGSebFjhUmpUqV47bXX+Pnnn3nvvff8xuYJCwtj5syZlClThueff5433ngjiJFeXBMmTOC1
117jlltuYcqUKcEOxwSRiDBkyBDvdFJSElWqVKFv38zHqI2Li8vxuFm+Xn31VcLCwtINGpxdmcVR
p04dDh7M2WNI8+fPTzfOVk6pKufOnSOPhw35EbjcZ7q2O88YY4zJVXlZwaoJ7PKZ3u3Oy47NQCcR
qSQil+BcaaztpjVw074WkS9EJH2f5DAQ+C61EuZLRO4TkTgRiTtw4EC2d6awq1GjRrpBTQGaNWvG
zp07iYqKIjQ0NAiRXXwnTpzgvffeA5wTv1WrVuX1iZ/Jx0qXLs3mzZs5deoUAIsXL6Zmzax/ylq3
bn1ed32TkpICzo+JiaFNmzY5vpt6vnFkV25VsHbv3s2mTZvYu3cvW7ZsyYXIMlQW2CYisSKyHNgK
lBORhSKyMC83bIwxpmjJl920q+o24G/A/3C61V0PJLvJxYBLcXqCegx4X3y6vhORJu66IzLIe6qq
tlbV1lWqVMm7nShELr300mCHcFGVKVOGVatW0aZNG3r37s1bb72V7d4VTeHUp08f71AHMTExREZG
etO++eYbrrnmGlq0aEGHDh3Yvn07ALGxsd67XIcOHaJ///54PB7at2/Pxo0bARg3bhxDhw7l2muv
ZejQoem2+9NPP3HixAkmTpxITEyMd35ycjJjxoyhadOmeDweXn/9dcB5jrJDhw6Eh4fTtm1bjh8/
7hdHYmIivXv3pkmTJtxzzz1+Fw7++9//0rZtW5o3b86IESNITnZ+csuUKcPYsWMJDw+nffv27N+/
n9WrV7Nw4UIee+wxmjdvzk8//ZTlMVTVgJ3snDlzhrNnzwLw2WefZZnPBXgWp8l6NDAO58Lds8DL
7ssYY4zJFXlZwdrDH3edAGq587JFVf+jqq1UtTNwGPjeTdoNzFXHN0AKUBlARGrh9Ao1TFWzLvGN
yUCVKlVYtmwZc+bMoUSJEsEOxwCI5M0rGwYNGsSsWbM4ffo0GzdupF27dt60q6++mpUrV7Ju3Tom
TJjAU089lW796OhoWrRowcaNG5k0aRLDhg3zpm3dupUlS5b4VaBSzZo1i0GDBtGpUye2b9/O/v37
AZg6dSoJCQmsX7+ejRs3cscdd3D27Fluv/12/v73v7NhwwaWLFlCqVKl/PIbP348HTt2ZMuWLQwY
MIBffvkFgG3btjF79my+/PJL1q9fT2hoKDNnzgTg999/p3379mzYsIHOnTvz73//mw4dOhAREcFL
L73E+vXrufLKKzM8didOnODnn39m48aN3vh9lStXDnCaYp44cSLDfC6Uqn6R2SvPNmyMMabIyctu
2r8F6otIXZyK1SBgcHZXFpGqqvqbiFyO8/xVezdpPtANWC4iDYASwEERqQAsAqJU9ctc3A9TRJUp
Uybg/AULFtC5c+cCObiyyRmPx0NCQgIxMTH06dPHL+3o0aPceeed/PDDD4hIwMGqV61axYcffghA
9+7dSUxM5NixYwBERESkqwiliomJYd68eYSEhDBw4EA++OADRo0axZIlSxg5ciTFijk/4Zdeeimb
Nm2iRo0atGnjtJpOrbj4WrFihbep4Y033uj9DC9dupS1a9d61z116hRVq1YFoESJEt47YK1atWLx
4sXnceScO1SJiYneY1WjRg2/9AoVKhAWFsauXbu47rrrzivvYBORv+I8W5wC/Abc5fMssDHGmCIq
zypYqpokIqOAz4FQ4C1V3SIiI930KSJSHYgDygEpbnfsjVX1GPChiFTC6QDjz6p6xM36LeAtEdkM
nAXuVFV1t3UV8KyIPOsu2zu1EwxjcsP8+fMZOHAgjRo14rPPPqNWrVrBDqnoCPJzcBEREYwZM4bY
2FhvhQHgmWeeoVu3bsybN4+EhAS6du16XvmWLl064PxNmzbxww8/eAfyPnv2LHXr1mXUqFE53oeM
qCp33nknzz//fLq04sWLe5vIhoaGpntWLDk5mcOHD3P06FFSUlKoX7++X7pvRe/MmTOkpKT4DRdR
vHhxv20UMC+p6jMAIjIap8nhyMxXMcYYU9jl6TNYqvqJqjZQ1StV9Tl33hRVneL+/6uq1lLVcqpa
wf3/mJvWSVUbq2q4qi71yfOsqg5R1aaq2lJVl7nzJ6pqaVVt7vOyypXJNb/++iuDBw8mJSWFLVu2
8OCDDwY7JHMRDR8+nOjoaJo1a+Y3/+jRo95OL6ZPnx5w3U6dOnmb3MXGxlK5cuWAd5h8xcTEMG7c
OBISEkhISGDv3r3s3buXnTt30qtXL958801vZefQoUM0bNiQffv28e233wJw/PjxdJWhzp07eztw
+fTTTzl8+DAAPXr0YM6cOfz222/e/Hbu3JlpfGXLluX48eOoKgkJCd5KVtptFi9enMsvv5xGjRrh
8XjSjcVXkKWWV67SOAMYG2OMKeIKT0lnTB6rXr0606ZNo3jx4lx11VW8+eabwQ7JXES1atUK2N35
448/zpNPPkmLFi3SVS5S78qMGzeOtWvX4vF4iIqK4p133slye7NmzUrXs+eAAQOYNWsW99xzD5df
fjkej4fw8HDee+89SpQowezZs3nwwQcJDw+nV69enD592m/96OhoVqxYQZMmTZg7dy6XX+70Wt64
cWMmTpxI79698Xg89OrVi3379nnXO3PmDL/99hv79u3zdlQxaNAgXnrpJdq0aeN3Ry+16aOvqlWr
Urp06aDcpXIHn9+Y0SsX8n9ORHYBd+DcwcpoOevB1hhjiggpyt1Pt27dWuPi4oIdhilgli5dSt26
dalXr16wQyn0tm3bRqNGjYIdRo58+OGHLFy4MFuVqfwuPj7e2wFF3bp1qVSpkl96YmIi586do3z5
8jYqEcUAACAASURBVISFheWoIhXovRaRtaraOueRg4hc4f77Z/dv6iD0dwCoalQW6y8BqgdIGquq
C3yWexIIU9XorGKysscYY/Kn3Ch3IG87uTCmUOrRo0fA+Zs2baJRo0bejgdM0bVw4ULGjh3LW2+9
FexQsiUlJYVTp05x/PhxSpUqRfny5f3Sy5cv761gHT16NF0FK+10fqKqOwFEpJeqtvBJihKR74BM
K1iq2jObm5oJfILTDbwxxpgizM4EjckFmzZt4tprr6Vjx47MmjUry+drTOEWERFBREREruaZOo5U
SkoKxYsX90tLTk7m0KFDpKSkICLeHgBTnTlzhh07dnjXbdCggV/6yZMniY+PB6BixYoZVrDKly+f
Lq0AERG5NrWXWRHpwAU2kxeR+qr6gzt5ExB/gTEaY4wpBKyCZcwFOnToEP369eP48eN8+umnDB48
mI8//jjYYRUaqlogephTVW8lKDQ0NF3Mx44d81aQKlSo4NfZg6qyc+dOkpOTSUlJ4aqrrvJbPzk5
mXXr1gEQEhJCy5Yt/fJOSUnxdkpRrFixdBUscMazSs0rLd9Yjh07lu6YX3LJJel6B8xNF6mp+nDg
bRFJrSEeceddiBdEpCFON+07sR4EjTHGYBUsYy5YhQoVGDJkCM899xxlypQJ2NW1yZmwsDASExOp
VKnSBVWyVJVz586RlJRESkoKYWFh6ZpyHjp0iDNnzqCqVKpUiZIlS/ql79y5k9OnT5OSkkLdunUJ
CwvzS9+8eTNnzpwBoGnTpunSd+zY4e0EIzw83K9SIyIkJiZ6KxqplbRUvsumpKSkqwClTU8rq/Ri
xYoRFhbmbR54MSu1qkpiYmK645WbRCQEuOr/2bvzuKrr7PHjr8MmoIALiyngFop7KGlNuVRWNqOV
7WVNezlN69SULZPWNL/qWzbTNm1mVtNmNqNtM5ppmpW5m4K45oKKiAugyHLh/P74XBCQ5apcLst5
Ph73wf1s73u4Ih/Ofb/f562q/UsTLFXNPtF2VfXSEw7OGGNMk2MJljEnyM/Pj6eeeooePXrQtm3b
o8p4m+MXGxtLeno6J1p1rbi4mPT09LLtqKgoQkNDK5yze/fusqp7MTExR/3Bv2vXLgoLCwFnyF3l
BCwzM7NskeF169YRFBRU4fiePXvKeo/S0tKOSvD27t1blvysXbu2QoJVehycZCw1NfWocuf5+fmI
CH5+fqxdu7bCMVUlICCg2uPgrHFVWFjInj17Tvj9PlbBwcFeXVNOVUtE5EFgWl0kVsYYY0xNLMEy
po5cd911Ve7fvXs3UVFRTWr9n/oSGBhIly5d6qSt5557jvfeew9w1pi66qqrKhy///77+e9//wvA
F198wahRoyocHzduHAsWLABg7ty5nHXWWRWOX3vttaxevZqQkBC+++67oyriPffcc2RlZRESEsI/
/vEPTjrppArHS4cAhoWF0a1bN0JCQioc7969+1FJlzkmc0TkAeAT4FDpTlXd57uQjDHGNEWWYBnj
RVlZWZxxxhkkJSXx7rvvHtVrYupWQUEB7777LjExMVx00UUVjv35z39m2bJltGzZsspCDZdeeil9
+/YlJCSEk08++ajjzz33HAcPHiQkJITevXsfdXzx4sU1JkC1VRS85pprajxuydUJu9L99Y/l9ilg
6y0Yc5wOFbjYceAwUa1a0Do0sFHMlzWmPliCZYyXFBYWcskll7Bp0yY2bdpEVlYWc+fOtRuQl/z8
88+MGTOGXbt20bNnT0aPHl2h17BPnz6sWbOm2utvvvnmGtsfNGhQjcctAWrYVLVuukKNaeZUlVXp
2XyyZBufr9zJoUJn6HOLAD9iwoNpHx5MTEQw7cNbONsR7n3uR1CAjeYwTZ8lWMZ4iZ+fH0lJSXz/
/feICPfcc48lV17Uo0ePsrWa1q5dy8yZMxkzZoyPozINiYj0AXoBZRPsVPU930VkTONxIK+QGSt2
8PGS7aRl5BIc6Meofh048+RIsg4WkJlbQEZ2Phk5+fySfoDZ2fkUuI4uqtOuZVBZ4lWWkIW3cCdl
zsN6w0xjZwmWMV4SEBDAiy++SI8ePcjLy+Piiy/2dUhNxo4dOwgPDycsLKxsX+vWrbnjjjt47733
eOCBBzj33HN9GKFpaERkAjAcJ8H6GrgAWAhYgmVMNVSVRZv38fGSbfx3TQaFrhL6dozgqYv7cOEp
HQgPDqzx2uzDRWTk5JORnc/unHwysgvIyHGe73YnYlkHC4+61nrDTGMn9bT+SIOUnJysS5cu9XUY
ppk6fPgwwcHB9indMdi0aRPPPvssU6dO5a9//SsPPfRQheO5ubkEBQUdVeHPNE4iskxVk+uordVA
f2CFu1x7DPAvVa33TNzuPaahy8zN57NlO/hkyTa27M0jLDiAi0/pyJWnxtGnY90uNl7oKiEz9+gE
rLQ3rPR5db1h0eFOAlbaIxbXJpT+ca3pGtkSPz+7v5pjU1f3HevBMsYHioqKGDVqFJ07d+a11147
qqS3qdr8+fN56623AHjhhRe4++67K1TbK9+jZUwlh93l2l0iEg5kAnG+DsqYhqK4RFmwfg8fLd7G
t2mZFJcogzq35e5zErigz0mEBHlnnmlQgB+xbUKJbVN9EShVJeewy+kNy8lntzv5Kv989Y7sCr1h
YcEBnBLXmqS41iTFt6F/XGvatrR7rakflmAZU89UlTvvvJO5c+cCzgK033zzzVHrIpmjXXvttUyY
MIH09HS6detGRkZGnZVxN03eUhFpDbwFLAMOAj/5NiRjfC99fx7Tlqbz6dLt7MrOp13LIG45swtX
nBpHt6hWvg4PcNb/iwgNJCI0kB7tq/8grdBVwta9h1ix/QArtx9gxbYDvDJvIyXuwVqd2oWSFNfa
Sbzi29DzpHAbZmi8wv6iM6aelZSUlC1YC3D22WdbclWOqjJr1iyee+453nnnHeLj48uOBQUF8eqr
rxIREcHQoUNteKXxmKre4X76uoj8DwhX1V98GZMxvlLoKmHO2t18tHgbCzdmATAkIYrHR/XinJ4x
jTbpCArwIyEmjISYMK5IdjqoDxW4WL0j251w7eenzXuZsXJn2fm9O4STFNeGU+Kd3q7YNiF2b2mi
SkqUAlcJ+UXF5LuKKSgqId9VTH6Rs6+qYajHy/6qM6ae+fv7M2XKFBITE1mzZg2PPfaYr0NqUMaN
G8ebb74JwKRJk3jxxRcrHL/wwgt9EZZp5ETkfWAB8L2qpvk6HmN8YWPmQaYt3c5ny9LZe6iQDhHB
3H12Apcnx9Y4RK8xa9kigNO6tuO0ru3K9u3KPsyKbQfKkq4PF29lyg+/AhDZKohT4tqQFO/0dPWL
jSCshmIe5vgUl2hZUpNfVOx+lFBQmvC4iimovK+Kc/KLnESpwjlVJU9FJRQW110CVRsrcmETjY0P
qepRn5SVlJRUWL+pufnf//7HBRdcAEBERAQ7d+60BZqbqToucnEWMMT96AasABao6os1XugFdu8x
9elwYTFfr97Fx0u2sWTLfgL8hBE9Y7hyUBxDE6Lwt0IQFBWXsC4j1xlauO0AK7bvZ/OeQwCIQEJ0
qyO9XPGtSYgOa1Lvm6u4hHxXiZPQVJHwFFROXCqcdyS5KXCfc3RSVLrvyDlFxceffwT6Cy0C/AkO
9Dvqa3CgP8GB/rQIKH3uHGsR6EdwwNHHKm8nd25XJ/cdS7DsJmcamLvvvhsRYdKkSU166GBeXh6z
Zs06aq0qVWXIkCEMHDiQ+++/v8IQQdO81GWC5W7PHzgVOAsYh1P4IrGu2veU3XtMfVizI5uPl2xj
5oqd5Ba46BLZkitPjeOSAR2JDguuvYFmLjuviJXpRxKuldsPcCCvCICWQf70jY0gKb5NWSGN6PD6
fU9VlbzCYnLzXeTkF5FzuMj91UVufhE5+a4K+3Lc+w7mFx2VALlKjj8XCPL3c5KX0oSlUjJTIcEJ
9Hfvr5wQHUl0WlRzTmkS1CLAjwB/730IbVUEjWmCXn31VV5++WUANmzYwPTp05tc742q8uyzzzJp
0iSysrJYvnw5SUlJZcdFhAULFjTrXjxT90TkW6AlTmGL74FTVTXTt1EZU7dy8ouYuXInnyzZxpod
ObQI8OO3fU/iylPjGNylrc0tOgYRoYEM6x7FsO5RgHPv2rI3j5Xb97uTrgO8tWBzWXLSsXVI2Tyu
U+Ja06djBMGB1VdeLClRDhY6SVBuWTJ0JCnKrSJBqphMuSiuJTEKDvQjLDiQ8OAAwkMCaR0SSGyb
kLIEpnwCFFwuAWpRRY9QhfPK9QRZKfyqWYJlTAOhqixcuLBsu1WrVgQHN71PGUWEFStWkJXlTKx+
+umnmTZtWoVzLLkyXvALMBDoA2QDB0TkJ1U9fKINi8j9wPNAlKpmnWh7xhwLVWXZ1v18tHg7X63e
SX5RCYntw3jiwt5cfEpHIkJt/lBdEBG6RLakS2RLxiTFApBfVEzKzuxy87kO8NUvuwAI8BN6nhRO
t6iWHCosLkuKct0JUm6Bi9oGkbUM8ncSpJAAwoMDiQprQbeolhX2hYcEur8GVEimwoIDaBHgndL6
pnaWYBnTQIgIH3zwAd26dWP27NlMnTq1SSQaRUVFBAZWvME//PDDTJs2jU6dOnH22Wf7KDLTnKjq
fQAiEgbcALwDtAdOaFVqEYkDzgO2nWCIxhyTvQcL+PfyHXy8ZBub9hyiZZA/Y5JiuerUOPrFRlhv
VT0IDvRnYKe2DOzUtmxfZm4+K8slXEu37qdVCyfp6dg6hPCQMCchcidCFZOjI4lTq+AAAr04FM54
l83BsnHwpgEqKCigRYsT+rvP51JSUnjmmWdIS0tj8eLFR93sZ8+ezVlnnXVU8mVMqToucnEnToGL
gcAWnGGC36vq3BNsdzrwV2AmkOxJD5bde8zxKilRftiUxceLtzM7NYOiYmVAfGuuOjWe3/U7iZYt
7HNzY06EzcEypgmrKrl6++232bJlC0888USD69mqqvLhHXfcwYIFC4CKlQFLnXfeefUWnzFAMPAC
sExVXXXRoIhcBOxQ1VW19RaIyG3AbYAVbqmGqpK+/zB+fkLH1iG+DqfBKCou4adNe5mVksHs1N3s
yS2gdWgg153WmasGxdE9pvqFd40xvmEJljGNwLx58xg3bhwul4u0tDTeffddnxe/mDZtGu+++y7L
ly9n4sSJ3H777RWODxgwoCzBmjNnzlEJljH1SVWfF5EzgeuAd0QkCmilqr/WdJ2IzMEZSljZo8Aj
OMMDPXn9N4E3wenBOpbYm6rMnHxWpWfzS/oBVqVnszr9APvdVdoSoltxds9ozu4RzcBObbxaNawh
OlxYzPz1e5iVksG3a3eTk+8iJNCfsxKjuKDPSZzXO8bm1xjTgFmCZUwjMHnyZFwu50P3jRs3UlLi
/cXySpO5FStWEB0dzfnnn1/h+JYtW/j6668BWLFixVHXDxo0iIsuuoiHH36YwYMHez1eY2oiIhOA
ZKAHzvyrQOBfwBk1XaeqI6ppry/QBSjtvYoFlovIIFXNqMPQm4TsvCJ+2XGAX9KzWbXd+ZqRkw+A
v5+QEN2K83q1p19cBIcLi5m3LpMpC3/ljfmbCQ8OYFiPaM5OjGJY92jatgzy8XfjHdl5RXybtptZ
KRnMX7+H/KISWocGcl7v9pzfuz1DEiJrrEpnjGk4LMEyphF49913iYyMZNq0aXzxxRe0atWqTtuv
aojfhx9+yPXXXw/A6NGjj0qwypdWT01NParNq6++mquvvrpO4zTmBIwBkoDlAKq6013w4rio6mog
unRbRLbg4Ryspi6v0EXKzpyyROqX9ANs2ZtXdrxLZEsGd21Lv9jW9I+NoHeHCEKCKiYOtwzpSm5+
EQs3ZDE3LZN56zL5YtVO/ASS4ttwdmI0ZydGk9g+rFEXc8jMyWdW6m5mp2Tw06a9uEqU9uHBXJEc
x8je7RnUpW2z670zpinwapELERkJvAj4A5NV9ZlKxxNxPkkcADyqqs+XO9YamIxTUleBm1T1JxE5
BXgdZzy9C7hDVReLSDtgOs4iklNV9c7a4rOJxqaxyczMJDo6uvYTPbB9+3Yee+wxli9fTnh4OD/8
8EOF46tXr6Zfv34AxMbGsn379grHs7Oz+frrr0lKSiIhIQF/f/tk1dStOi5ysVhVB4nIclUdICIt
gZ9UtV8dtb+FZljkotBVwrqMXFalH+CXdCehWr87l9LleU6KCKZfbIQ7mWpN344Rx1U2vKREWb0j
m2/TMpmXlsnqHdkAdIgI5ix3svWbbpFHJWoN0ZasQ8xKyWBWSgYrth9A1Uk6z+/dnpF92tOvY4St
LWSMj9TVfcdrCZaI+APrgXOBdGAJcLWqppY7JxroBFwM7K+UYL2LU+FpsogEAaGqekBEZgN/V9X/
ishvgQdVdbj7ZpmEk5D1sQTLNBcLFy7kp59+4oEHHjjqk9yMjAxWrFjB+vXrueeeeyocy8rKIirK
WUAxODiY3NxcAgKOdGq7XC4SExPp1asXAwYM4C9/+YslUaZe1XGC9QCQgHNPehq4CfhIVV+qi/aP
RWO99xSXKJv3HKwwb2rtzhwKi50hy21CA8t6pfrFtqZfXATRYd5Zyy8zJ5956zKZm5bJ9xuyyCss
pkWAH7/p1o6zE6M5KzGa2DYNY5F2VWXtrtyypCotIxeA3h3CGdm7Pef3aU9CdKtG3RNnTFPRGKoI
DgI2qupmABH5GLgIKEuwVDUTyBSR35W/UEQigKE4a5WgqoVAYellQLj7eQSw033OIWChiJzspe/H
mAZn8+bNjBkzhqysLNauXcvrr79OUJAzP8HlctGlSxfy8515DmPHjiUyMrLs2sjISOLi4ti+fTv5
+fls2rSJHj16lB0PCAhg48aN9fsNGeMl7iIX5wI5OPOwHlfVb3wcVoNVWtFvVfqReVNrdmRzqLAY
cBZA7dMxghvO6Ey/2Aj6x7Ymtk1IvSUJ0eHBXHlqPFeeGk+Bq5jFv+5jbpqTcM2bmQIzU+gRE+YU
ykiMJimudb0OtSspUZZv2+9OqnazbV8eInBqp7b8ZVQvzusVQ1zbhpEAGmPqnjcTrI5A+TFF6YCn
M927AHtwKj31B5YB97iTqHuBWSLyPOAH/OZYgrJSuaYpefrpp8nKckYkffnll+zevZu4uDjASZD6
9u3LkiVLAKcQxbnnnlvh+n/+85+0bt2a/v37ExZmpX5N0+ZOqL4BEBE/ERmrqh/4OKwGITM3n1+2
H+mZ+qVcRb8gfz96dgjn0oGxZT1UXaNa4d9AhrG1CPBnSEIUQxKieHxULzZnHWLuWifZemvBZl77
bhMRIYEM7xHF2YnRDOseRevQui+UUegqYdHmvfwvJYNv3OXUA/2FM06O5A/DuzGiZwxRYY17fUNj
jGcaapGLAJx5WXep6s8i8iIwHvgL8AfgPlX9TESuAN4GqqzyVBUrlWuakldeeYWioiI+/vhjZsyY
UZZclTrzzDMJDAwkKSmJmJiYo64fNWpUfYVqjE+ISDjwR5wP/T7HSbD+CDwArAKabYK1LiOXKQt/
ZcGGPezKdnq6/QS6x4Rxbq+YsnlTPdqHERTQOAotiAjdolrRLaoVtw7tSk5+Ed+vdwplfLcuk5kr
nUIZAzu14azEaM5JjKF7zPEPz8srdLFg/R5mpexmztrd5Oa7CA3yZ3iPKM7v3Z6zEqMJD7bF1I1p
brw5B+t0YKKqnu/efhhAVZ+u4tyJwMHSOVgi0h5YpKqd3dtDgPGq+jsRyQZaq6qK8xsxW1XDy7V1
A85EY5uDZZoFVSU1NZXevXv7OhRj6lRdjIUXkZnAfuAn4Bycyn+CMypi5YlHeex8ee9RVRZsyGLy
95v5fkMWwYF+jOgZwylxrekf15reHcIJDWqon72emJISZVX6gbKhhCk7cwDo2DqkrCrh6d3a1VoK
PTuviDlrnXLqCzYcKac+omcMI3u350wrp25Mo9UY5mAtARJEpAuwA7gKuMaTC1U1Q0S2i0gPVV2H
c1Msnbu1ExgGfAecDWyo68CNaUxExJIrY6rXVVX7AojIZGAXEK+q+b4Nq37lFxUzc+UO3l74K+t3
HyQ6rAV/Pr8H1wyKp00TXVeqMj8/ISm+DUnxbbj/vB5kZB8plDF9WTrvL9pKcKAfZ3SLLKtM2KF1
CAC7c/KZnbqbWWsyWLT5SDn1K5PjON/KqRtjKvFagqWqLhG5E5iFU6Z9iqqmiMg49/HX3T1VS3GK
VpSIyL1AL1XNAe4CPnBXENwM3Ohu+lbgRREJAPJxz6eCsjK54UCQiFwMnFe+aqExxphmp6j0iaoW
i0h6c0qu9h4s4F+LtvH+oi1kHSyk50nhTLq8P6P7d2g0w/68pX1EMFcPiufqQfHkFxXz86/7mJeW
ybdpu/k2LROAxPZhhAT5s2LbAQC6Rrbk1qFdOb+3lVM3xlTPq+tgNXQ2RNAYYxquOhoiWAwcKt0E
QoA893MtP8S8vtTHvWdjZi5vL/yVz5bvoNBVwlk9orh1SFdO79bOyoHXQlXZtOcg37oLZeS7ShiR
GM3IPu052cqpG9Ok1dsQQRFpp6p7T/SFjDHGmPqmqs1mMoyq8uOmvbz1/Wa+W7eHFgF+XDoglpvP
7MzJ0VYl1FMiwsnRYZwcHcbtw7r5OhxjTCPkyRDBRSKyEngH+K825y4vY4wxpoEpcBXzxapdTP5+
M2kZuUS2CuJP53Zn7OB42rWysuDGGFPfPEmwuuOUQb8JeElEpgFTVXW9VyMzxhhjTLX2Hyrkw8Xb
ePfHLWTmFtA9phX/d2k/Ljylg1WxM8YYH6o1wXL3WH0DfCMiZwH/Au4QkVU4pdN/8nKMxhhjjHHb
vOcgU374lenL0skvKmFo9yiev7wLQxIibX6QMcY0AB7NwQKuBa4DduNU9/scOAX4FOjizQCNMcaY
5k5V+fnXfUz+fjPfpmUS6OfHxUkduPnMrvRob/OrjDGmIfFkiOBPwPvAxaqaXm7/UhF53TthGWOM
MaaouISvftnF5IWbWbMjh7Ytg7jr7ASuO60TUWE2v8oYYxoiTxKsHtUVtlDVZ+s4HmOMMabZy84r
KptflZGTT7eoljx9SV/GJHW0+VXGGNPAeZJgzRaRy1X1AICItAE+VtXzvRuaMcYY07xs3XuId37Y
wrSl28krLOaMk9vx9CV9GdY9yha1NcaYRsKTBCuqNLkCUNX9IhLtxZiMMcaYBk9EJgK3Anvcux5R
1a+PtR1VZenW/Uz+fjOzU3cT4CeM7t+BW87sSq8O9b4OsjHGmBPkSYJVLCLxqroNQEQ6AbYWljHG
GAN/V9Xnj+dCV3EJX6/J4O3vN7MqPZuIkEDuGN6N35/emZjw4LqO0xhjTD3xJMF6FFgoIvMBAYYA
t3k1KmOMMaaJKlblrQWbmfrjFnYcOEyXyJb89eI+XDqgI6FBntyWjTHGNGSerIP1PxEZAJzm3nWv
qmZ5NyxjjDGmUbhLRH4PLAXuV9X9VZ0kIrfh/nCyRfuT+dvXaxncpS0TL+zNOYnRNr/KGGOaEE8/
KmsB7HOf30tEUNUF3gvLGGOM8T0RmQO0r+LQo8BrwF9xhs3/FZgE3FRVO6r6JvAmQEzXXvrFnWfS
NzbCKzEbY4zxLU8WGn4WuBJIAUrcuxWwBMsYY0yTpqojPDlPRN4CvvTk3Li2oZZcGWNME+ZJD9bF
OGthFXg7GGOMMaaxEJGTVHWXe3MMsMaX8RhjjGkYPEmwNgOBgCVYxhhjzBH/JyKn4Izq2ALc7ttw
jDHGNASeJFh5wEoR+ZZySZaq3u21qIwxxpgGTlWv83UMxhhjGh5PEqzP3Q9jjDHGGGOMMTXwpEz7
uyISAsSr6rp6iMkYY4xpspYtW3ZQROx+Wr1IwJaDqZ69PzWz96dm9v7UrEddNOJJFcHRwPNAENDF
Pd78SVW9sC4CMMYYY5qZdaqa7OsgGioRWWrvT/Xs/amZvT81s/enZiKytC7a8fPgnInAIOAAgKqu
BLrWxYsbY4wxxhhjTFPiSYJVpKrZlfaVVHmmMcYYY4wxxjRjnhS5SBGRawB/EUkA7gZ+9G5Yxhhj
TJP1pq8DaODs/amZvT81s/enZvb+1KxO3h9R1ZpPEAkFHgXOAwSYBfxVVfPrIgBfSk5O1qVL62So
pTHGmDomIstsroAxxpjGxpMqgnk4Cdaj3g/HGGOMMcYYYxovT6oIzsNZpb4CVT3bKxEZY4wxxhhj
TCPlSZGLB4A/ux9/AVYCHo2rE5GRIrJORDaKyPgqjieKyE8iUiAiD1Rx3F9EVojIl+X2tRWRb0Rk
g/trG/f+diIyT0QOisgrnsRnjDHGeJOITBGRTBFZU25flfex5qia9+c5EUkTkV9E5D8i0tqXMfpS
Ve9PuWP3i4iKSKQvYmsIqnt/ROQu989Qioj8n6/i87Vq/n+dIiKLRGSliCwVkUG+jNGXRCTOnTuk
un9W7nHvP+Hf0bUmWKq6rNzjB1X9EzDcg6D9gVeBC4BewNUi0qvSaftwimY8X00z9wBrK+0bD3yr
qgnAt+5tgHycBPCoRM0YY4zxkanAyEr7qruPNUdTOfr9+Qboo6r9gPXAw/UdVAMylaPfH0QkDmdu
/Lb6DqiBmUql90dEzgIuAvqram+q/xuzOZjK0T8//wc8oaqnAI+7t5srF3C/qvYCTgP+6M5VTvh3
dK0JljuLK31Eisj5QIQHbQ8CNqrqZlUtBD7G+YEvo6qZqroEKKridWOB3wGTKx26CHjX/fxd4GJ3
W4dUdSFOomWMMcb4nKouwPkwsbwq72PNUVXvj6rOVlWXe3MREFvvgTUQ1fz8APwdeJAqpnA0J9W8
P38AnlHVAvc5mfUeWANRzfujQLj7eQSws16DakBUdZeqLnc/z8Xp1OlIHfyO9qRM+zKcfwzByfR+
BW724LqOwPZy2+nA4GOI7R84vzzCKu2PUdVd7ucZQMwxtImI3AbcBhAfH38slxpjjDF14YTujiNp
bAAAIABJREFUY83MTcAnvg6iIRGRi4AdqrpKRHwdTkPUHRgiIn/D+dD9AfeH+cZxLzBLRJ7H6Wj5
jY/jaRBEpDOQBPxMHfyO9qSKYJdjbfREicgoIFNVl4nI8OrOU1UVkWP69EZV38Rd4z45OblZf/Jj
jDHGt47nPtZciMijOB/sfuDrWBoK99I5j+AMDzRVCwDa4gz5OhWYJiJdtbZ1iZqPPwD3qepnInIF
8DYwwscx+ZSItAI+A+5V1ZzyH1wc7+9oT6oIXlLTcVX9dzWHdgBx5bZj3fs8cQZwoYj8FggGwkXk
X6p6LbBbRE5S1V0ichLQbLt+jTHGNEp2H6uFiNwAjALOsT+MK+gGdAFKe69igeUiMkhVM3waWcOR
Dvzb/XOzWERKgEhgj2/DajCux6lxAPApR0/FaVZEJBAnufqgXE5zwr+jPakieDNOdjvW/ZiM02U/
GueXX3WWAAki0kVEgoCrgM89CUpVH1bVWFXt7L5urju5wt3G9e7n1wMzPWnTGGOMaSDsPlYDERmJ
M0XgQvdanMZNVVerarSqdnb/jZQODLDkqoIZwFkAItIdCAKyfBpRw7ITGOZ+fjawwYex+JQ4n1K8
DaxV1RfKHTrh39GezMEKBHqVjkV0Z3JTVfXGmi5SVZeI3AnMAvyBKaqaIiLj3MdfF5H2OCXfw4ES
EbnX/Vo5NTT9DE53783AVuCK0gMissXdVpCIXAycp6qpHnyPxhhjTJ0TkY9wKu9Gikg6MIEa7mPN
TTXvz8NAC+Abdy/NIlUd57Mgfaiq90dV3/ZtVA1HNT8/U4Ap7tLkhcD1zbUXtJr351bgRREJwJmj
dpvvIvS5M4DrgNUistK97xHq4He01PYzJyJrVbVnuW0/IKX8vsYqOTlZly71aEkvY4wx9UxElqlq
sq/jMMYYY46FJz1Y34rILOAj9/aVwBzvhWSMMcYYY4wxjZMnVQTvFJExwFD3rjdV9T/eDcsYY4wx
xhhjGh9PerAAlgO5qjpHREJFJMy9IJcxxhhjjDHGGLdaqwiKyK3AdOAN966OOBVajDHGGGOMMcaU
40mZ9j/iVNnIAVDVDUC0N4MyxhhjjDHGmMbIkwSrQFULSzfcZR2bZblLY4wxxpiGRETaichK9yND
RHaU2w6qdO4sEQmrpb10EWldzf5Pym1fJSJ1skitiDzlXqrHmCbBkzlY80XkESBERM4F7gC+8G5Y
xhhjjDGmNqq6FzgFQEQmAgdV9fny57gXVBVVPf8EX26wiPRQ1XUn2E6dKfe9lfg6FmNKedKDNR7Y
A6wGbge+Bh7zZlDGGGOMMeb4icjJIpIqIh8AKcBJ5XunROQLEVkmIikicouHzU7CWYi18mtV6IES
kTQRiXXHsEZE3heR9SLynoicLyI/isgGESm/zl2SiCxy77+pXFvjRWSxiPwiIo9X970d8xtkjBfV
2IMlIv7Ae6o6FnirfkIyxhhjjDF1IBH4vaouBXA6e8pcr6r7RCQUWCoin6nq/lra+wi4U0S6HEMM
PYArgDScqtT5qvobEbkU50P8y9zn9QV+A4QDy0XkK2AgEA8MBgT4WkR+A2RW/t6MaUhq7MFS1WKg
U+UxvMYYY4wxpsHbVEMCcp+IrAJ+AmKBbh6058LpxRp/DDFsVNVU9xC+VOBb9/7VQOdy581Q1XxV
zQQWAKcC5wEXACtwkrOTge7u82v63ozxKU/mYG0GfhCRz4FDpTtV9QWvRWWMMcYYY07Uoap2isgI
YChwmqoeFpGFQLCHbU4FHgTWl9vnouKH9uXbKij3vKTcdgkV/w6tXEBNcXqtnlLVtyvFfzLVfG/G
NASezMHaBHzpPjes3MMYY4wxxjQ+EcA+d3LVG6e3yCPuytIvAfeU270FZzgfIjIIiDuOmC4WkRYi
EgUMAZYCs4CbRaSlu+1YEYk8jraNqVfV9mCJSICqulT1ifoMyBhjjDHGeNVXwG0ikgqsA34+xuvf
omKxi0+Ba0VkDbAIZ/TTsVoDzAfaARNUdTfOnKtEYJF7/lgucM1xtG1MvRLVqpe0EpHlqjrA/fxl
Vb2rXiOrB8nJybp0qQ3fNcaYhkhElqlqcu1nGmOMMQ1HTUMEy5eaOcPbgRhjjDHGGGNMY1dTglV1
15YxxhhjjDHGmCrVVEUwUUR+wenJ6uZ+jntbVbWf16MzxhhjjDHGmEakpgSrZ71FYYwxxhhjjDFN
QLUJlqpurc9AjDHGGGOMMaax82QdLGOMMcYYY4wxHrAEyxhjjDHGGGPqSE1zsMqISAgQr6rrvByP
McYYYxqxZcuWRQcEBEwG+mAf5BrTlJQAa1wu1y0DBw7M9HUwDVmtCZaIjAaeB4KALiJyCvCkql7o
7eCMMcYY07gEBARMbt++fc+oqKj9fn5+tuSLMU1ESUmJ7Nmzp1dGRsZkwPKAGnjyydJEYBBwAEBV
VwJdvBiTMcYYYxqvPlFRUTmWXBnTtPj5+WlUVFQ2Tu+0qYEnCVaRqmZX2me/NI0xxhhTFT9Lroxp
mtz/t23oby08mYOVIiLXAP4ikgDcDfzo3bCMMcYYY4wxpvHxJAO9C+gNFAAfAtnAvZ40LiIjRWSd
iGwUkfFVHE8UkZ9EpEBEHqh0bIqIZIrImkr7nxORNBH5RUT+IyKt3fvbicg8ETkoIq94Ep8xxhhj
mh5/f/+BiYmJvUofjzzySPvjaefSSy/t/M4777Spi5jef//91suWLQsu3b733ns7zJgxI6wu2h49
enSX7t2793riiSeij+W6rKws/2eeeSaqLmJobEJDQ5Pq8/WuvPLKTuX//U/EU089Fd21a9feF154
4TFP2XnyySejc3NzrQfKyzzpwUpU1UeBR4+lYRHxB14FzgXSgSUi8rmqppY7bR9Oj9jFVTQxFXgF
eK/S/m+Ah1XVJSLPAg8DDwH5wF9wxoXa2FBjjDGmmWrRokVJWlpaau1n1i2Xy0VAQNV/Ws2YMaO1
y+XKHjhwYD7AP/7xj5118Zrbtm0LWLVqVctt27atqf3sivbu3ev/9ttvR48fP36Pp9cUFRURGBh4
rC/V5NX2vnzyySdb6+q13n777ag5c+as79atW9GxXvvGG2/E3HrrrfvCwsJKPL2mpp9rUzVP3q1J
ItIemA58oqqe/gceBGxU1c0AIvIxcBFQ9gtPVTOBTBH5XeWLVXWBiHSuYv/scpuLgMvc+w8BC0Xk
ZA/jM8YYY4wX3XQTcWvWEFqXbfbpQ96UKWw/1uv27t3rP3DgwJ4zZ87c0L9//4LRo0d3GT58eO79
99+fFRoamnT11VdnzZ8/PzwqKqros88+29yhQwdX+etnzpwZNn78+Lji4mL69++f9957720NCQnR
jh079r3wwgv3zZ8/P/zee+/NyM3N9X/nnXeiioqKpHPnzgXTp0//ddGiRSFz5sxpvWjRorBnn332
pM8++2zT448/ftKoUaOyb7zxxv01tX3FFVfsnTVrVoTL5ZJPPvlkc1JSUn75uEaMGNE9MzMzKDEx
sdc//vGPbSkpKcGVXz8sLKxk+/btATfddFOnbdu2tQB45ZVXtr744osx27dvb5GYmNhr2LBhOa+9
9lr6H/7wh9i5c+dGiIj++c9/3nXrrbfu//LLL8MmTJjQISIionjz5s3BW7ZsOeZkrjo3zbwpbk3m
mrr9GYnukzfloinH/DOyc+fOgBtvvLHTjh07ggBeeOGFbeedd96hefPmhd53333xBQUFfsHBwSVT
p079tX///gUvvfRSuxkzZrTJy8vzKy4ulgkTJux88sknO7Rt27Zo3bp1IX379s2bMWPGr35+fgwa
NKjH888/v33o0KF5oaGhSTfffHPm7NmzI4KDg0u+/PLLjXFxca6UlJQW11xzTZfDhw/7jRw58sDk
yZNj8vLyVpSP8ZprrolPT09vccEFFySMHTs2a+jQoQeris3lcnHHHXfEzps3L0JE9Prrr89SVTIz
MwOHDRvWvU2bNq6ff/55/RtvvNF20qRJ7VVVRowYceC1117bAU4P39ixY/csWLAg/KWXXtp2/vnn
H6ybf53modYuQlU9CzgL2AO8ISKrReQxD9ruCBV+Aaa799Wlm4D/1nGbxhhjjGnECgoK/MoPEXzr
rbfatGvXrvjvf//7tuuvv77Lm2++2ebAgQMB999/fxbA4cOH/ZKTkw9t3Lgx5YwzzsgdP358h/Lt
5eXlye23397lk08+2bR+/fpUl8vFc889Vza0rl27dq7U1NS1t9122/6xY8fuX7Nmzdp169al9ujR
4/BLL70Uee655x4aMWLEgaeeeio9LS0ttXfv3gWeth0ZGelKTU1de9NNN+155plnYip/r1988cXG
uLi4grS0tNSRI0cerOr1AcaNGxc/ZMiQ3HXr1qWmpKSkDhgwIH/SpEnppde+8cYb6e+9917r1atX
h6xduzbl22+/Xf/444/Hbt26NRAgNTU19J///Oe2ukyuGprbb7897k9/+tPuNWvWrP3Pf/6zady4
cZ0B+vfvn79kyZK0tWvXpk6YMGHHgw8+GFt6TUpKSujMmTM3LVmyZB3A2rVrQ1599dXtGzduTNm2
bVuLb775plXl1zl8+LDf6aeffnDdunWpp59++sGXX345CuDOO++Mu+OOOzLXr1+fGhsbW2Xv1Icf
frgtOjq6aP78+esnTJiQWV1skyZNitq2bVtQampqyvr161NvueWWvY899lhm6bU///zz+i1btgRO
nDix43fffbc+NTU1ZcWKFS3ff//91qUxDh48+NC6detSLbk6dh7196lqBvCSiMwDHgQeB57yZmC1
EZFHARfwwTFedxtwG0B8fLwXIjPGGGMMwPH0NNWF6oYIjhkzJmfatGltHnzwwU7Lli1LKd3v5+fH
Lbfcsg/gpptu2nvJJZdUGA2zatWq4NjY2IJ+/foVANxwww17X3311WggE+D3v//9/tJzly1bFvL4
4493zM3N9T906JD/sGHDKldirqC2tq+55pr9AIMGDcr7/PPPa50PVt3r//jjj2HTp0//FSAgIIB2
7doVZ2Vl+Ze/9vvvvw+74oor9gUEBBAXF+caPHjwwYULF4ZGRESU9OvX71BiYmJhba9/rI6np8lb
fvjhh/ANGzaElG4fPHjQPzs722/fvn3+V155ZZctW7YEi4gWFRVJ6TlDhgzJiYmJKS7d7tu376HS
oXu9e/fO27RpU1Dl1wkMDNSrrroqG2DgwIGH5syZEw6wYsWKVrNnz94IcMstt+ydOHFibOVrK6su
trlz54aPGzduT+mwxfIxllq4cGHL0047Lbe0t/bKK6/cN3/+/FbXXXfdAX9/f2644Yb9la8xnqm1
B0tEeorIRBFZDbyMU0Gw1n9wYAcQV2471r3vhInIDcAoYKyqHlMpWFV9U1WTVTU5KqpZzus0xhhj
mqXi4mLWr18fHBwcXLJ3795qP2QWkeoOVan8fJbbbrutyyuvvLJt/fr1qQ899NDOgoKCEyooEBwc
rAABAQHqcrlqDayuX79UaGiox3N2GitVZfny5WvT0tJS09LSUjMzM3+JiIgoeeihhzoOGzYsd8OG
DSlffPHFxsLCwrL3tPL70qJFi7K/S/39/anq3ywgIED9/PxKn1d5jqdqiu1EBAUFldi8q+PnyT/C
FJxFhs9X1eGq+pp77lRtlgAJItJFRIKAq4DPTyBWwKlMiNOLdqGq5p1oe8YYY4xpHp588smY7t27
50+dOnXzTTfd1LmgoEAASkpKKK0WOHXq1HaDBg3KLX9d//7983fs2BG0Zs2aFgDvvfdeuyFDhuQe
/QqQl5fnFx8fX1RQUCAff/xx29L9rVq1Ks7JyTnq765jadsT1b3+GWeckVs69NDlcrF3717/iIiI
4kOHDpXFNHTo0Nzp06e3dblc7Ny5M2Dx4sWthgwZcuh4Y2lszjzzzJynn366rBLjjz/+GAKQk5Pj
HxsbWwjwxhtvRHrr9U855ZSDU6dObQMwZcqUtrWdX1Ns55xzTs4bb7wRWVTkjDTcvXu3P0DLli2L
s7Oz/QCGDBly6Oeffw7btWtXgMvl4tNPP207fPhwGw5YBzyZg3W6qv5DVY+p2o2quoA7gVnAWmCa
qqaIyDgRGQcgIu1FJB34E/CYiKSLSLj72EfAT0AP9/6b3U2/AoQB34jIShF5vfQ1RWQL8AJwg/ua
XscSszHGGGMav8pzsO64446Oq1atavH+++9H/vOf/9w+cuTIg6eddlru+PHjTwIICQkpWbx4ccuE
hITeCxYsCHv66ad3lW8vNDRUX3/99S2XX355t+7du/fy8/PjgQceqLLy3vjx43cOGjSoZ3JycmJC
QkJZQYqxY8fue+mll9r37NmzV0pKSovjadsT1b3+a6+9tm3+/Plh3bt379WnT59eK1asCG7fvn3x
wIEDDyYkJPS+/fbbY6+77roDvXv3PtyzZ8/ew4cP7/7EE0+kx8fHu2p6vcYqPz/fLyYmpl/pY+LE
iTFvvvnm9uXLl7fs3r17r27duvV+5ZVXogAeeuihjIkTJ8b27Nmzl8vlvbfj5Zdf3v7yyy/HdO/e
vdfGjRuDW7VqddSwvsqqi+2+++7bExsbW5iYmNi7R48evd5+++22ANdff33WyJEjuw8ePLh7p06d
iiZMmLBj2LBh3Xv27Nm7f//+h6699toDXvsGmxGpboSdiExT1SvcQwPLnySAqmq/+gjQm5KTk3Xp
0qW+DsMYY0wVRGSZqib7Og5zbFatWrWlf//+Wb6O41iEhoYmVa7WZkx9y83N9WvZsmWJn58fb775
ZptPPvmk7bfffrvJ13FVtmrVqsj+/ft39nUcDVlNgyvvcX8dVR+BGGOMMcYY01z98MMPoffcc0+8
qhIeHl48derULb6OyRyfahMsVS3tHr9DVR8qf8y9wO9DR19ljDHGGNO4WO+VaQhGjhx5cN26dfW+
QLape54UuTi3in0X1HUgPpGfAcUFtZ9njDHGGE+VlJSUHHdVNGNMw+X+v93kK0qeqGoTLBH5g3v+
VQ8R+aXc41fgl/oL0YvydsBXfWDHV76OxBhjjGkq1uzZsyfCkixjmpaSkhLZs2dPBNBkF5uuKzXN
wfoQ+C/wNDC+3P5cVd3n1ajqi38LOLgR5o+CDr+FAX+H8O6+jsoYY4xptFwu1y0ZGRmTMzIy+uDZ
SBljTONQAqxxuVy3+DqQhq7aKoJHnSgSDQSXbqvqNm8FVV+Skwfq0vfHwponoCgH/AKhx73Q5zEI
DPd1eMYY06xZFUFjjDGNUa0JloiMxllbqgOQCXQC1qpqb++H511lZdoP74ZVj8DmKc6B4PZwyrPQ
5VoQ+/DNGGN8oakmWJGRkdq5c2dfh2GMMaaSZcuWZalq1Im2U9MQwVJPAacBc1Q1SUTOAq490Rdu
UEJi4LS3IWEcLL0b9i6CRdfDhn9C8svQ7lRfR2iMMaaJ6Ny5M7YGozHGNDwisrUu2vGke6ZIVfcC
fiLip6rzgCb3iSLgJFLn/QCnvev0Yu39GWYNgkU3O71cxhhjjDHGGFMDTxKsAyLSClgAfCAiLwKH
vBuWD4kfdP09jF4PPR905mVtngJfdoe1L0BJka8jNMYYUw9EZIqIZIpIlRWzxPGSiGx0V9kdUN8x
GmOMaXg8SbAuAg4D9wH/AzYBo70ZVL3ZuROWLIGSKsr5B4ZB0rPw2zVOhcGiHFhxP3zdD3bNrv9Y
jTHG1LepwMgajl8AJLgftwGv1UNMxhhjGrhaEyxVPaSqxarqUtV3VfUl95DBxm/XLhg0CDp0gBtv
hOnTITu74jnh3WH4VzDsSwhLgJw0mHc+zL8Icjf5Jm5jjDFep6oLgJqWJbkIeE8di4DWInJS/URn
jDGmoappoeFcEckp98gt/7U+g/SaqCiIj4fdu2HqVLj8coiMhLPPhkmTYO1aKK2y2PF38NvVTnXB
gFaw43P4qhesehSKDvr02zDGGOMTHYHt5bbT3fuOIiK3ichSEVm6Z8+eegnOGGOMb1SbYKlqmKqG
l3uElf9an0F6TXw8bNkCq1fDM8/AkCFOQjVvHjzwAPTqBd26wV13wf/+B0UKvR505md1+T2UFELK
/4MvE2HLh0eSMWOMMaYcVX1TVZNVNTkq6oQrABtjjGnAPFrkSUTOFJEb3c8jRaSLd8OqRyLQpw88
9BAsWAB79sBHH8F11zm9Wb/+Cq+8AhdcAG3bwujR8O5MiH0Kzv0R2g6Ewzvgx7EwZyjsW+Hr78gY
Y0z92AHElduOde8zxhjTjNWaYInIBOAh4GH3riDgX94MyqfatIGrroL33oOMDPjpJ3jsMUhKgsOH
4csv4Q9/cHq/zrkdvhsB/uMhIBL2LIT/DYTF4yA/y9ffiTHGGO/6HPi9u5rgaUC2qu7ydVDGGGN8
y5OFhscAScByAFXdKSJhXo2qofD3h9NOcx5//atTdfDrr53HN984QwtXr3bObdMaTu0BJ2+A3Ddg
6yfQ70lI+AP4efI2G2OMaUhE5CNgOBApIunABCAQQFVfB74GfgtsBPKAG30TqTHGmIbEk7/8C1VV
RUQBRKSll2NquDp0gFtucR4FBfD99/DVV85jwwaYfQBmAwKcfABOuRvOfBEufwNOOsfX0RtjjDkG
qnp1LccV+OOxtrt3L+zf7wyYMMYY0/R4Mgdrmoi8gVN+9lZgDjDZu2E1Ai1awIgR8Pe/w/r1zuPv
f4dzz4XAINgAfArcswl6j4DRXeCjN+GgVRw0xpjmbMsWiI52pvZOngxZNqLcGGOaFFEPKt+JyLnA
eTh9M7NU9RtvB1YfkpOTdenSpXXf8MGDMGcOfPk5fDEdMnOPHAv0h2HDYNSF8NvfQkJC3b++McY0
ASKyTFWTfR1HXevZM1kvumgpn34Kmzc7o9GHD4fLLoMxYyAmxtcRGmNM81RX9x2PEqxKL+wHXK2q
H5zoi/ua1xKs8lThp1nwzoMwf7UzUr/8W56QAL/7nfMYOhSCgrwbjzHGNBJNNcEqvfeowqpVzhr3
n37qDIQQcW4Fl10Gl1zijEw3xhhTP7yeYIlIOM7Y8o44lZK+cW8/AKxS1YtO9MV9rV4SrPIyv4e5
d8CCNbASWB0AB11HjoeGQnIyDBp05BEf79xxjTGmmWnqCVZ5qpCS4iRb06c7zwHOOONIshUf74Ng
jTGmGamPBGsmsB/4CTgHiMYZIniPqq480RduCOo9wQIoKYZNk+GXRyFvL2wU2JoEy/JhTerR50dH
V0y4Tj3VWY/LGGOauOaUYFWWlgaffeYkWyvdd9zBg+HSS51H1671EKgxxjQz9ZFgrVbVvu7n/sAu
IF5V80/0RRsKnyRYpQr3wy8TYMM/QYuhRTvo+CBkJsCylbBkCSxe7JSbquzkk51EqzTpSkqCkJD6
/x6MMcaLmnOCVd7GjUeSrdLLBgxwerYuu8ym8hpjTF2pjwRruaoOqG67KfBpglXqwBpYdjfsnuds
B7WBzmOh283Quj/8+quTaJU+li2D/Eo5bkAA9O1bsaerZ09n5rQxxjRSlmAdbcuWI8nWokXOvn79
jiRbPXvWXZzGGNPc1EeCVQwcKt0EQnAWUhSc5T/CT/TFfa1BJFjgDL7f/m9IfRr2LTuyv02Sk2h1
vsZJvACKipzB+eWTrpQUKCmp2GbLljBwoM3nMsY0WpZg1Sw9Hf79byfZWrjQuZX06nUk2erTx37l
G2PMsfBZFcGmpMEkWOXtXwWb3oYt/3KGEQL4B0PsJU6yFTMcpNLyZQcPwooVFZOuLVuObrsxz+cq
LASXyykEYoxpFizB8tyuXfCf/zjJ1vz5zmduCQlHkq2kJEu2jDGmNo0iwRKRkcCLgD8wWVWfqXRc
3Md/i9M7doOqLncfuw+4Baeo+WrgxvLzv0TkfuB5IEpVs0SkHTAdOBWYqqp31hZfg0ywShXnw/YZ
sHkKZMyhrLZ7yy7Q9UboegO0jKv++szMI/O4Sh/79h19Xl3P51KFw4edpK8uH0VFTvtnngnXXAOX
Xw6RkccfpzGmwbME6/hkZsKMGU6yNXcuFBdDly5Hkq1TT7VkyxhjqtLgEyx3YYz1wLlAOrAEZ/2s
1HLn/Ba4CyfBGgy8qKqDRaQjsBDopaqHRWQa8LWqTnVfFwdMBhKBge4EqyWQBPQB+jT6BKu8g1tg
81TY/A7kbXPvFDjpfKdXq+OF4F/L+lmqzoqW5ZOu2uZz9e3r9BodPAiHDnmeDHnjZ6p0Pllx8ZE4
zzsPrr4aLroIwsLq/jWNMT5lCdaJ27sXZs50kq05c5zPquLjnUqEl10Gp50Gfn61t2OMMc1BY0iw
Tgcmqur57u2HAVT16XLnvAF8p6ofubfXAcMBP2AR0B/IAWYAL6nqbPd504G/AjOBZFXNKtfmDe59
TSfBKlVSDLu/dYYQps+AkkJnf4tI6HytuzBGH8/b83Q+17EKDoZWrer2ERTkJG8zZsBHH8Hs2UeS
rZAQuPBCp2dr5EhbrNmYJsISrLq1fz988YWTbM2a5Yy87tDhSOn3M8+02kjGmOatMSRYlwEjVfUW
9/Z1wODyiY+IfAk8o6oL3dvfAg+p6lIRuQf4G3AYmK2qY93nXAScrar3iMgWjjHBEpHbgNsA4uPj
B27durWOv/N6UrAXtnzgJFsHfjmyv90gJ9HqdBUEHkcdkvLzudLSnOTlWBKhli3r5w69Zw98+il8
+CH88MOR/W3aOH8pXHMNDB1qfy0Y04hZguU9OTnw5ZdOsvXf/zqDGaKjnQWNL7sMhg1zBgoYY0xz
0qQTLGAT8BlwJXAA+BRnftW/gXnAeaqafTwJVnkN4SZ3wlSdyoOb3oatH0JRjrPfPwTiL3eSragh
TXvA/dat8PHHTrL1S7lks0MHuOoqJ9kaMKBpvwfGNEF1mWCJSAjOWo7r6qK9E9HQ7j0HD8JXXznl
37/6CvLyoF07GDPGSbbOPhsCA30dpTHGeF9d3Xe8OfJ6B1C+CkOse58n54wAflXVPaq2QzXjAAAg
AElEQVRahJNY/QboBnQBVrmTq1hguYi098p30BiIQLtkGPQajNkFp78P0cOh+DD8+h7MGQZf9oCU
ZyBvp6+j9Y5OneChh2DVKli9Gh55BDp3hp074YUXIDkZEhNh4kRYv97X0RpjalNSAhkZddaciIwG
VgL/c2+fIiKf19kLNHKtWsGVV8K0ac7ggM8+c6a4fvyxM+o6JgZuvNFJvgoKfB2tMcY0fN5MsJYA
CSLSRUSCgKuAyje0z4Hfi+M0IFtVdwHbgNNEJNRdafAcYK2qrlbVaFXtrKqdcYpnDFDVursTN2YB
odDlWhgxD0ZvgN6PQEgHyN0Aqx6GmfHw3WinOmFJka+j9Y4+feBvf3MKevz0E9x1lzPuZf16eOIJ
6NHDSbheeAF2VM73jTE+N3Om8xf9xRfXZasTgUE4IyJQ1ZU4H9aZSkJDnWGCH37oJFszZ8Lo0U4J
+FGjnF+n113nTIc9fNjX0RpjTMPktQRLVV3AncAsYC0wTVVTRGSciIxzn/Y1sBnYCLwF3OG+9mec
IYHLcUq0+wFv1vaa7l6tF4AbRCRdRHrV6TfVmISdDP3/BhdthWFfQdwlgMDOL+H7MTAjDlY8CNlp
vo7UO0Sc8lgvveQkUrNmwfXXO9UGly37/+zdd5hTZfbA8e+hd+m9ihTpIIIUFQQpNkRZy9rXFfta
10XXXtbuurv29rOsrsqAioqioiKoNBHpIL136XVmzu+Pc2Myw8wwJZkkM+fzPHmS3Htzc3Ink/ee
+za45RZo1Aj69oWXX856CHvnXOwcPAjffWc1zl262Bk7wJFHwuDBcOONUX03Vd2eaVmu2seLyCAR
WSgii0VkRBbrjxCRj0XkFxGZKyKXRSXiBFCunI0f9MYbNvT72LHWZHDsWGs+WKuWtcJOSbGBZp1z
zhmfaDiB2sHH3L6NsOwt66+1Y354ec2e1ler8TlQulL84isMe/fa2cE771gP7wPBSIylS9tJ3R//
aJdrfULjoiEtDebMgUmTYPp0qFLFmo82aRK+r17d++cVph074NJLbczwnTttIJpeveD22609WoSo
dTYWeRUYD4wAzgb+ApRW1asO87rcTDdyB3CEqv5NRGoBC4G6qnogu/0me9lz8CB8+60lVh98YDVd
5cvDKadYAnbqqT5zhnMuOSX8IBfJINkLuXxThc2TYemrsOI9SN1ly0tVhMbnWrJVs0fRP+ncts3O
Dt55x2bjDA1PX7GiXZ49/3w4+WTv3Z1M9u+3ud4mTYKJE22Eye2ZKy4yqVQpY8IV+bhpU2sTVdT/
F2Jl716rpfr8c0tu77vPfn9697bmvIMG2QgKRxyR5cujmGBVAP4ODAAEa1nxQOTk9dm8LjfTjdyO
9SW+FmgKfAm0VNVs57soSmVPaqr9u6WkWN+t9euhbFn70w4bZtersvnzOudcwvEEKwqKUiGXbwd3
wcqRlmxtihjuvMrR1p+rQiMoVSl8K10p4/OS5YrGyef69dbD+513YMqU8PIaNeCcc6xmq2dPn5Ez
0WzfDj/8EE6opk49tBd+kyZ2Qn/ccTYW9YoVsHx5+H7nzpzfo1y5rBOv0LJ69Xw6gMzeesv+l779
1o552bI2isIbb+RpN/Eepj2Xo+FWxvoTtwYqA+eq6qdZ7KtoTBGSg7Q06/o6cqQlW2vW2PWpAQNs
9owhQ6zC2DnnEpUnWFHgCVYm2xfA0tdg2RvWnDA3pETGhCurJCyrZTltU7IClIjjCeuSJTaZ8Tvv
wPyIppSNG4eHfe/QoWgklslm3TpLpEIJ1axZh06M3a6dJVTHH2+3Ro2y3hdYbcq2beGEK3PytWLF
4fvnlS5t75E5+QrdN2xYtCcU2rHDaoC//RaefNKSzWuusWWDBtnthBPy1ew2ijVY35BFnytVPekw
r8tNgjUM6AXcjI10+yXQUVV3ZLff4lD2pKfb9Y6UFLutWGH/BiedZDVbZ55pfbiccy6ReIIVBcWh
kMuX9IOwdiys+8Lm1UrdZbeDu8KPQ7e0HFvY5F/J8ocmXuXqQvt7oVqH2LxnZqp2Av/OO5ZwrVoV
XtemjTUhPOMMaN/ek61YUIVff82YUC1ZknGb0qVtVMjjj7ekqlev6F8i37kz++Rr+XLr/Z+TEiUs
ycoq+Wre3JKzZEvAVqyw/4tx46wZZmqqNbX8+Wc46ijr21imTIHfJooJ1jERT8th/bBSVfW2w7wu
N00EP8Xmc5wYPP8aGKGqU7Pbb3Ere1RtbKFQsrVkif1b9OljydbQoVC3+E624pxLIJ5gRUFxK+Ri
Ij0VUndnTLoyJ2JZJWaH2yY7patC38+g5nGF9xnBLsd+/72dVI4cCVu2hNfVrw8DB9qV+v79vQ1M
fqWm2lxmoYRq0iTYsCHjNpUqWVPNUELVrVv8ByTZuxdWrsw6+VqxwuZjy+l3tnRpaNbMEpPQrXlz
u2/aNCqJSoFt3gxffgmdO9uccuPG2fe9U6dwLVWPHlGPNZZNBEVkqqp2O8w2pbBBLvphczROA/6o
qnMjtnke2KCq94pIHWz0246qujm7/Rbnsid03SolxX5KFy6061PHH2/J1llnQYMG8Y7SOVdceYIV
BcW5kEtomm4TJWdOuuY/Cas/sME4TvwE6vSJT3wHD9rJ5siR1nk/ckLUEiWge/dwwtW1q/fPyc7e
vdbfLVQ79cMPsCtTcl27dripX+/e0LFj8tX2HDhgtZ+Zk6/ly22+ttWrs39tiRJW0xWZfIVuRx5p
/cNiIS3N/jaff2636dPtzPi+++Duu62f29at1v8shqJYgxV51aMEcAzwb1VtlYvXngI8DZQEXlPV
h0JTjajqCyJSH3gdqIcNoPGIqv43p3162WNUYd68cM3WnDm2vGdPS7bOPttaZjvnXGHxBCsKvJBL
MumpMPlSWP62Da7RexQ0OCW+ManC7NnhE9FJkywBC6le3Xp4Dxpk9zE+IU1oW7daLWCohmr69IzH
CqzmJjKhatGi6De/3LvXEq3Fiw+9rVx5aB+zEBFrehhZ4xVZA1Ypj1MurF1rt65dLYGqUcNi6949
XEt1zDGFesEgignWMqwPlgCpwDLgflWdVNB954eXPVlbsMAGx0hJgZkzbdmxx1qyNWyYXVNwzrlY
8gQrCryQS0KaDtOuhsUvQYnS0PN/0PjseEcVtnMnfPONJVuffWa1FJE6dQrXbvXsmRjNv2JB1Wpt
Qk39Jk4MX54OEbEaqciEqjgnoFnZv9++Q1klX8uXW7PK7NStm3XNV/PmULWqbbNwoU1V8MEHNiJB
x47hM9uJE6Ft27g2eY33KIKx4mXP4S1eHE62Qoeqc+dwstWyZXzjc84VTZ5gRYEXcklKFWbcAgv/
aaMYHvc6NLso3lEdKjRIw+efW5+Vb76xGoGQSpWgXz9LtgYOtH44ySYtDZYts9EWM992ZBpErWxZ
6zMVSqh69PAJcgoiNdVquLJKvpYsCU+inZUaNaypZaiPW7NmVsN6+eVWZZAgClrQichZOa1X1dH5
3XdBeNmTN8uXw+jRlmz9+KMta98+nGy1aRPX8JxzRYgnWFHghVwSU4XZ98Kc++35sc9Di6viGtJh
7dtntQKh5oTz5mVc36pVuHbrxBPjP3hDpP37LVmcP9/iDiVRCxceOu9USI0a1rwslFB17WpJlou9
tDSbhGjxYli0CCZMsAmYV660Jn77shn9s1w5++6FmgS2ahXXJppRSLD+L4fVqqp/yu++C8LLnvxb
vTqcbE2aZEXB0UeHky0f1NU5VxCeYEWBF3JFwLzHYWYw0nLnx+HoW+MbT16sXGk1W59/Dl99lbHG
p2zZjCe6rVsXzlnDzp3WESKyJmrePOsjlJaW9WsaNrQznNCtTRu790lu4mvlSrj/fhgzBjZtsu/U
gAHw8MNQrdqhNV7z51t/wkiNG4e/g/36QZUqhfoRvImgy8m6dda6NSXFriGkp1sr2FCy1aWLJ1vO
ubzxBCsKvJArIhY9B9Ovtcft7oH29yRfqXrwYMZR2376KeP6xo3DtVv9+hW8ad3mzRkTqNDjyLm+
IpUoYT3MMydRrVsX+km3y8aOHTB2rNUcnnyyNf9r3RoGD7aJhgYPPvzAFxs22AiZoWatmyNGGi9V
yvoNhhKujh3texFD0UywRORUoC02DxYAqnp/NPadV172RN/GjfDhh5Zsff21XQ9q2jScbHXrlnzF
gnOu8HmCFQVeyBUhS9+AKX+yQTBa32K1Wclcmm7cCF98kfWJbsmSGU90O3XK+kRX1ZqJRSZQoYQq
cn+RypSx3uOhBCp0a9kydkOCu/zbuNFqqEaPhvHjrd/VH/4A779v61NT8z+sfXo6zJgR/g7++GPG
Wsw6dTKOkFmzZsE/TyZRHEXwBaAC0Bd4BRgGTFXVywu67/zwsie2tmyBjz6yQTK+/NKuXzVqZMO+
Dxtm3T9jfG3AOZekPMGKAi/kipiVKfD9+aCpcNRVcOyzNghGsgud6IaaE2Y+0a1d205w+/SxxCmU
UC1YYE3+slKpUsYEKlQr1axZ8s0zVdxs3hxOZnr2tO9Ds2ZWS3XWWXDccbEZSn3bNkviQrWskfN3
iVgfu1DS361bVL5HUUywZqlqh4j7SsBnqnp8gYPMBy97Cs+2bfDxx1azNW6cdRmtVy+cbPXu7VMV
OufCPMGKAi/kiqA1n8LEsyF9PzS9CI57DUoUsYQhpxPdzGrWPLRZ39FHW7+pZK7hK05Cs7GOHm0d
TubNsz5VlStbL//KlaFDh8L9e4ZiCiX9EyZkHLWwalVrpjhwoN0aNszX20QxwZqiqt1FZDJwFrAF
mKuqRxV03/nhZU987NgBn35qydbYsTbWS+3adm1i2DC7RuXXl5wr3jzBigIv5IqoDd/AhNMhdTc0
OsvmyioZg/mmImsSfvsN9uzJuL5EifC8Tlu2HDpyW8mSNldRaF+ZR+MrVcqaYYGdUGcedrtMGXv/
efNg5Eir5Wra1OYuCiVSPtBEchs/Hq6+2kZwBGvbNHQoDB+eWEPc795tSVYo6Q/FG9KuXbh2q3fv
XI8mGcUE6y7gP0A/4Fls0uGXVfXugu47P7zsib9du2yqwpQUS7p277bui2eeacnWSScV3WkKnXPZ
8wQrCryQK8I2/QjfDoaD26HeYDh+FJQqX/D97t1rJfJLL9nErKGk6JJL4M03M25bo0a4r9NZZ1nt
Q6SmTW0OKbAmfl9+mXF9+/Ywa5Y97tEDJk/OuL5HD/jhB3vcrh3MnWu1GHXrWo3BgAHw4IO2/uOP
bTCKhg2hQQPvT5WIDh60JOWDD2DIEPv7zZ0LN91k358hQ5JnIualS8O1W+PH29lrSIUKdvYaSria
N892N1EYpr20qh7MtKwsUE5Vt+d3vwXlZU9i2bPHvq6jRlmXxp07rRL2jDMs2Tr5ZP/JdK648AQr
CryQK+K2/gzfDID9m6H2iXDix1C6cv72tXw5PP20JVG//QYtWsBll8Htt9v6b7899Kp92bJw8cX2
+MsvbR+RKleG886zx2PH2oAUkapXt44CYD22N27MuL52bTvpBkv6Nm2yUeBWr7Zbhw7w2GO2vmpV
2B5xPlmzpk0q+8gj9vyxx6y2rGHD8K1ixTwcIJcvaWl2RvfBB/DJJ/bdKl/e/i5/+Uu8o4uOAwfg
++/DtVuhiwYhzZuHk62+fTN876KQYG0ExgD/A77WBCnwvOxJXPv22awZKSn2s7ttm/1Un366JVuD
Btm/qHOuaPIEKwq8kCsGts+Hr/vB3nVQozv0/QzKVMvda/ftC7cb+f57O/k76yxrntW3b3L1YVqw
wBK4UPK1erUNRHDZZfYZsxq++8474YEHbP0NN2RMvho2tBq4ww37HU0HD1osu3fbJeemTaF0aZvH
afbs8PLQNrfeaped33vPEpg9e6zvUMjo0fb6F1+05CZSqVLhGsenn7ZamEiVK8M779jjhx8O1ySG
1K4Nr75qj+++G37+OeP6Jk3gmWcsnmbNrHPI6adb878BAxJrkuloW7s2PELmF19YUhlSpoxNSh0k
XNK+fUETrBrYiIHnAS2AUcD/VHVyji+MMS97ksOBAzbke0qK/Rxs3Wr5/6mnWrJ1yil+Hcq5oiZa
CZZ353RF2xFHQ/+JlmRtmQJf9YWTvoBytbN/zYIF1gTwjTfgnHPg+edttLY1a5K3T1Pr1nbLSsWK
lnysXZsxATvuOFu/aZN1UtiwIWOC8u9/w/XX2yS1V155aALWrZslGhs3WnPKUOITul1wgY2d/MMP
dowzr3/3XWjVyv4W111nCVakJUtsbq6UlHBNYqQrrrAEa9UqS3DKl884XFjos2zfbp89UmRP999+
O3R9ZP+nrVsPXR95nLZsOXR9KIESscvlTZpYslcc1K8Pl15qt7Q0mDYtPBT8lCmWzI4fD3/9a4Hf
SlW3AC8CL4pIfeAPwD9FpDbwrqr+vcBv4oqsMmXClavPP28teFNS7NrM++/bT8rgwZZsnXqqTwno
nAvzGiy/ilg87F4FX/eHnYugSms46Suo0CDjNqNHw7/+Bd99Zye7Q4fCNdfAiSfGJ+ZEc+AArFsX
TsC6dLGmkrNmWYK1erWtDw0h/8EH1mP8k0+sdiaz8eOtL86YMXDjjZboVaxoyUfFivDUU7b/yZOt
rU5oeeh2xhl2RrNuHaxfn3FdhQreQz0ZbdliCWfQnFDWr4/aRMMAwfDsZwE3A/VUtU609p0XXvYk
t7Q0mDjR+myNGmU/QWXL2oCZw4bZz13VqvGO0jmXH95EMAq8kCtm9m6wPlnbZkHFZtBvPKxPsz4g
InDVVdZXavhwazpXO4daLpe1tLRwP7CjjrJ+ZNu2Wf+0yASoYkWrXUqmZpaucKkiJUoUuKATkXLA
6cD5QE/gc+Bd4EtVTcvptbHiZU/RkZ5uU9GlpNht9Wq7PnfyydaFdsgQa2XunEsOnmBFgRdyxdD+
rfDFQBg3Hb4tA3ODDvg9e9rQURUr2vDqzrm4i8IgF+8A/YEJWFL1qaruy/lVsedlT9GUnm4tXkPJ
1vLl1ir5pJOsZuvMM/26nXOJLloJlp9JuuJj2zb4+z/gsqU2E87GA3BBRagVDLVeubInV84VLZ8D
zVX1D6o6Kj/JlYgMEpGFIrJYREZks00fEZkpInNFZEKBo3ZJqUQJ6N4dHn/cZiqYPt26Ei5bZq2o
69WzZOu556xZoXOu6PKzSVe07d8PCxfa43Ll4L//hT59YexH8L8BcMpumHkWbJ4S3zidc1Gnqm+q
6s78vl5ESmKXYwYDbYDzRaRNpm2qAs8BZ6hqW2wgDVfMicAxx9ggo4sWwS+/wN//bonVtdfadIQn
nGBjBa1eHe9onXPR5gmWK5p+/RVuu81Gszv1VGu7Ua6cXUpMSYHBZ0CfMdBwKBzcZgNgbPg23lE7
5xJLN2Cxqi5V1QNYM8Mhmbb5IzBaVVcCqGqmCetccSdi0xLefz/Mn2/zh997rzWquOEGG0y1Rw94
8slDp0t0ziWnmCZYh2taIebfwfpZItIlYt1yEZkdNLuYnul114vIgqA5xmMRy28P9rVQRAbG8rO5
BPXdd9CvH7RsaaPQnXACPPtseH3kDJEly0Lv96HpBZC6C74dDGs/K/yYnXMxIyIlRKRnPl/eAFgV
8Xx1sCxSS6CaiHwrIj+JyMXZxDFcRKaLyPRNmzblMxxXFLRpY9PjzZpls4I8+KA1trj1VpsW79hj
4dFHbYo/51xyilmClZumFcG6FsFtOPB8pvV9VbVTZGczEemLXUHsGDTHeCJY3gabTLItMAh4LojB
FXWLF9tcSwCbN1vj94cesvmPRo2ysXOz61tVohT0eBOOGg5p++C7IbBqdOHF7pyLKVVNx8qiWCkF
HAOcCgwE7hKRllnE8ZKqdlXVrrWSdT49F3WtWlnTwRkzrCh79FErrkaMsFkqOne24izU0t05lxxi
WYOVm6YVQ4A31UwGqopIvcPs92rgEVXdDxmaYwzBJo7cr6rLgMVBDK4oOnDAZnrs399Koeees+VD
htgEtHfcYT2Kc0NKwLEvQKubIP0gTDoHlv03drE75wrbeBE5WyTP8wKsARpFPG8YLIu0GhinqrtV
dTPwHdAx/6G64qp5c2vZPmWKNRV86imb0u/OO22e+Pbt4b77rIlhMR4A2rmkEMsEKzdNK3LaRoGv
giYXwyO2aQkcLyJTRGSCiBybh/dzyU4V7rrLGq2fe671tXrgAbjiCltfsmT+RgIUgS5PQru7QdPg
x4vh1xejG7tzLl6uBEYCB0Rkh4jsFJEduXjdNKCFiDQTkTJYK4kxmbb5COgtIqVEpALQHZgfzeBd
8dOkCdx0k80isnq1DYZRvbolWO3awdFHW+I1c6YnW84lokQe5KK3qnbCmhFeKyInBMtLAdWB44C/
Au/n5aqkt4NPUlu22L2ItZXo0QPGjrXmgHfeaUMyFZQIdLgPOj0GKEy7CuY/WfD9OufiSlUrq2oJ
VS2tqlWC51Vy8bpU4DpgHJY0va+qc0XkKhG5KthmPjYc/CxgKvCKqs6J3adxxU2DBnD99TBhAqxd
aw02GjSwEQo7d7ZGHCNG2LDwnmw5lxhimWDlpmlFttuoauh+I/AB4eZ+q7ERm1RVpwLpQM1cvp+3
g0826enWTqJJE5g61Za9+y58+CEMHmw1VtHW5q/QNeiy8fOtMPs+L7WcS2LBgEoXishdwfNGIpKr
JuSqOlZVW6pqc1V9KFj2gqq+ELHN46raRlXbqerTsfkUzkHdunD11TB+PKxfDy+9ZE0Ln3jCBsdo
1swGy5g82YpP51x8xDLByk3TijHAxUHhdxywXVXXiUhFEakMICIVgQFA6Irgh0DfYF1LoAywOdjX
eSJSVkSaYQNnTI3h53Oxtno1nHwy3HKL9bVq1syWF8ZkwC2vgeNet/5Zs++Fmbd5kuVc8noO6IEN
qQ6wi9gOfOFczNWqZa3jx42zcZ7+7/+gbVtrTtijh12XvPFGmDgR0tLiHa1zxUvMzlRz07QCGAss
xQakeBm4JlheB5gkIr9gSdKnqvp5sO414EgRmYMNnHFJUJs1F3gfmIc117hWVf0nJVmNHGk9eqdM
gZdfhg8+sNKkMB15CfR6D6QUzH8Cpl0D6pcEnUtC3VX1WmAfgKr+hl2cc65IqF4dLr0UPv3Ukq23
3rKJjl94wWYradjQJjj+5htITY13tM4VfaViuXNVHYslUZHLIptVKHBtFq9bSjajMAUjEl6YzbqH
gIcKELJLFAsX2vi1//0vHHVU/OJoPAxKloeJZ8PiFyB1Nxz3mg3v7pyLjd0rYd3nh98u9w4G03Yo
gIjUwpqXO1fkVK0KF15ot507LelKSbEaruees2uVQ4fCsGHQpw+ULh3viJ0rekSLcbOnrl276vTp
0w+/oSsc331nw6/37x++xFYqQRKZDd/AhNMtwWp0NvR8B0r6BXDnoiJtH2z8DtZ+bonVDhuETy7g
p8h5EPNLRC4AzgW6AG8Aw4C7VPX9gu47P7zscfGwezd89pklW598Ys+rV4czz7Rkq18/KOPFmivm
RCQ65Y4nWF7Ixd2BAzat/WOPQa9elmjlebqaQrDpR/h2MBzcDvUGw/GjoFT5eEflXPJRhZ2LgoRq
HGz8FtL2hteXqgx1+yEnfhiVgg5ARFoD/QABxgej/8WFlz0u3vbutb5bo0bBmDGwYwcccYRNJXn2
2TBgAJQrF+8onSt8nmBFgRdyCWDePLjgApvMY/hwePJJqFQp3lFlb+vP8M0A2L8ZaveBE0ZDmWrx
jsq5xHdwJ2z4OlxLtXt5xvXVOkO9QVB/ENTsASVKR6+gE3lLVS863LLCUrlZZT3mnmPi8dbOHSI9
HX77DTZths2bIS0VSpSEmjWsOWH16oUztpRziWDCZROiUu4kSPsrVyzNn2+9cCtVgo8+gjPOiHdE
h1e9M/T/Dr7uZ1fdP24B7e6FFldCCW/I7tzvVGHbL+Faqk2TQCN615etAXUHQr2BUG8AlK8by2ja
Rj4J+mN5huMcljzVqGE3bRmRbG2yATNC60PJVixmR3GuqPEEyxW+gwetV23r1tY08LLLbHKPZHHE
0XDyJJj8J9g4AX66Hn59Bjo/AfVPTczmjc4Vhv1bYN2XVkO1bhzsWx9eJyWgZs9wLVW1LnaZPIZE
5HbgDqC8iOzAmgcCHABeiumb56BVjVZ8e+m38Xp753IlNdVa7KekwOg3Yd4GazY4eLD12TrtNKhy
2Om6nUsucll0zuG8iaA3ESxco0fDzTfbLInNm8c7moJRhTVj4Oe/ws5fbVmdk6DLk1CtU3xjc4lD
02H3Ctg+H3YsCG7zYediKFMVKjaFSs2gYjO7r9TMlpWpnvjJenoabJ0Wbva3ZSrBQH2mfP1wQlW3
f56b00axieDDqnp7QfcTLV72uGSTlgbff2/J1qhRsHatDYgxYIAlW2ecAdW8tbwrArwPVhR4IVeI
duyAG26A11+3ZoH/+x+0aBHvqKIj7YAN4T77XjjwGyBw5GXQ4QGoUD/e0bnCkrrXBm7YsSBjMrVz
oY2Sl1elKkckXM3CiVjoeek49VXcs9Zqp9Z9Duu/DL7zgRJloNbxllDVGwRHtC1QkhjFBKsENslw
M1V9QEQaAfVUNS6T0XvZ45JZejpMnmzJVkoKrFplA/7272/J1pAhULNmvKN0Ln88wYoCL+QKyfff
w0UXwYoVcPvtcM89RXPijf1bYc6D1lww/SCUrABt/gZH3wKlKsY7Ohct+zaHa6FCSdT2+cGgDdn8
npavD1VaQ5Wj7f6Io6HyUXBwB+xaZrfdy2H3svDz1J05x1G2ZlDr1TRc+/X7fWMoGaUhwNIOwObv
w7VU22ZlXF+pebiWqnafqCZ+UUywnsfmvTpJVY8WkWrAF6p6bIGDzAcve1xRoQrTpoWTrWXLrI9W
376WbA0dCrVrxztK53LPE6wo8EKukFxzDXz+uU0t36tXvKOJvZ2LYebfYNVoewQeP4IAACAASURB
VF6+AXR8CJpdZP1QXOLLrlnfjgU2gmRWpKQlTaEk6vdkqjWUzmNHBVU4sDWceO1aFk6+di+DXcsh
fX/O+yhfP1PS1TT8vELDnCfL3rU0nFBt+NrmfwspWcGawtYfZANUVI7dROBRTLBmqGoXEflZVTsH
y35R1SwntI81L3tcUaRqAwKnpMDIkfDrrzZAxgknhJOt+t6owyU4T7CiwAu5GFq4EPbvhw4dbDbD
tLTi1xt243cw42bY+pM9r9bF+mfV6RPXsFyEyGZ9kU37cmrWV6pykEAFNVGhZKrSkYU3+bSmw74N
EbVfEYnYrmWwZyVoWvavl5JQoXHG2q/yDeC3ny2pCvUpDDmiXbjZX63eULJsLD9dOMzoJVhTgJ7A
tCDRqoXVYHUucJD54GWPK+pUYc6ccM3WvHnWWrhXL0u2zjoLGjWKd5TOHcoTrCjwQi4GVOGFF+CW
W6BzZ5g0KfE76seSpsPyd+CX22HPalvWcAh0egyqtIxvbMXJwZ3WtG37/HBNVH6a9VVpbcsT/Tud
nmrft+xqv/auJdvPDVC6qg1KEaqlqtCwkALPKIoJ1gXAuUAX4A1gGHCnqo4s6L7zw8seV9zMm2eD
Y6SkwKyglXH37pZsnX02NGsW3/icC/EEKwq8kIuyDRvg8svh009taKH/+z9vDxCSugcWPAXzHrHm
VlIKWl4L7e6GstXjHV3Rsn+r1cRsnQG/zbDHOxaRZUKRXbO+Kq2gzBGFHnqhSdtvTSAjk689q6w/
Vf3BUKNbzk0IC0m0CrpgX62BfthQ7eNVdX409psfXva44mzRonCyNWOGLTvmGEu2hg2Do2LX6ti5
w/IEKwq8kIui+fPhxBNh50547DG49lqf+j0re9fBrLthyauAWk1B+7uhxbWF17ysKNm7wZKoUDK1
dUZQK5VJidLWzO2IdtYnKpRIVWruxz2BRTnBqgY0ImL+R1WdkYvXDQL+BZQEXlHVR7LZ7ljgR+A8
VU3JaZ9e9jhnli61ZGvUKJgyxZZ17BhOtlq3jm98rvjxBCsKvJCLotRUuOoquOkmaNs23tEkvt9+
gRm3wIbx9rzSUdD5MWh4ZuI3P4sHVathiUykfvs5aOqWScnyULUjVO9it2pdbLhwT6SSThSbCD4A
XAosIVyVqap60mFeVxJYBJwMrAamAeer6rwstvsS2Ae85gmWc3m3cqVNlZmSYoMPg51OhJKttgWb
9cG5XPEEKwq8kCugKVPgttvs0pNPepF3qrB2LPx8q/UJAptDqMtTUCMqF+2Tk6bbKHYZkqkZsH/L
oduWrgLVOlsSVb2LPa7SKiGat7mCi2KCtRBor6oH8vi6HsC9qjoweH47gKo+nGm7G4GDwLHAJ55g
OVcwa9bABx9YsvXdd1ZctmwZTrY6dfJky8VGtModPwtxeZeaCg89BA88AA0awOrVnmDlhwg0OBXq
DYDFL8Pse2DTRBh3LDS9yIZ2r1jEh1lKT4UdCzM18/s56zmgytaISKSC+0pH+tD3LjfmAFWBjXl8
XQNgVcTz1UD3yA1EpAEwFOiLJVjOuQJq0ACuu85u69fDhx9asvXoo/CPf8CRR4aTra5dPdlyiad4
12DVrq3T27eH3r3tdtxxULlyvMNKbIsXw4UXWu3VhRfCM8/AEUV4MIDCdGAbzP0HLPwXpB+wiWJb
32qTFUdx8ta4STsA2+eGk6mtM2DbL5C299Bty9cPJ1GhmqkKjbwULWaiWIPVFfgIS7R+n0BMVc84
zOuGAYNU9c/B84uA7qp6XcQ2I4EnVXWyiLxONjVYIjIcGA7QuHHjY1asWFHQj+VcsbN5syVbo0bB
V1/Z9d7GjcPJVvfu3v3bFYw3EYyCrg0b6vQ6dWxmvPR0+68cOtQukwBs2wZVq8Y3yERz7rnwxRc2
FPu558Y7mqJp1zKYOQJWvm/Py9WFjg9Cs0uhRMm4hpYroUlydyyyflKhhGr7HEg/eOj2FZtmrJWq
1hnK1y30sF3iiWKCNRd4EZgNpIeWq+qEw7zusE0ERWQZNjIhQE1gDzBcVT/Mbr/eRNC5gvvtNxgz
xk7ZvvgCDhywmq+zz7Zbr15QMgmKTJdYPMGKgt8LuZ07YfJk61VZubLN4aQKtWtDpUrhGq7eveHo
o4vf5ZFNm2DfPpsVcONG+xVrGJ95cYqVTT/YRMVbgqGVqnawiYrr9o9vXGD/H/s2wK4lsHOx3XYt
Dj8+uC2LF4nN/RXZzK9aJx+m3mUrignWNFXNc/M9ESmFDXLRD1iDDXLxR1Wdm832r+N9sJwrdNu3
wyefWLL12Wewfz/UqWMTGg8bBiecAKW8U4zLBU+woiDHQu7AAXjuOZsod9Ikm+MJYMQIePhhWz9t
mjX+LVu28IIubGPHwp/+BO3aWX28K1yqsOI9mPk32LPSltU/FTo/bhPfxvS9022UvqwSqF2LbT6v
7JSqbPNLVe0QkUx1hNLeBNflXhQTrKewpoFjyNhEMDfDtJ8CPI0N0/6aqj4kIlcFr38h07av4wmW
c3G1c6eduqSk2P2ePdZNfOhQS7b69oXSpeMdpUtUnmBFQa4LOVVYssQSrQ4doEsX+PFH6NnTkqtj
jw3XcJ1wQvL349q61Wb/S0mBF1+E9u3h7bft3sVH6l7rmzX3HzYAhJSEo66E9vdCuVr53296miVu
WdVE7VoCafuyf22Z6jaPVOWj7FbpqPDjsrW8v5QrsCgmWN9ksfiww7THiidYzhWOPXvg88/tdObj
j2HXLqhWDc4805oR9u9ftK+Ru7zzBCsKClTI7dgB33wTruGaPt16W06YYEnWrFl2690bmjSJ/8lm
aiqkpdkvyfr1MHKkjYMaeXvmGTj5ZPsVOiPo+33LLfDgg1CuXHzjd2bfRph1Dyx5yWqYSleBtn+H
Vn+xQTGykn4Qdi0/tAZq52LYvSzrflEh5WqHE6fIBKpSc2/a52IumhMNJxJPsJwrfPv2WV+tlBT4
6CM7jatSxU53hg2DgQP9VMd5ghUVUS3k9uyxJoPdu9t/6F13WWIC1uuyd2/rcTl8eHQvl6jar8Sa
NfZL0bCh9Zm6996MydOGDdbk8corbVCPzp2tjrx+fYuvQQObJLhHD9iyBebMgebNva9Voto21+bP
Wve5Pa/YFDo8AKWPODSR2r0CNC37fZVvEE6aMtRGNbcEzrk4iWaCJSKnAm2B30+hVPX+aOw7rzzB
ci6+9u+H8ePtWvNHH9mAGZUqwWmnWbI1eDBUqBDvKF08eIIVBTEt5NLSLEmZNMkGz5g40eqmt2yx
QTL+8x8bpbBXL0vKKlY8dB+pqVbbtGYNrF0L9erZUPJ798Kpp4aTp91BX5g777S5qTZvthn5QolT
6Hb66dac8eBB+zWpWbP4DdhR1KwdBz/fYsOfZ0ugYuNMNVCh+yOhlJciLjFFsYngC0AFbK6qV4Bh
wFRVvbyg+84PT7CcSxwHD1qDpJQUm9x482ZLrk45xZKtU0+15MsVD55gRUGhF3KbN4cn5B02DEaP
thqokiWtX9ef/2w1XAAtWsDSpTZ8fMgll8Drr9trTjoJatXKmEB16QKtWhXe53GJIT0Vlr4GS16D
MtUOrYmq1AxKeiNzl3yimGDNUtUOEfeVgM9U9fgohJlnnmA5l5hSU+16eEqKzbW1YYM1Sho0yE7b
TjvNp/4s6jzBioK4F3LbttlgGaF+XLVrW301wO23WxO+yASqSROo7v1enHPFQxQTrKmq2k1EJgNn
AVuAuap6VIGDzIe4lz3OucNKS4MffggnW2vWQJky1lV92DDru+WnZEWPJ1hR4IWcc84lrigmWHcB
/8Hms3oWUOBlVb27oPvODy97nEsu6ekwZYolWykpsHKlzavVr58lW2eeGW6g5JJbtMqdmHbAEZFB
IrJQRBaLyIgs1ouI/DtYP0tEugTLG4nINyIyT0TmisgNEa/pJCKTRWSmiEwXkW4R624P9rVQRAbG
8rM555xLfCJSAhivqttUdRTQBGgdr+TKOZd8SpSwMcCefBKWL4epU+Hmm+HXX+GKK6BuXRvy/YUX
wtOmuuItZgmWiJTErhQOBtoA54tIm0ybDQZaBLfhwPPB8lTgFlVtAxwHXBvx2seA+1S1E3B38Jxg
/XnYKFGDgOeCGJxzzhVTqpqOlUWh5/tVdXscQ3LOJTERGy/s0Udh8WL4+WcYMQJWrYKrr7bxyPr0
sZlv1q6Nd7QuXmJZg9UNWKyqS1X1APAuMCTTNkOAN9VMBqqKSD1VXaeqMwBUdScwH2gQvEaB0NjR
RwBrI/b1blB4LgMWBzE455wr3saLyNki8Z6Q0DlXlIhAp042K8+CBTB7Ntx9t41pdv314Vl6nn7a
EjBXfMQywWoARH6dVhNOknK9jYg0BToDU4JFNwKPi8gq4Ang9jy8HyIyPGhaOH3Tpk15+DjOOeeS
1JXASGC/iOwQkZ0isiPeQTnnig4RaNfOpiGdMwfmzbOZc3btsmlGGze2mXaeeAKWLYt3tC7WEnoS
pGAo3VHAjaoaKgyvBm5S1UbATcCredmnqr6kql1VtWutWrWiG7BzzrmEo6qVVbWEqpZR1SrBc59F
2zkXM0cfbdOTzpwJixbBww/byIR//SsceSR07WrLfv013pG6WIhlgrUGaBTxvGGwLFfbiEhpLLl6
W1VHR2xzCRB6PpJwM8DcvJ9zzrliSESqiUg3ETkhdIt3TM654qFFC+unNW2aTXH6xBM2E88dd0DL
ltCxo9V2zZ8f70hdtMQywZoGtBCRZiJSBhuAYkymbcYAFwejCR4HbFfVdUE7+VeB+ar6VKbXrAVO
DB6fBPwasa/zRKSsiDTDBs6YGv2P5ZxzLpmIyJ+B74BxwH3B/b3xjMk5Vzw1awa33GLToK5caf2z
qlSBe+6BNm2gbVt7PHs2FOOZlJJezBIsVU0FrsMKsvnA+6o6V0SuEpGrgs3GAkuxASleBq4JlvcC
LgJOCoZjnykipwTrrgCeFJFfgH9gow+iqnOB94F5wOfAtaqaFqvP55xzLmncABwLrFDVvli/3m3x
Dck5V9w1agQ33AATJ8Lq1TbyYO3aNmhGhw7QujX8/e82UqEnW8nFJxr2yR6dcy4hRXGi4WmqeqyI
zAS6q+p+EZmrqm2jEGaeednjnMvJhg3w4Yc2qfE331jfrSOPtEmNhw2z/ls+JmpsJMVEw84551wC
WC0iVYEPgS9F5CNgRZxjcs65LNWpA1deCV9+CevXwyuvWF+tp56Cbt2gaVNrZvjDD5CeHu9oXVY8
wXLOOVekqepQVd2mqvcCd2F9fM/MzWtFZJCILBSRxSIyIov1F4jILBGZLSI/iEjH6EbvnCvOataE
yy+Hzz6DjRvh9det+eAzz0CvXtbM8C9/ge++s5oulxg8wXLOOVckiUg5EblRRJ4RkStFpJSqTlDV
Map6IBevLwk8CwwG2gDni0ibTJstA05U1fbAA8BL0f4czjkHUK0aXHIJfPwxbNoEb78N3bvDyy/D
iSfaxMbXXANffw2pqfGOtnjzBMs551xR9QbQFZiNJUlP5vH13YDFqro0SMjeBYZEbqCqP6jqb8HT
ydgUIc45F1NVqsAf/wijR1uy9d57cMIJ8MYb0K8f1KsHw4fDF1/AwYPxjrb48QTLOedcUdVGVS9U
1ReBYcDxeXx9A2BVxPPVwbLsXA58lsf3cM65AqlUCc45B95/35KtUaPg5JPhf/+DgQOtT9dll8Gn
n8L+/fGOtnjwBMs551xR9ft122DqkJgRkb5YgvW3bNYPF5HpIjJ906ZNsQzFOVeMVagAZ50F77xj
ydaHH8Jpp1lN12mn2TDwF10EH30Ee/fGO9qiyxMs55xzRVVHEdkR3HYCHUKPRWRHLl6/BmgU8bxh
sCwDEekAvAIMUdUtWe1IVV9S1a6q2rVWrVr5+CjOOZc35crBkCHw5ps2QMann8LZZ9v9mWdasnX+
+VbjtWdPvKMtWjzBcs45VySpaklVrRLcKqtqqYjHVXKxi2lACxFpJiJlgPOAMZEbiEhjYDRwkaou
iv6ncM65gitbFk45BV57zebZGjfOkquvvrK5tWrVgj/8wfpy7doV72iTnydYzjnnXBaCZoXXAeOA
+cD7qjpXRK4SkauCze4GagDPichMEfEZhJ1zCa10aRgwAF56Cdats1EHL70UJk6E886zZGvoUBul
cPv2eEebnERV4x1D3HTt2lWnT/ey0DnnEpGI/KSqXeMdR7R52eOcS0RpaTZ5cUqKNRtcswbKlLEB
M4YNgzPOgOrV4x1lbEWr3PEaLOecc84554q5kiXh+OPhX/+ClSst2br+epgzx0YhrFMHBg2CV16B
zZvjHW1i8wTLOeecc84597sSJaBHD3jiCVi2DKZNg5tvhl9/hSuugLp1oX9/eOEF69PlMvIEyznn
nHPOOZclEejaFR59FBYvhp9/hhEjYNUquPpqm9S4Tx945hlYuzbe0SYGT7Ccc84555xzhyUCnTrB
gw/CggUwezbcfbc1Gbz+emjQAHr1gn/+05oZFleeYDnnnHPOOefyRATatYN777V+WvPmwQMPwO7d
1pywSRPo3h0efxyWLo13tIXLEyznnHPOOedcgRx9NNx5J8ycCYsWwcMPQ3o63HYbNG8OxxxjyxYV
gxkDPcFyzjnnnHPORU2LFtZPa9o0q7164gkb8v2OO6BVK+jQAe6/32q9iiJPsJxzzjnnnHMx0awZ
3HIL/Pij9ct6+mmoUsWaFrZtC23aWD+uWbOgqEzP6wmWc84555xzLuYaNYIbboBJk2D1aht5sE4d
eOgh6NjRarfuuANmzEjuZMsTLOecc84551yhql8frr0WvvkG1q2DF1+Epk3hscesv1bz5tZ/a+rU
5Eu2PMFyzjnnnHPOxU3t2jB8OHzxhU1c/Oqr0Lq1NSfs3t1GJLz5Zvj+exs4I9F5guWcc84555xL
CDVqwJ/+BGPHWrL1xhs299azz0Lv3tbM8PrrYcIESEuLd7RZ8wTLOeecc845l3CqVYOLL4YxY2DT
Jnj7bavReuUV6NPHmhlefTWMHw+pqfGONswTLOecc84551xCq1IF/vhHGD3akq3337ck6803oX9/
qFsXrrgCxo2DgwfjG6snWM4555xzzrmkUakS/OEP8N57lmyNHg0DB9rzQYNsZMJLL4VPPoH9+ws/
Pk+wnHPOOeecc0mpQgUYOtSaD27caM0JTz8dPvzQ7mvXhgsvtOd79xZOTDFNsERkkIgsFJHFIjIi
i/UiIv8O1s8SkS6He62IVBeRL0Xk1+C+WsS624PtF4rIwFh+Nuecc0VfQcox55xzhatcOUuq3njD
kq2xY2HYMPjsM0vCatWC886DlBTYvTt2ccQswRKRksCzwGCgDXC+iLTJtNlgoEVwGw48n4vXjgDG
q2oLYHzwnGD9eUBbYBDwXLAf55xzLs8KUo4555yLrzJlYPBgG/J9/Xr48kuryfr6a2teWKuWJV/v
vgs7d0b3vUtFd3cZdAMWq+pSABF5FxgCzIvYZgjwpqoqMFlEqopIPaBpDq8dAvQJXv8G8C3wt2D5
u6q6H1gmIouDGH7MNsIdC+GrPhmXNT4HWl4DqXvg21MOfc2Rl9pt32aYNOzQ9S2uhibnwu5V8ONF
h65vfQs0PN3ee+qVh65vdyfU7Q+/zYSfbjx0fcd/QK2esOkH+OWOQ9cf8zRU6wTrv4I5Dx66vtuL
UKUVrP4YFjx56Poeb0HFRrDiPfg1i/OE3ilQriYsfd1umfUZC6UqwKLnYOX7h67v/63dz38C1nyS
cV3J8tD3M3s8+wHYMD7j+rI14PhR9njm7bA505+2QkPo+V97/NONdgwjVW4J3V+yx1OGw85FGddX
62THD+CHC2HP6ozra/aATg/b44lnw/4tGdfX6Qft77LH3wyGtEz10A1Og6NvtceZv3fg3z3/7tlj
/+4duj5+8l2Oqeq6wg/XOedcVkqXtoEw+ve34d4nTrRarFGj7Fa2rPXdihbRGE2NLCLDgEGq+ufg
+UVAd1W9LmKbT4BHVHVS8Hw8liw1ze61IrJNVasGywX4TVWrisgzwGRV/W+w7lXgM1VNyRTXcOwq
I0A7YE5MDkDRUBPYHO8gEpwfo5z58cmZH5+ctVLVyvF684KUY6o6PdO+IsueVsDCQvgIIcn+PUvm
+JM5dkju+JM5dkju+JM59qiUO7GswYo5VVURyVOGqKovAS8BiMh0Ve0ak+CKAD8+h+fHKGd+fHLm
xydnIjL98Fslh8iyp7Al+/csmeNP5tghueNP5tghueNP9tijsZ9YDnKxBmgU8bxhsCw32+T02g1B
M0KC+415eD/nnHMutwpSjjnnnCumYplgTQNaiEgzESmDDUAxJtM2Y4CLg1GYjgO2B+3Wc3rtGOCS
4PElwEcRy88TkbIi0gzrcDw1Vh/OOedckVeQcsw551wxFbMmgqqaKiLXAeOAksBrqjpXRK4K1r8A
jAVOARYDe4DLcnptsOtHgPdF5HJgBXBO8Jq5IvI+1vk4FbhWVdMOE2ZcmmskET8+h+fHKGd+fHLm
xydncT0+BSnHEkyyf8+SOf5kjh2SO/5kjh2SO/5iH3vMBrlwzjnnnHPOueImphMNO+ecc84551xx
4gmWc84555xzzkVJsUmwROQ1EdkoInMillUXkS9F5Nfgvlo8Y4ynbI7P4yKyQERmicgHIlI1njHG
U1bHJ2LdLSKiIlIzHrElguyOj4hcH3yH5orIY/GKL96y+f/qJCKTRWSmiEwXkW7xjDGeRKSRiHwj
IvOC78oNwXL/jc6l7I5hpm36iMj24Ds3U0Tujkes2RGR5SIyO/Q/kcV6EZF/i8jioFzqEo84MxOR
VhHHdKaI7BCRGzNtk1DHviDnRCIySEQWBn+HEYUX9e/vn+/zlcN9x2Itm9jvFZE1Ed+NLGZ7j/9x
D2LIKv73ImJfLiIzs3ltvI99gcqZPB9/VS0WN+AEoAswJ2LZY8CI4PEI4NF4x5lgx2cAUCp4/Kgf
n4zHJ1jeCOsAvwKoGe84E+n4AH2Br4CywfPa8Y4zwY7PF8Dg4PEpwLfxjjOOx6ce0CV4XBlYBLTx
3+iCH8NM2/QBPol3rDl8huU5/Y4G/yefAQIcB0yJd8xZxFgSWA80SeRjn99zouDzLQGOBMoAv2T+
nsUp9lydrxzuOxan2O8Fbs3F9yquxz27+DOtfxK4O0GPfb7Lmfwc/2JTg6Wq3wFbMy0eArwRPH4D
OLNQg0ogWR0fVf1CVVODp5Ox+V2KpWy+PwD/BG4DivVoMdkcn6uBR1R1f7DNxkNeWExkc3wUqBI8
PgJYW6hBJRBVXaeqM4LHO4H5QAP8NzrXcjiGRckQ4E01k4GqEsyLmUD6AUtUdUW8A8lJAc6JugGL
VXWpqh4A3g1eV2iS+Xwlh3OJw4n7cYec4xcRwUb2/l+hBpVLBSxn8nz8i02ClY06Gp6vZD1QJ57B
JLg/YVcOXUBEhgBrVPWXeMeSoFoCx4vIFBGZICLHxjugBHMj8LiIrAKeAG6PczwJQUSaAp2BKfhv
dL5kOoaZ9QyaUX0mIm0LNbDDU+ArEflJRIZnsb4BsCri+WoSL4k8j+xPMBP52EPu/t+S4W+Q0/nK
4b5j8XJ98N14LZsmaslw3I8HNqjqr9msT5hjn49yJs/Hv7gnWL9TqwMs1rUQ2RGRv2Nzi70d71gS
hYhUAO4AEqoPQ4IpBVTHmvL8FZu/TuIbUkK5GrhJVRsBNwGvxjmeuBORSsAo4EZV3RG5zn+jcyen
YwjMABqragfgP8CHhR3fYfRW1U7AYOBaETkh3gHlhdhk1GcAI7NYnejHPoNk/X/LxflKIn7Hnsea
nnUC1mHN7JLR+eRce5UQx76wypninmBtCDUvCO6LbROm7IjIpcBpwAXBF8+Z5kAz4BcRWY41R5gh
InXjGlViWQ2MDprzTAXSgWI7EEgWLgFGB49HYk0Qii0RKY0Vem+raui4+G90HmRzDH+nqjtUdVfw
eCxQWhJocB5VXRPcbwQ+4ND/iTVYv9eQhsGyRDEYmKGqGzKvSPRjH8jN/1vC/g1yc76Si+9YoVPV
DaqapqrpwMvZxJSwxx1AREoBZwHvZbdNIhz7ApQzeT7+xT3BGoOd5BDcfxTHWBKOiAzC+hedoap7
4h1PIlHV2apaW1WbqmpTLJnooqrr4xxaIvkQG+gCEWmJdQzdHNeIEsta4MTg8UlAds0qirygZvNV
YL6qPhWxyn+jcymHYxi5Td1QLbLYqJUlgC2FF2X2RKSiiFQOPcYGLcg8ausY4GIxxwHbI5r2JIJs
r+An8rGPkJv/t2lACxFpFtTYnRe8Lq5yc76Sy+9YocvUj3AoWceUkMc9Qn9ggaquzmplIhz7ApYz
eT/+BR2VI1lu2I/eOuAgdjJ8OVADGI+d2HwFVI93nAl2fBZjbU5nBrcX4h1nIh2fTOuXU7xHEczq
+1MG+C/2IzoDOCnecSbY8ekN/ISNRjQFOCbeccbx+PTGmmXMivi9OcV/o6NyDK8Crgq2uQ6YG3zn
JgM94x13RPxHBnH9EsT492B5ZPwCPIuN5jUb6BrvuCPir4glTEdELEvYY5+XcyKgPjA24rWnYCOw
LQn9nRIg9izPVyJjz+47lgCxvxV8n2dhJ+31EvG4Zxd/sPz10Hc9YttEO/Z5KmcKevwleJFzzjnn
nHPOuQIq7k0EnXPOOeeccy5qPMFyzjnnnHPOuSjxBMs555xzzjnnosQTLOecc84555yLEk+wnHPO
Oeeccy5KPMFyLkpEpIaIzAxu60VkTcTzMpm2HReaEyKH/a0WkarZLH8v4vl5IvJKlD7DgyJyYzT2
5ZxzLva87HEu8ZSKdwDOFRWqugXoBCAi9wK7VPWJyG2Cie5EVQcW8O26i0grVV1YwP1ETcRnS493
LM45V1x42eNlj0s8XoPlXIyJyFEiMk9E3sYm2KsXeYVQRD4WkZ9EZK6IF19cFAAAIABJREFU/DmX
u30SuCOL98pwFVBEFohIwyCGOSLylogsEpE3RWSgiPwgIr+KSNeI3XQWkcnB8j9F7GuEiEwVkVki
cnd2ny3PB8g551zUednjXPx4DZZzhaM1cLGqTgewC26/u0RVt4pIBWC6iIxS1d8Os7//AdeJSLM8
xNAKOAdYAMwA9qlqTxE5GxgBDAu2aw/0BKoAM0TkU+AYoDHQHRBgrIj0BDZm/mzOOecShpc9zsWB
12A5VziW5FAI3CQivwA/Ag2B5rnYXyp2JXFEHmJYrKrzgmYU84DxwfLZQNOI7T5U1X2quhH4DjgW
GAAMBn7GCsijgJbB9jl9Nuecc/HjZY9zceA1WM4Vjt1ZLRSR/sAJwHGquldEJgHlcrnP14HbgEUR
y1LJeOEkcl/7Ix6nRzxPJ+NvgWZ6H8WuHD6oqq9miv8osvlszjnn4s7LHufiwGuwnIuvI4CtQQHX
FrtilyuqegD4N3BDxOLlWJMKRKQb0CgfMZ0pImVFpBZwPDAdGAdcLiIVg303FJGa+di3c865+POy
x7kY8gTLufj6FKggIvOAB4EpeXz9y0DkMLwjgToiMgcYDizNR0xzgAnAD8A9qrpBVccCKcBkEZkN
vA9Uyse+nXPOxZ+XPc7FkKhmrpF1zjnnnHPOOZcfXoPlnHPOOeecc1HiCZZzzjnnnHPORYknWM45
55xzzjkXJZ5gOeecc84551yUeILlnHPOOeecc1HiCZZzzjnnnHPORYknWM4555xzzjkXJZ5gOeec
c84551yUeILlnHPOOeecc1HiCZZzzjnnnHPORYknWM4555xzzjkXJZ5gOeecc84551yUeILlXBEj
IktEpEcutisnIioiDWMQwyARWRzxfL2I9A4e3yciz0T7PROdiPQJ/ja7RGRQlPed+XhH5TsgIpeL
yMdZbSsir4vIbdH6DM4551xR4QmWc1EmIteJyHQR2S8ir2exvp+ILBCRPSLyjYg0yWY/lwQn47tE
ZK+IpEc835bd+6tqc1X9MQqfY7KI7Aveb5OIvC8itQq6X1W9R1WvK+h+MotIAHYHMa8WkUdFRHL5
+gxJSgw8BDymqpVU9fMs3n998J3YJSLrROQVESmfnzeK1ndAVV9V1dOzWXepqj4GhXLsnHPOuaTh
CZZz0bcWeBB4LfMKEakJjAbuAqoD04H3stqJqr4RnIxXAk4HVoaeq2rVLPZdKoqfIeTPwfu3AmoD
j8TgPaKtVRBzf+Ay4MI4xxPSBJh7mG0GBLF3BXoCt8Y8Kuecc85FlSdYzkWZqo5W1Q+BLVmsPguY
q6ojVXUfcC/QUURa5+e9glqPW0VkLrAjYlmoOV4vEZkiIttEZK2I/DM/iZiqbgXGAJ0i3ru8iDwb
1LasFpHHRaR0LmJ+REReCR63FpFUEbks2McmEflrxLaVROSdIP45InJ7bmtKVHUBMDlTzFcGtYc7
RWSxiPwpWF4D+AA4MqKWsIaIlBSRu0RkqYhsFpG3ReSQ5DZi/9cGzfO2iMhoEakTLF8N1Ae+EJFd
uYh9DfAVhx7vp0VkVfA3/o+IlM0mjrx+B84UkeXB8X8oVOsnIleJyFfZvMe7InJnNseuSVCTWCVi
+57B+5c83Od3zjnnkpknWM4VrrbAL6EnqrobWBwsz69zgZOBGlmsOwhcF6w7HqsJ+3Ne3yBoGngm
FmvIfUAHoD1wDNAHyE+fnJJYjc1RwCnAQyJyZLDuQaAWVvtzKnBRHmJuC/TIFPM6YDBQBbgKeFZE
2qrqFmAosDSilnALVoM0AOgNNMSO5z+zeb9TsJrJoUADYDPwFoCqNgQ2Eq6hOlzsjYP3jYz9qSCG
9liNYktgxOGPRK6+A6djyVw34HzgglzsF4Bsjt0KYApwdsSmFwFvq2pabvftnHPOJSNPsJwrXJWA
7ZmW7QAqF2Cf/1TVtaq6N/MKVZ2qqtNUNU1VlwCvACfmYd8visgOLDkoD9wUse4C4B5V3ayqG7Bk
KNcJUCb3qOo+VZ0GLMASN4BzgAdVdXtw0v5cLvY1V0R2A3OAT7HPDICqjlHVZWq+AiZgyVN2rgJG
BMd3H5ZUnptNv64LgJdUdVaw7W1AfxGpm4uYQz4TkZ3ACmA5dkxDzT8vB25Q1W2quh1rrnne4XaY
y+/Aw8F+lwHPYElWQb1B0DxTRMpgf8u3orBf55xzLqF5guVc4dqF1Z5EOgLYKSKNI5pYHbYZWYRV
2a0QkTYi8pmIbAgSpbuBmnnY95WqWgXoAtTFmrkRJBh1sUQgZAVWc5NXaaq6OeL5HqCSiJQI3iPy
82X7WSO0xRLWi4FeQIXQChE5Q0SmishWsYFCTiKb4xF8xkbA2KB53TbgZ+x3M6vawvpEHA9V3YYl
z3k5JoNVtTJWe9UO66cX2ndpLHkMxfIh1i8uR7n8DkQe1xXB+xXUKOBYEWmA1UyuVtVZUdivc845
l9A8wXKucM0FOoaeiEhFoDnWLytyEIvDNiOLoDmsexmYATQPEqX7gVyNqpfhDVR/Bh4D/hM8V2A9
1nQvpDGwJq/7zuE904ENWLO4kEa5fa2qvgXMAm6H34/1SOABoPb/t3ff4XJVZfvHvzedSAlIiAES
AjGgdOSAEMpLEURBigUjP4RQRFCpQUC60usriCg9gLz0rqEjnYAJvUhPKAlJIEiREkKe3x9rTbIz
OXVmcuacM/fnuuaamV3WfvbMJGeeWWs9OxcKuYeZr0eUtRGk89k0InoXbguUJYQl4ym8Hnmu1iJU
8JpExJ2k4icn50UTgGmk97EUx6IR0VyiV649n4Hi6zogn0uHQm7mHD4mzc3akdSz6d4rMzNrCE6w
zGpM0jySFiDNLZpbqXx4qajADcAqkn6UtzkaeCoXZJgTFgY+iIiP85ykX1TR1gXA1yV9Nz+/Ajg6
F4NYEjgc+Ft14c7mauBwSYvmeUl7d3D/E4Ff50IMC5J6gSYB0yVtQ5o3VjIRWFJSMbn9K3CSpP4A
kpaU1GzZctLr8QtJq+T39iTgnoh4p4Mxl5wObCvpmxHxBakq5ZmSllDSX9Lm7WinPZ+BQ/JrPJA0
X6vZypataO61A7iUNN9rS+DyDrZpZmbWLTnBMqu9I4BPSQUIdsqPjwCIiMmkif/HA++Tigq0OY+m
CgcAe+Qhh3+m41+cZ8hzvM4mFXKANNTseVKv3JPAQ6Rerlo6gvQ6jQNuJSVcn7d354gYTSqFf2Du
dToIuIVU4XE7YGRh86dIlRLH5WF4i5PO5y7gnjw36mHScMnmjvV3UkJ3M6kH6GtUPieNiBgPXEn+
7AD753ZHk+bx3UYqDNKW9nwG/kE6/9GkXr6OJsrNvXYA/yQltg9GxIQOtmlmZtYtKY2CMTPr+iQd
AGwZEd9tc2PrEiQ9DJwTEbXu3TQzM+uS3INlZl1WHga3rqS58vC2/UjDLK0bkLQ+qZz8dfWOxczM
rLN0+IKjZmadaH7S3KNlgSmkeTwXtLqHdQmSrgS+C/y6uUsImJmZ9VQeImhmZmZmZlYjHiJoZmZm
ZmZWIw09RHCJJZaIgQMH1jsMMzMzq9CYMWPejYg+9Y7DzKykoROsgQMHMnr06HqHYWZmZhWSNK7e
MZiZFXmIoJmZmZmZWY3UJcGSdJGkSZKeLSxbXNKdkl7O94sV1v1O0iuSXpT03bxsfkm3SXpW0q8K
254nqdkLgZqZmZlZ7Ukali9o3qVIGivpoA5sv7GkkLTEHIonJP14TrRddpy6vh+S/i5pRL2OX2/1
6sEaAWxZtuxQ4O6IGAzcnZ8jaSVgKLBy3uccSXOTyv8+CKwG/Dxvuzowd0Q83gnnYGZmZlY1SRtJ
ulnS2/kL+LBmtpGkYySNl/SppHvz9QFba/eY4o/ZNYy3uSThKmD5Wh+rmWN3NAFaGzhnTsbUQf2A
W+odRHM6moxay+qSYEXE/aRr2hRtC1ySH18CbFdYfmVEfB4RrwOvAOsAXwC9gHkB5W2PBY6cg6Gb
mZmZ1dpCwLOki6m3dN24g4HhwD6kpGEScKekhTslwjZExKcRManecZRImg8gIiZHxCf1jqckIt6J
iM/rHYfNWV1pDlbfiJiQH78D9M2PlwbeLGz3Vl52JzAQGAWcJWkb4PGIGN/aQSTtKWm0pNGTJ0+u
ZfxmZmZmHRYRIyPisIi4Fphevl6SgP2BkyLiuoh4FtgFWBjYsbk2cy/Y0cDKucdnRs+YpEXzlIpJ
kj6SdJ+kpsK+i0q6LK//TNJrkvbP68bmza7JbY4tHa84JK3UeyZpqKRX83FuLPY8SZpH0v9Kel/S
FEmnSTpH0r0tnNNA4J/56eR8/BF53b2S/pLbmAw8VIq32Csj6UBJT0v6b+4xvEBS7+aO19Zr0cL2
/SXdlM/nE0n/ljS0sH5G75+kgfn50PwefCrpCUmrSVpF0sM5zgclLVf+2pYdt9UhgZIG5bjeyW0+
Lmnrwvp7gWWBU0ufl8K6ITm+T/Jr9hdJixTW95I0QtLHkiZKOqylOBpFV0qwZoh09eNWr4AcEdMi
YseIWBO4hvQfz+mSzpB0bU64mtvvvIhoioimPn1c1dXMzMy6vOWArwF3lBZExKfA/cCQFva5Cjgd
eJE0LK0fcFVO1v5B+rF6a2DN3M49kvrlfY8DVs3rVwR2A97O69bO97/IbZaeN2cg8FNge2CLfKzj
C+sPAoYBewDrkUYl/b9W2nsT+FF+vHI+/n6F9TuRRjVtCOzcQhvTSd8ZVyYlp+sAf2rlmK29Fs05
hzTCapN8jP2B/7SyPcDvgZNJr89/gCtyTIfn+BYAzmqjjbYsBNwKbA6sDlwHXC/pG3n9D0mdGH9g
5ucFSauSPnc35/1+CKwBXFRo+7Tc7o+AzfJ5bFRlvN1aVyrTPlFSv4iYkP+Bl7qZ3wb6F7Zbhtk/
2L8CLgXWBT4g/WO+h/RhMDMzM+vOvpbvJ5Ytn0hKlGYTEZ/mHo1pEfFOabmkTUlfkPvkJA3gSEk/
IM1pP4XUk/F4RDyW148rtDs55Wj8p9huC+YBhkXEB/nY5wG7FtbvB5wcEdfl9fsz+xz94jl9Kak0
xWRSRLxbtsnrETG8tYAi4o+Fp2MlHQzcJGmXiJit95BWXosWLAtcFxFPlWJqY3uAMyJiJICk00lz
tI6MiH/mZWcDZ7ejnRbleJ4qLDo+v+c/Bo6LiCmSvgQ+KntffwtcFRGnlxZI2ht4QtKSwCfA7sBu
EXF7Xr8rKVlrWF2pB+tmUnc3+f6mwvKhSlUDlwMGA6UPOUrVBrcmJVi9SL9MBLBgJ8VtZmZm1l2s
Rfq+NDkP6fo4J2KrAIPyNn8BfirpqTzk7n8qPNa4UnKVjQeWhDT0jpQ4zvhOl0cwPUblxrS1gaRN
lapVvyXpI+B6YD5mJrHlOvpanAkcIekRScdJWqsdcT9deFxKop8pW/YVSb3a0VazJH1F0imSns9D
Mj8GmoABbey6FrBT2WflobxuUL7NBzxS2iEiPi6Lv+HUq0z7FaQ3YsX8Ad8dOAnYXNLLwHfycyLi
OeBq4HngNuDXEfFlobmjgOPzrw63k7qFnwEu66zzMTMzM5uDSj0KfcuW9y2sa6+5SF/Y1yi7fYNc
KCwibiX1xJwGLAH8Q9LFFcT9RdnzYM5+9/xvayslLUsaHvkC8BNS8rBbXj1fc/t09LWIiAtJQzov
BlYAHpZ0TBtxF1+naGVZ6bWbzswCbyXztnGM00jnfCTwP6T3/DFaOO+CuYALmPWzsjqpw+PJNvZt
WHUZIhgRP2th1WYtbH88s47ZLa47oPD4M9IYXzMzM7Oe4nVSIrU58C8ASQuQflT+bSv7TQXmLlv2
OCkxmx4Rr7W0Yx5+dxlwmaRbgSsk7ZUr4H3RTLsdEhEfSHqHNIfrHphRzGNtWk8ap+b7So7fREoo
Dij9WF8s9NBKrK29Fs1t/xZwHnCepENIQyGPqSDelkwG+kpS7vWDlPi0ZgPg0sJwzAVIvU8vFbZp
6fOyckS80lyjkl4lfR7WBV7Ly75C6hF9td1n1MN0pSGCZmZmZg1H0kKS1pC0Bum72YD8fADMGDr3
R+AQST+UtArpmqIfA//XStNjgWUlfUvSEpLmB+4iDfG6SdL3JC0naT1Jv5e0YY7nD5K2kzRY0jdJ
hQ1eKyQUY4HNJH0tT9Wo1JnAwZK2l7QiqShHP1ovdDYur99KUh9JC3XgeC+TXt/983n/jFSEokXt
eC3Ktz9T0paSls/v55akUVi1dC+wOHCYUnXA3UlzqVrzErB9/iysCvyNVDyjaCywoaSlNbPa48nA
OpL+KmlNSV+XtLWkc2HGcMALgZMlba50bbaLKEvUJJ0o6e6Kz7ibcYJlZmZmVl9NwBP5tiCpqtwT
pIpuJacA/wv8GRhNSkS2iIiPWmn3OmAkcDep1+NnOVn7PqnX6HxSlcGrSRXySpe6+Zw0cugpUjK2
MPCDQrvDSVXy3sxxVuo0Us/QxaTL7gi4AfispR0i4m1S+fnjSUMd2138ISKeJvUmHUhKevYgVTJs
TVuvRbm5SBUAnyddUmgiM2sM1EREvADsDexJmr+1OXBCG7sdSCog9wCpmuCo/LjoKFJhuVdJn5fS
a7YRqSLkfaTX4URmLbhyEKl8/g35/llSZcqifsyc49fjaWbPYuNpamqK0aNH1zsMMzMzq5CkMRHR
1PaW1h1IegJ4MCL2qXcsZpXqSmXazczMzKxB5KIT3yX1jMxLurbWavnerNtygmVmZmZm9TCddEHg
U0lD654HvhcRHl5k3ZoTLDMzMzPrdBHxJqm6nVmP4iIXZmZmZmZmNeIEy8zMzMzMrEacYJmZmZmZ
mdWIEywzMzMzM7MaqSrBkjR321uZmZmZmZk1hmp7sF6WdKqklWoSjZmZmZmZWTdWbYK1OvAScIGk
UZL2lLRIDeIyMzMzMzPrdqpKsCLio4g4PyKGAIcARwMTJF0i6es1idDMzMzMzKybqHoOlqRtJN0A
/BE4HVgeuAUYWYP4zMzMzMzMuo2q52AB2wKnRsSaEXFGREyMiGuB2yppUNIBkp6T9KykKyQtIGlx
SXdKejnfL5a3XV/S05JGSxqcl/WWdIckV0g0MzMzM7NOVW0SsnNE7B4RD5cWSFofICL27WhjkpYG
9gWaImIVYG5gKHAocHdEDAbuzs8BhgPfB/YH9srLjgBOiIjplZ2SmZmZmQFI2k7S/ZImSfpU0jhJ
N0rassL2dss/mE+V9J8O7Ndb0jGSvlXJcVtpNwq36ZLelXSTpJUrbG9gjnP5ZtaNlTSi6qCty6s2
wTqrmWV/qrLNeYAFJc0D9ALGk3rJLsnrLwG2y4+/yNv0Ar6QNAjoHxH3VhmDmZmZWUOTtC9wA2nE
0u7AVsBxefWmFbS3FHAe8HDe/zsd2L03aa5/TROsbASwHrARcCQwBLhNUu8K2hpIinO2BAvYHji2
shCtO5mnkp0krUf68PWRdGBh1SKkXqeKRMTbkk4D3gA+Be6IiDsk9Y2ICXmzd4C++fGJwKV5258D
p5F6sFqLfU9gT4ABAwZUGqqZmZlZT3cQcGNE7F5Ydg9wfoVTMQaTvideEhEP1iLAGnk7Ikblxw9K
+hD4G7AlcGWtDhIRT9SqLevaKu3Bmg9YiJSgLVy4fQj8uNJg8tyqbYHlgKWAr0jaqbhNRAQQ+fGT
EbFuRGxC+qVgQmpGV0n6m6S+lImI8yKiKSKa+vTpU2moZmZmZj3d4qQftmdTnIohqY+kcyW9JOkT
SW9K+r889aO0zQjg3vz07jwkb0Rh/Z6SnpL0WR6md6GkxfO6gcDredPzC0P6hkn6k6SJkuYtxidp
YUkfSTqpgvN+PN/P8ku8pN9IekTSFEn/yZco2qqwfmPgn/npnYU4N87rx5ad87C8fl1Jl0v6UNJ4
SWdJWqDs2MtLGplf30mSTs+vWeTXx7qQinqwIuI+4D5JIyJiXA3j+Q7wekRMBpB0PamnbKKkfhEx
QVI/YFJxJ0ki9VwNJQ1RPJjURbsvcHgN4zMzMzNrFI8Bu0h6DbgpIl5qYbvFgamk72ITgX6kefIP
SfpGRHxGGho3hjS95NekJKb0fe+kvP1ZwG+BpUlDEVeRNIT0A/oPgetJo5duzsd9Ncf4G9Lwu6sL
Me0IfAU4t4LzHlhov2g50nDCV0k9cT8A/i7pexFxWz6nXwN/Jn0H/Vfe7/k2jncZcAXpHNcDjgHe
Jw01RNJ8wJ3A/MDepNdtD5rp1JB0TN5vuYgY2+aZ2hxR6RDBP0bE/sDZkqJ8fURsU2E8bwDrSupF
Gva3GTAa+C+wC3BSvr+pbL+dgZERMSXvOz3felUYh5mZmVmj2wu4FjgFOEXSe6Qv+hdHxB2ljSLi
RWCf0nNJcwMPkb7XfQ+4ISJelfRC3uT50pC83PvyW+D3EfGHQhsvAQ8CP4iIGyWVhte9VhjOBzBZ
0n3AL5k1wfolaarJ67RNee7/PMCq+XxHMTORK53n8MIOc5EKr61ASnpui4gPJZWSqRfK4mzN/0XE
0fnxXZK+DfyMnGABw0gjtb4dEY/l498KPElZLxvp+++X5NFeVh8VJVikTBvSnKeaiYhHJV1L+gVg
GvAEaTLkQsDVknYHxgE7lPbJCdUwYIu86AzSNbimkn69MDMzM7MOioiXJK0JrE/6nrUuqadoqKQj
I6JU8AJJe5MSskGknqOSFds4zOakKSuX5ySn5FHgI1LhiRvbaOMc4EpJgyPiZUlrA2uSeoTa47B8
KxkLbBoRXxQ3krQW8HtgbaAPoLzqxXYepyX/KHv+DLMWAFkXeKOUXEGaMiPpOmC14o45Sf0DVleV
DhEck+/vq204kDP4o8sWf07qzWpu+0+ATQrPHyD9+mBmZmZmVYiIL4H7861UCfA24GhJf46I9yXt
QxredwapN+p9UtI0Clig2YZnWjLfv9LC+q+2I8wbSHPFfkkqzLEXqQr1Le3YF+Ai4C+kWDcDjiIl
bN/Jc/+R1J/UY/U8qbfuDVJnwLHAN9t5nJZMKXv+OWk4YMls02OyiVUe1+aQSocIPkMrXY8RsVpL
68zMzMyse4qI8ZIuAM4kVQV8jDQH/u6yIXTLtbPJ9/L9FqTErKX1rcX0RY7pV5JOyfGcHhHT2hnD
hIgYnR8/mOf2H02a43RNXr4lsCiwQ0S8Vdoxj6Sa0yYAKzWzfLZibtY1VDpEcOuaRmFmZmZmXUqp
wFgzq76R70sVBnuRKkkX7drOw9xJmjc0ICLubGW7z/P9gi2sP5c0zO8aUu/P+e08fnNOBn4BHCXp
2tyLVUqkZgwblLQCafjkW4V924qzEqOAXSWtU5iDJeBHNTyG1VClQwRrWTnQzMzMzLqeZyXdRZrb
/jrpeqffJw3Buzoi3sjb3QYcIukwUo/WprTzsj25+MXJpMJpKwL3AZ8B/Unzsy6IiH+ShsO9R5r/
9TSpANrrEfFebudtSTeT5ojdEhFvVnrSEfGppBOAs0nzuK4D7iINCbxU0umkYXu/Jw0VLF726KW8
3W6SppASrhcj4qNK4yFVLjwEuF7S4cysIrhYXl8smX8UaYjjIH9fr5+KroMl6cF8/1Gu2T/LfW1D
NDMzM7M6OJzUE/MH4A7gKlIZ8UOBnxe2+wOpB+kA0nyo1YDvtvcgEXEYsCepoMXVpGrRh5CGDL6c
t5nOzKTiLlIJ9B+UNVUazldJafZy55MKqx0hSRHxHPD/gGVJ1QUPJr0O95edy3uksvGrk5LFfwFr
VRNIREwlDaF8GvgrcAnwJqkcPMAHhc3nIpWQF1Y3ynP3GlJTU1OMHj267Q3NzMysS5I0JiKa6h2H
1Z+ky0lD9pYvXgi5p5L0d+CbETGo3rHYrCqdgzWDpG8BG5CKXjwYEU+0sYuZmZmZWU1IWhdYA/gp
cGBPTK4kHQh8TOrRWxj4CbAV6Rpc1sVUlWDlcZ4/IV1ZG2CEpGuK10UwMzMzM5uDHiElH5eQronV
E31OGoI5gDQE8EVgj4i4sK5RWbOqGiIo6UVg9Yj4LD9fEHgyItq6qFyX4CGCZmZm3ZuHCJpZV1NR
kYuC8cx6Abn5gberbNPMzMzMzKxbqvRCw38izbn6AHhO0p35+eak8pxmZmZmZmYNp9I5WKVxdWNI
5ThL7q0qGjMzMzMzs26s0gsNX1LrQMzMzMzMakYaSLpAcjUuIWJY1bFYQ6m2iuBg4ERgJQpzsSJi
+SrjMjMzMzMz63aqLXJxMfAXYBqwCXAp8LdqgzIzMzMzM+uOqr3Q8IIRcbckRcQ44BhJY4CjKm1Q
Um/gAmAVUuGM3Ui1/q8CBgJjgR0i4n1J65MSvKnAzyLi5bz/1cCWPfFCc2ZmZmZWkbeBDTq4z8dz
IhDr2apNsD6XNBfwsqTfkD64C1XZ5pnAbRHxY0nzAb2Aw4C7I+IkSYcChwKHAMOB75MSr73y8yOA
E5xcmZmZmVnBNCLG1jsI6/mqHSK4HykB2hdYC/g5sEuljUlaFNgIuBAgIqZGxH+AbUlX5ybfb5cf
f5GP3wv4QtIgoH9E3FtpDGZmZmZmZpWqqgcrIv6VH34M7Fp9OCwHTAYulrQ6qQz8fkDfiJiQt3kH
6Jsfn0ia9/UpKbk7jdSDZWZmZmZm1ukqvdDwHyNif0m3kOZJzSIitqkinm8B+0TEo5LOJA0HLLYd
kiI/fhJYN8e0ETAhPdRVpN6t4RExsSz2PYE9AQYMGFBhmGZmZmZmZrOrtAfrsnx/Wq0Cyd4C3oqI
R/Pza0kJ1kRJ/SJigqR+wKTiTpJE6rkaCvwJOJg0L2tf4PDithFxHnAeQFNT02zJoZmZmZmZWaUq
vdDwmHx/Xy2DiYh3JL0pacWIeBHYDHg+33YBTsr3N5XtujMwMiKmSOoFTM+3XrWMz8zMzMzMrDWV
DhF8hmaGBgIijeJbrYqY9gEuzxUEXyPN7ZoLuFrS7sA4YIdCLL2AYcAWedEZwEhS6fYdq4jDzMzM
zMysQyodIrh1TaMoyPOqmppZtVkL239Cushx6fkDwKpzJjozMzOH2J7bAAAabUlEQVQzM7OWVVSm
PSLGlW550eD8eBIwpWbRmZmZmZnVxrJI0YHbsHoHbN1TVdfBkvQLUiGKc/OiZYAbqw3KzMzMzMys
O6r2QsO/BtYHPgSIiJeBJasNyszMzMzMrDuq6kLDwOcRMTVVSQdJ89B88QszMzMzs3p6G9igA9u/
O6cCsZ6t2gTrPkmHAQtK2hz4FXBL9WGZmZmZmdXUNCLG1jsI6/mqHSJ4KDAZeAb4Jak8+hHVBmVm
ZmZmZtYdVdWDFRHTgfPzDQBJ6wMPVRmXmZmZmZlZt1PphYbnJl3sd2ngtoh4VtLWwGHAgsCatQvR
zMzMzMyse6i0B+tCoD/wGHCWpPGkiwMfGhEu025mZmZmZg2p0gSrCVgtIqZLWgB4BxgUEe/VLjQz
MzMzM7PupdIiF1Pz/Csi4jPgNSdXZmZmZmbW6CrtwfqGpKfzYwGD8nMBERGr1SQ6MzMzMzOzbqTS
BOubNY3CzMzMzMysB6gowYqIcbUOxMzMzMzMrLur9kLDZmZmZmZmlnXJBEvS3JKekPT3/HxxSXdK
ejnfL5aXry/paUmjJQ3Oy3pLukNSlzw3MzMzMzPruSpKQiTdne9Prm04M+wHvFB4fihwd0QMBu7O
zwGGA98H9gf2ysuOAE4oVTk0MzMzMzPrLJUWuegnaQiwjaQrSdUDZ4iIxysNSNIywFbA8cCBefG2
wMb58SXAvcAhwBdAr3z7QtIgoH9E3Fvp8c3MzHqyTz6Bxx6D11+vdyRmZj1TpQnWUcCRwDLAGWXr
Ati0ipj+CBwMLFxY1jciJuTH7wB98+MTgUuBT4GfA6eRerBaJGlPYE+AAQMGVBGmmZlZ1zdlCjz0
EDzwQLqNGQNffFHvqMw6QcRYyjoBzDpDpVUErwWulXRkRBxbq2AkbQ1MiogxkjZu4dghKfLjJ4F1
874bARPSQ11F6t0aHhETy/Y/DzgPoKmpKWoVu5mZWVfw1lszk6kHHoBnn03L55sP1l4bDjwQNtwQ
VloJ5uoBs5UHDqx3BGZms6q0BwuAiDhW0jbARnnRvRHx9yqaXJ807PD7wALAIpL+BkyU1C8iJkjq
B0wq7iRJpJ6rocCfSD1gA4F9gcOriMfMzKzLioAXX5w1oRo7Nq1beGEYMgSGDk0J1dprw4IL1jVc
M7OGUFWCJelEYB3g8rxoP0lDIuKwStqLiN8Bv8ttbwwcFBE7SToV2AU4Kd/fVLbrzsDIiJgiqRcw
Pd96VRKHmZlZVzRtGjz55Mxk6sEHYfLktG7JJVMitf/+6X611WCeqv7Km5lZJar9r3crYI1SxT5J
lwBPABUlWK04Cbha0u7AOGCH0oqcUA0DtsiLzgBGAlOBHWsch5mZWaf55BN49NGZCdUjj8B//5vW
Lb88fP/7KZnacEMYPBjk2SZmZnVXi9+2egNT8uNFa9AeALkS4L358XvAZi1s9wmwSeH5A8CqtYrD
zMyss7RUkEKCVVeFYcNmJlRLLVXvaM3MrDnVJlgnAk9I+iepSstGzLxGlZmZmbWipYIU8847a0GK
9deH3r3rG6uZmbVPVfWDIuIKUhW/64HrgPUi4qpaBGZmZtaTRMC//w3nnw877wzLLQf9+8OOO8Jl
l6UeqWOPhXvvhQ8+SD1ZJ50EW23l5MqqI2mYpCjcpkp6VdIJkhaosM1jSlWdC8tC0jEVtDVC0lvt
2K50HgMLy8ZKGtHGNsdIquYSQs3FMrbsNf2PpDslbVBhe71znN9qZt29ku6tOmjrNFUPEczXp7q5
BrGYmZn1GNOmwRNPzFqQ4t130zoXpLA6+QnwFulao9uTCostDOxTo/bXy+3PKf/Ix5jQwW2OBo4H
7qlxPLcDx5A6LAbn44yUtFqka3B1RO+8/1vA42XrflVdmNbZ/N+5mZlZDbRVkGKrrVyQwuruyYh4
JT++U9JgYDdJ+5UKllUjIkZV20Yb7U8GJle7TQ29WzjnhyW9AjxIumzQSbU6SEQ8X6u2rHP0gEsM
mpmZdb4pU+CWW+Dgg2G99dIwvk03hWOOgUmTUkGKK69M86xefRVGjIDdd4cVVnByZV3G46RL2ixR
XChpOUmXS5os6XNJT0ravq3GyocISvq6pMskvS7pU0mvSfqLpMVa2H+IpH9J+iwPwdunbP1sw/+a
aWOWbQrDGA8vDOc7RtLwfG59yvZXjvPKts63GaWepwFlbQ6VdE9+PT+W9ISkXQrrBwKv56fnF+Ic
ltfPMkRQ0sZ5/TaSzpb0br79TdIsA4ol9ZF0haQPJb0v6eK8X+RLItkcUHUPVh5rOjgiLs4f0oUi
4vW29jMzm1NKPQmjR6d5LhtuCH371jsq6+7aW5BiyBBYrNmvj2ZdzkDgA+C90gJJ/YFHgUnAAaTe
oJ8C10naLiI6Mi1kKWA8MDwfYznSpXxGkobxFS0CXAWcDLxC6gU6S9JHETGioydWsB7wCDACODcv
ewv4BDgO2BU4pbD9FjnO3So41sB8/2rZ8kHAjfk400hF4S6QtGBE/JU0nPGHpJoGJzJz6k15O+XO
BP5OuizRirn9L0nXjC25nlRd+3ek1/VHwJ/KG8rJ3MXAJrmSt1Wh2gsNHw00kd7Ui4F5gb8B61cf
mplZ+7RU2rpo8OCZw7M23DAN2XIvgrWkVJCiNHfqgQdg7Ni0bqGFUhL105+mz9I668CCC9Y1XLP2
mlvSPMycg/UjYP+I+LKwzTGkytD/ky+TA3B7Trz+QAfm3UfE/cD9peeSHiJ9yX9A0poR8URh84WB
PSOi1HN0m6Slgd9LuiQiZimo0YEYRin9Z/92+RBGSVcBe0o6tdD+L4F/tzPJUH495wK+DvwFeBm4
qCyG4ws7zEW6DFE/YG/grxHxuaTSa/FaB4Za3h8RpV6+OyStCOwhaVhEhKQtgA2An0bE1Xm72yXd
TFkvGzCdlJxV9DrbrKrtwdoeWJPcJRoR4yUtXHVUZmatePPNWQsHlHoS5ptv1p6EddaB116bue2N
N8JF+c9ev36wwQYzE65VV4W5567fOVl9tVaQok+f9BnZb790v/rqLkhh3da/y56fExFnly3bktTD
9EFOHkpuB06VtEhEfNieg0maDzgI2BlYFihWLFwRKCZYX5IqUhddCVwALM2cKZ5xDqm3ZzPgLkn9
gB8AB7dz/x3zrWQKMCQi3i9ulOe6/YHUc/U1Zk7R+bzy0IFU0KPoGWB+oC/wDqnS95fADWXbXUs6
zxki4lLg0irjsazaPxFTc4YcAJK+UoOYzMxmKPYklG7jxqV1Cy+cehKGDk1ffNdee/aehD594Nvf
hoMOgunT4YUXZm3rmmvSdosumtoqJVxrrw3zz9+552qdpz0FKUoJuOdMWQ+yPSlR6QMcCPxK0qP5
y3XJkqSEaOcW2vgq0K4EizTcbR9ScvEw8BGwDGnYWnl5+PcjomzsARPz/RxJsCLiMUljgL2Au4A9
SEP4LmlnE7cCR5FGcK1DOt/rJa0VEZ8BSFoIuJM0JPFQ0rC/qaTeq0qGIRZNKXteSthKr20/Wn9d
bQ6pNsG6WtK5QG9JvyB9UC6oPiwza1TtKW19wAGVlbaeay5YeeV022uvtGzcuFkTrltvTcvnnz/1
gJUSriFDYJFFanuu1nlaGkYqpd7LYcPS+7zBBrD00vWO1myOebZURVDSPcDTpF6p6yIi/8TAe8AD
pLlQzRnfgeMNBS6NiONKC3LC0ZzFJM1blgyUZs++3YFjdtQ5wLl5OOIewDURUZ64tGRKRIzOjx+R
9AFpysw+wKl5+Xqk3rsNI+LB0o5lvYNzygRaf11tDqnqzY2I0yRtTvolY0XgqIi4syaRmVlDqHdp
62WXTbeddkrP33135pybBx6Ak0+GE05Iydnqq886j8uFM7quloaRuiCFWZLn/fwWuIl0naVSQnAb
KSl4LiI+rfIwvYDy3pNdW9h2btKcsGL1vqHAG1SfYE0FWpopeQVwGvB/pHlJf63iOJeQkqvfSvpz
RHxCeg2g8DrkKorblu1b6n2q5YzOUaTXdXvg6sLyn9TwGNaMaotcnBwRh5C6PsuXmZnNpr09CRtu
CEst1fnxLbEEbLddugF8/DGMGjUz3vPPh7POSuuKhTM22AAGDfJQsnpobRipC1KYtSwibpb0L2C4
pLNzQnUU8Bhwv6SzgbHAYsAqwPIR0ZFhbbcBu0h6hlTc4ofAkBa2/Qg4RdISpEIRPwO+AwyrtMBF
wfPAVpJuA94HxkfEeICI+FTSCFLFxGci4uFKD5KnzRxFquy3N3A6aWjkh8Cfc3G4rwBHAO8CixZ2
n0jqPRwq6Wngv8DrhUIjlcRzRy4scl5+XV8BfgysnjeZce0zSTuTinNsFhH3VXpMS6rtntwcKE+m
vtfMMjNrUG0VpBg+PCUn66+friPU1Sy0EHznO+kGMHXqrEMYXTij883JYaRmDegIUgGLvYD/jYg3
JDWRqgmeQJqv9R7wLO2fm1SyD6kiYamK3khS4vRYM9t+SOqxOpNUVnwisF9EdPSYzfkNcBZwC6kI
xO9J51dyDSnBOne2PTsoIv4h6RHgIEnnRMRkpWuInU4qLjGedI6LA0cX9psuaQ/Sa34X6Tv6rqTy
8tXYnlSW/WRSwYubgSNzux8UtpuL1NvlnwlrQJX8KCBpb1J38vLMWqN/YeChiNipNuHNWU1NTTF6
9Oi2NzSzdmlPQYpiEYme0JPQXOGMN99M61w4ozbaGkZaHLY5J4aRWtcmaUxENNU7Duu+JB0P7Acs
1d4Kid1Z7pncFVg8IqqtZGjNqDTBWpTUXXwiqSJKyUcdmBhYd06wzKrTnp6E0q2RehLKC2e88EJa
7sIZ7dPWMNLisEwXpDAnWFYpSWuSaghcAJwXEQfWOaSayxcQXhR4DpiPVIb/18CpEXFoK7taFSpK
sGZrRFqSQrnNiHijwnb6k2rw9yVd6Oy8iDhT0uKkq3sPJI0F3iEi3pe0PumiblOBn0XEy5J6kyby
bRkR05s5zAxOsMw6xj0JlSkvnPH44/Dlly6cUdJWQYrSa9NVh5FafTnBskpJGkv6znk78POI+Ki+
EdWepJ8AhwODSMMjXyfNtTq1re/JVrmqEixJPwDOAJYCJpHKUL4QEStX2F4/oF9EPJ4vWDwG2A4Y
RiqFeZKkQ4HFIuIQSdcD+5ISr+0jYrik04C/t+cK3E6wzFo3ZUr6wltKDlrqSahXQYruqrxwxqhR
8Gmu1TV48KzzuHpa4Yz2DiPdYAMXpLD2cYJlZl1NtQnWU8CmwF0RsaakTYCdImL3mgQn3QScnW8b
R8SEnITdGxErSrqKNFFvYI7jfOCEiPhpe9r/6lebYqutnGCZlfvyS3jqKXjuufS8VJCi9MXXPQm1
VV4448EHU3ILqXDGuuumYhvd3QcfwMMPexip1ZYTLDPraqpNsEZHRFNOtNbMFVCeiojV29y57bYH
AveTyoK+ERG983KRrkrdW9IapOsVfAr8nHQdgyMj4uVW2t0T2BNgnnnWWKt//yeqDdWsR1phhZ5X
kKK7KC+cMWZMSsK6uwUWmHUOmoeRWi04wTKzrqbaBOsu0hC+E4ElSMME146Ilq5x0N52FwLuA46P
iOsl/aeUYOX170fEYmX7bEQqRfkX4FjSBd2GR8TElo7jIYJmZmbdmxMsa1H6sf71Klu5hIhhVcdi
DWWuKvffFviEdO2A20gl239QTYOS5gWuAy6PiOvz4ol5aGBpntaksn1Euo7DsaRrChxMGi64bzWx
mJmZmZmZdURVCVZE/DcipkfEtHwhuLNJ5R8rkhOlC0mFMs4orLoZ2CU/3gW4qWzXnYGRuUR8L9KV
qafnx2ZmZmZmZp2iounEkhYh1dBfmpT83JmfHwQ8BVxeYTzrk+ZSPSPpybzsMOAk4GpJuwPjgB0K
sfQiVRncIi86g3Sl8KnAjhXGYWZmZmY9y9vABh3c5+M5EYj1bJXWa7oMeB94BNiDlAQJ2C4inmxt
x9ZExIO5neZs1sI+nwCbFJ4/AKxaaQxmZmZm1iNNI2JsvYOwnq/SBGv5iFgVQNIFwARgQER8VrPI
zMzMzMzMuplK52B9UXoQEV8Cbzm5MjMzMzOzRldpD9bqkj7MjwUsmJ8LiIhYpCbRmZmZmZmZdSMV
JVgRMXetAzEzMzMzM+vuqr0OlpmZmZmZmWVOsMzMzMzMzGrECZaZmZmZmVmNOMEyMzMzs0awLFJ0
4Das3gFb9+QEy8zMzMzMrEacYJmZmZmZmdVIpdfBMjMzMzPrTt4GNujA9u/OqUCsZ3OCZWZmZmaN
YBoRY+sdhPV8HiJoZmZmZmZWI06wzMzMzMzMasQJlpmZmZmZWY10mwRL0paSXpT0iqRD87KTJT0t
6dLCdjtJ2r9+kZqZmZmZWaPqFgmWpLmBPwPfA1YCfiZpdeBbEbEaMFXSqpIWBHbN25qZmZmZmXWq
7lJFcB3glYh4DUDSlcA2wLySBPQCvgAOAv4UEV/ULVIzMzMzM2tY3aIHC1gaeLPw/C2gLzASeAKY
AHwAfDsibmytIUl7ShotafTkyZPnVLxmZmZmZtaAukuC1ayIOCUi1oiI4cCxwFGS9pB0taQjWtjn
vIhoioimPn36dG7AZmZmZmbWo3WXBOttoH/h+TJ5GQCS1gQEvAj8JCJ2AAZJGtypUZqZmZmZWUPr
LgnWv4DBkpaTNB8wFLi5sP5Y4EhgXmDuvGw6aW6WmZmZmZlZp+gWRS4iYpqk3wC3kxKoiyLiOQBJ
2wGjI2J8fv6kpGeApyPiqboFbWZmZmZmDadbJFgAETGSVNSifPmNwI2F5weRqgmamZmZmZl1KkVE
vWOoG0kfkeZtWdewBPBuvYOwGfx+dC1+P7oOvxddy4oRsXC9g7AuSBoIvF5YMo6IgXWJxRpKt+nB
mkNejIimegdhiaTRfj+6Dr8fXYvfj67D70XXIml0vWOwLipiLKkImlmn6i5FLszMzMzMzLo8J1hm
ZmZmZmY10ugJ1nn1DsBm4feja/H70bX4/eg6/F50LX4/zKxLaegiF2ZmZmZmZrXU6D1YZmZmZmZm
NeMEy8zMzMzMrEYaJsGSdJGkSZKeLSxbXNKdkl7O94vVM8ZG0sL7caqkf0t6WtINknrXM8ZG0tz7
UVg3XFJIWqIesTWalt4LSfvkfx/PSTqlXvE1mhb+r1pD0ihJT0oaLWmdesbYKCT1l/RPSc/nfwf7
5eX+W25mXUrDJFjACGDLsmWHAndHxGDg7vzcOscIZn8/7gRWiYjVgJeA33V2UA1sBLO/H0jqD2wB
vNHZATWwEZS9F5I2AbYFVo+IlYHT6hBXoxrB7P82TgF+HxFrAEfl5zbnTQOGR8RKwLrAryWthP+W
m1kX0zAJVkTcD0wpW7wtcEl+fAmwXacG1cCaez8i4o6ImJafjgKW6fTAGlQL/z4A/hc4GHA1nE7S
wnuxN3BSRHyet5nU6YE1qBbejwAWyY8XBcZ3alANKiImRMTj+fFHwAvA0vhvuZl1MQ2TYLWgb0RM
yI/fAfrWMxibxW7ArfUOopFJ2hZ4OyKeqncsxgrAhpIelXSfpLXrHVCD2x84VdKbpN5E97Z3MkkD
gTWBR/HfcjPrYho9wZohUr16/0rfBUg6nDQU5PJ6x9KoJPUCDiMNf7L6mwdYnDQs6rfA1ZJU35Aa
2t7AARHRHzgAuLDO8TQUSQsB1wH7R8SHxXX+W25mXUGjJ1gTJfUDyPcedlNnkoYBWwP/L3yRtnoa
BCwHPCVpLGm45uOSvlbXqBrXW8D1kTwGTAdcdKR+dgGuz4+vAVzkopNImpeUXF0eEaX3wH/LzaxL
afQE62bSH0ry/U11jKXhSdqSNN9nm4j4pN7xNLKIeCYiloyIgRExkPQF/1sR8U6dQ2tUNwKbAEha
AZgPeLeuETW28cD/5MebAi/XMZaGkXttLwReiIgzCqv8t9zMuhQ1SieBpCuAjUm/+k4EjiZ9abka
GACMA3aIiOYm+luNtfB+/A6YH3gvbzYqIvaqS4ANprn3IyIuLKwfCzRFhL/Uz2Et/Nu4DLgIWAOY
ChwUEffUK8ZG0sL78SJwJmno5mfAryJiTL1ibBSSNgAeAJ4h9eJCGsr8KP5bbmZdSMMkWGZmZmZm
ZnNaow8RNDMzMzMzqxknWGZmZmZmZjXiBMvMzMzMzKxGnGCZmZmZmZnViBMsMzMzMzOzGnGCZWZz
jKSvSnoy396R9Hbh+Xxl294uaeE22ntLUu8Wll9VeD5U0gU1OofjJO1fi7bMzMys55un3gGYWc8V
Ee+Rrt2EpGOAjyPitOI2+eKhiojvVnm4b0taMSJerLKdmimc2/Q2NzYzM7MewT1YZtbpJH1d0vOS
LgeeA/oVe6ck3SJpjKTnJO3RzmZPJ110tPxYs/RASfq3pGVyDM9KukzSS5IulfRdSQ9LellSU6GZ
NSWNyst3K7R1qKTHJD0t6aiWzq3DL5CZmZl1W+7BMrN6+Qawc0SMBkidPTPsEhFTJPUCRku6LiLe
b6O9K4DfSFquAzGsCOwA/Bt4HPgsIoZI+hFwKPDjvN2qwBBgEeBxSf8A1gIGAN8GBIyUNASYVH5u
ZmZm1jjcg2Vm9fJqKwnIAZKeAh4BlgEGtaO9aaRerEM7EMMrEfF8HsL3PHB3Xv4MMLCw3Y0R8VlE
TALuB9YGtgC+BzxBSs6+DqyQt2/t3MzMzKwHcw+WmdXLf5tbKOk7wEbAuhHxqaQHgQXa2eYI4GDg
pcKyacz6Y1Kxrc8Lj6cXnk9n1v8fo+w4Qeq1Oi4iLiyL/+u0cG5mZmbW87kHy8y6mkWBKTm5WpnU
W9QuETEVOAvYr7B4LGk4H5LWAfpXENN2kuaX1AfYEBgN3A7sLukrue1lJC1RQdtmZmbWgzjBMrOu
5h9AL0nPA8cBj3Zw//OBYgn4a4C+kp4F9gReqyCmZ4H7gIeBoyNiYkSMBK4FRkl6BrgaWKiCts3M
zKwHUUT5yBczMzMzMzOrhHuwzMzMzMzMasQJlpmZmZmZWY04wTIzMzMzM6sRJ1hmZmZmZmY14gTL
zMzMzMysRpxgmZmZmZmZ1YgTLDMzMzMzsxr5//rYtlIYj8KRAAAAAElFTkSuQmCC
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-6">Question 6<a class="anchor-link" href="#Question-6">&#182;</a></h3><p>Using the visualization above that was produced from your default Q-Learning simulation, provide an analysis and make observations about the driving agent like in <strong>Question 3</strong>. Note that the simulation should have also produced the Q-table in a text file which can help you make observations about the agent's learning. Some additional things you could consider:</p>
<ul>
<li><em>Are there any observations that are similar between the basic driving agent and the default Q-Learning agent?</em></li>
<li><em>Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance?</em></li>
<li><em>Is the decaying function you implemented for $\epsilon$ (the exploration factor) accurately represented in the parameters panel?</em></li>
<li><em>As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase?</em></li>
<li><em>How does the safety and reliability rating compare to the initial driving agent?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>The basic driving agent and the Q-learning agent doesnt have any similarities.For the basic driving agent, the violations kept increasing and the rewards kepy decreasing with the number of trails.However, for Q-learning agent , the violations kept decreasing and the rewards kept increasing.</p>
<p>Driving agent took around 20 trails before testing.At the start , epsilon was 1 and the decay rate was 0.05.Epsilon tolerance is 0.05.It takes 20 trails to reach the epsilon tolerance value.Hence the number makes sense.</p>
<p>The decaying function is accurately represented in the parameters panel.</p>
<p>As the training trails increased, bad actions started decreasing.Also, the average award also started increasing.</p>
<p>The safety and reliability rating hasn't improved.It is still at F.</p>
<p>**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Improve-the-Q-Learning-Driving-Agent">Improve the Q-Learning Driving Agent<a class="anchor-link" href="#Improve-the-Q-Learning-Driving-Agent">&#182;</a></h2><p>The third step to creating an optimized Q-Learning agent is to perform the optimization! Now that the Q-Learning algorithm is implemented and the driving agent is successfully learning, it's necessary to tune settings and adjust learning paramaters so the driving agent learns both <strong>safety</strong> and <strong>efficiency</strong>. Typically this step will require a lot of trial and error, as some settings will invariably make the learning worse. One thing to keep in mind is the act of learning itself and the time that this takes: In theory, we could allow the agent to learn for an incredibly long amount of time; however, another goal of Q-Learning is to <em>transition from experimenting with unlearned behavior to acting on learned behavior</em>. For example, always allowing the agent to perform a random action during training (if $\epsilon = 1$ and never decays) will certainly make it <em>learn</em>, but never let it <em>act</em>. When improving on your Q-Learning implementation, consider the implications it creates and whether it is logistically sensible to make a particular adjustment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Improved-Q-Learning-Simulation-Results">Improved Q-Learning Simulation Results<a class="anchor-link" href="#Improved-Q-Learning-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file and the Q-table as a <code>.txt</code> file in <code>/logs/</code>.</li>
<li><code>'learning'</code> - Set this to <code>'True'</code> to tell the driving agent to use your Q-Learning implementation.</li>
<li><code>'optimized'</code> - Set this to <code>'True'</code> to tell the driving agent you are performing an optimized version of the Q-Learning implementation.</li>
</ul>
<p>Additional flags that can be adjusted as part of optimizing the Q-Learning agent:</p>
<ul>
<li><code>'n_test'</code> - Set this to some positive number (previously 10) to perform that many testing trials.</li>
<li><code>'alpha'</code> - Set this to a real number between 0 - 1 to adjust the learning rate of the Q-Learning algorithm.</li>
<li><code>'epsilon'</code> - Set this to a real number between 0 - 1 to adjust the starting exploration factor of the Q-Learning algorithm.</li>
<li><code>'tolerance'</code> - set this to some small value larger than 0 (default was 0.05) to set the epsilon threshold for testing.</li>
</ul>
<p>Furthermore, use a decaying function of your choice for $\epsilon$ (the exploration factor). Note that whichever function you use, it <strong>must decay to </strong><code>'tolerance'</code><strong> at a reasonable rate</strong>. The Q-Learning agent will not begin testing until this occurs. Some example decaying functions (for $t$, the number of trials):</p>
$$ \epsilon = a^t, \textrm{for } 0 < a < 1 \hspace{50px}\epsilon = \frac{1}{t^2}\hspace{50px}\epsilon = e^{-at}, \textrm{for } 0 < a < 1 \hspace{50px} \epsilon = \cos(at), \textrm{for } 0 < a < 1$$<p>You may also use a decaying function for $\alpha$ (the learning rate) if you so choose, however this is typically less common. If you do so, be sure that it adheres to the inequality $0 \leq \alpha \leq 1$.</p>
<p>If you have difficulty getting your implementation to work, try setting the <code>'verbose'</code> flag to <code>True</code> to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the improved Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_improved-learning&#39; file from the improved Q-Learning simulation</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_improved-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAA1gAAAI4CAYAAAB3HEhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz
AAALEgAACxIB0t1+/AAAIABJREFUeJzsnXd4VUXawH9vek8gDUILvYceelGwgIhg2QUXsCu6rvqJ
utjR1ZV1VXZti7prR5BFUBQbIEhbQFroICAthJDee+b7Y869uTe5qRBCmd/z5Mk9Z86ZctrMO28Z
UUphMBgMBoPBYDAYDIYzx62hK2AwGAwGg8FgMBgMFwtGwDIYDAaDwWAwGAyGs4QRsAwGg8FgMBgM
BoPhLGEELIPBYDAYDAaDwWA4SxgBy2AwGAwGg8FgMBjOEkbAMhgMBoPBYDAYDIazhBGwLmFE5JCI
DKzBcT4iokSkeT3U4WoROeiwfUpEhli/nxORN892mWeKiEwTkeVncP5PIvL7s1knQ80RkW4iskNE
skXk7nNQ3gYRmXwOyvG22hRV32UZDOcbpj8zNBQiMl9Enmroepwppg85uxgBqwERkftFZLOIFIjI
hy7SR4rIPhHJFZGVItKqknxusV6KbBHJE5FSh+30yspXSrVVSv3vLLRjg4jkW+UlicgCEQk/03yV
Us8qpe4/03zK49DB5lh1PiEifxMRqYeyZonIvx33KaUuV0p9fpbLKd+mbBE5dTbLuIh4HFiqlApQ
Sr1bPrHc85xhvXud67NCltCuROS6WpzjJLgppQqsNp2sn1oaDJVj+rOqqa/+zIaIuItIvIhsq68y
zjWWgJpr3YsEEfm3iPg2dL3OR0wfcv5hBKyG5STwAvB++QQRCQMWAU8DjYHNgMtBuVLqI+ulCACu
BY7ZtpVSIS7y9jiLbbBxp1V+RyACmFUPZZxtOlp1HgXcBtS7luEc0NHh3jdxdUA93f8LiVbA7mqO
sT3PocAm4IN6rtMtQCowtZ7LMRjqC9OfNSyjgACgm4h0r48CGqjvuNK6F32BQcAjDVAHoOH7zmrK
N33IeYYRsBoQpdQipdSXQIqL5OuB3Uqp/yql8oGZQA8R6VSXsqyZoEdEZDeQ6bDPZr4wWEQ2iki6
iJwUkdl1+ZgopVKBJUBPh7J9ReQtawbqhIj8XUQ8a1Bnu/ZHRDqJSLGI3GblkSQijzocGyAin1n1
3yUij4uDqUY1dd4HbChX58Yi8rF1jY6LyLMi4vJ9EZF/WXXKFJFNIjLA2j8eeBiwzchusvZvEJHJ
IuJn7W/nkFcza9a2kbU9QbQ5W7qIrBGRLjVpU7n6XS0iB0XkaRFJBP5VXd4iEisicSKSJSKfisgi
sUwgpJyJpJQzubHu9z+s63ZKRN4QEe9ydXnCuofxIvIHh7z8ReR169wMEflZRDxEZIWI3FWuXftF
ZHQlbb5BRPZYbVsuIu2t/euBgcC/rWvfsqprp5QqRg8EHa9Nle+KiFwjIr9a6a9Vc3sQkQ5Af+Ae
YKyINC6XfpN1n7KsfEeKyKtAP4d2vOriPjS23okkEflNRB4T0Vpa6x6usK51umjzqlEOZd4lIkes
Mg+LyE3VtcNwaWP6s2rrXN/92S3AQmCZ9duW1y0isrZcXR4XkQUO7anue23vO0QkXES+s+qcKiJf
iUhTh7zbi8h669vxvYi8Iw5WHCIy1OHebBWRwdVdOwClVDywnIr3orK6bxSRa6zfI61v40hr+xoR
2eBwL1ZZbUkSkY9EJNChDFfPmlP/CHhVVm/rW/uTdR0yRfdLwxzSKx1rOJz7loikATMqKeO860MM
RsA6n+kKxNk2lFI5wEFrf135PXAFela+PEXA/VbaUPTM4Z21LUC0KcV4dF1tPAfEAN2BPsAI4LHa
5g24o2ex2gFjgBdFpI2V9gIQjtZOXANMqUWdu6IH3Y51ngtkAG2AWHSbKsvzf+i2hQJfAf8VEU9r
sPEaYJuRjXU8SSmVi+68Jznsngj8oJRKEy2ovY3WroUCnwBf1mWgAEQDnkAL4IGq8hZtgvEV8A56
tvk7YFwtynoNaI6+Jh2BDjh3DK0AAaLQz9wcEQmw0l4HOqE//I2BpwAFfISDhlFE+gNBwI/lCxc9
e/shcB969vln4CsR8VBKDQJ+wZqhVkodq6ohVmd9M1oAt1Hpu2INNBYA09HPYxL6ma2KW4C1SqmF
wDEcngerI34XeBAIBkYCx5VS08u1Y7qLfOeg73lr9Ht/r9UWG8PQmoRQ4E3ANvhrBPwdGKmUCgSG
ALuqaYPBUBWmP6vIWevPRCTIqudc6+8PIuJuJS8GeovzZNLNwGfW7+q+19E49B3oceMcoCX62wIw
26qHoL9/K9HXfhbO3+1o4EvgScq+719a35wqsep/Jc73oqq6/4y+NwDDgcPob55t+2eHfJ4Hmjjk
82S54u3PWh37x2Ho5992Tb607hlUP9YYBmwHwoBXK8n/vOpDDBZKKfPXwH/oj+mH5fb9B5hVbt86
4NZq8hoFHHGx/xRws4t9QyrJZwYwz/rtgx7kNq/k2A1ADnp2R6Ff2iiH9Hjgcoft64B91u+rgYOu
6oT+EP3b+t3JyjvM4dgdwHjr90lguEPa/Y75lquvrT0ZVr0VekDuaaW3svZ7OpxzG/Cd9XsasLyS
vAXIRZvqObWh3PWabP0eC+xxSNsC/M76/QHwZLlzjwL9q2lTuvX3ssM1Lt+eSvNGd2K/lUvbCjzl
qv2OzwfgARQCzRzSLwP2OtQlA3BzSM9Ez0p6ogdGHV20z986rqW1/SbwWiX34EXgY4dtd7SgM6D8
9a/meU632pIKDK3ieMd35W5gVbmyT1dWHnqwcgyYZm0/B2x0SP8IeKmKek522Ha8D95ACdDGIf1B
4HuHe7jLIa2xdW4I0Mhq+3WAT2XtNn/mz9Ufpj87p/2ZlX6nVS839LcyBxjtkL4QeMz63R1IQ2td
avK9duo7XJQ9AEiwfncA8gDvcmXb2v0s8F65838Gfl9J3qeALOtPAd8DgVZadXW/Bthk/V5lXaNV
1vZGYEwlZU4E/lfZs0Y1/aOL/Ka5OH4HcBM1G2scqOYdOe/6kKrqeyn9GQ3W+Us2eobekWAgS0Ra
SpnTb3Yt8jxeWYKIdLHU/okikgk8g54xqSn3KKWCgN7omaAoK1+xto86HHsUaFaLvG2UKKWSHbZz
gQBLnd4E5/ZV2lYHugKBaJvlwYCftb8V+kOTZKm+04F/ApGuMhFtbrFfRDLQHZcPNb92PwCRItJD
RDoC7YGvHerxhK0OVj3CqfradVVKhVh/jrOqp5RSRQ7bVeUdBZwol+9RakYUWlDa7ZDvl2hNko0k
pVSpw3Yu2negKbrTPFQ+U6VnvBehZ2Y90TOKn1RRh6MO55agBx+1eebuUdrfwwe4EfhaLHOmat6V
KByePYeyK+My9LP7X2t7LhArZaZTLXBxPWpAE8o6Xhvl3zvHICi51v8ApVQa8Af0bPUpEVkiDmas
BkMdMP1ZRc5mf3YLMF8pVWp9K7/CwUwQra2yaTVuBhYqpQqp2ffaqe8QkUAReV9EjlnX9kecv39J
SqmCSureCphcrt/pa51XGaOV1qRfCXRDD+RtZVVV97VoM9QwtFbqI6Cjtd3DSkdEokTkv6LN1TPR
Wpjyz4pjG+rSP7o6PoqajTWqu/fnXR9Sh7IuSoyAdf6yG/0RALRvCtAWbcfu6PRbm4dZVZH2HnoW
pq3VsTyP1sbUCqXUNuBl4A1rW6FfwlYOh7Wk6kFnbcssBRLRsy42WtT0XKXUJ+gZpcet3cfRA4JG
DsJKkFKqd/nzReQK4E/ABPTsf2P0DJ7t2lV1zbE6roXozu9mYLFSKs+hHs841CFEKeWnlFpUk7aV
L6rcdlV5J+B8LUHfMxs5lAmjoD/ENhKAYvRzZMs3WCnlyoynPPZzK0n/CD3wvxpItJ41V5zE4Xmz
TGWaUYdnzno+fkJfL5t9eVXvSgIOz541WKpq8HUL+ju8W3TUx9Xoe2UbHB2n8utR1bN1CijF+b7V
+L1TSi1VSo1EDwKOYfntGQx1xPRnNS+zVv2ZiLRFm/HeIdqP5xTaMuI6EQm2DvsWaC06GupEyswD
a/K9Ln+dZ1h162dd2ytx/v6FW6bVrup+HK3Ncux3/JVSs6u4JLoSSi1D+8P+rSZ1V0ploE2bHwa2
WH3tZmt7l1Iq08rn7+g+rZvVnjup+Kw4XoPq+kdXuDr+JDUba1Q5huA87UMMRsBqUCx/Fx+0GZG7
aAdDm3/NYnQ0oBusY54F4pQOyFAfBAIZSqlsyyfprupOqIJ/A+1E5Cprex7wrIiEikgE2r750zOr
bgUWAE+KSLBlq31vLc9/CfijiIQqpX5Dq85ftmbr3EQ77g5xcV4g2qwtCW1y8Tx6RspGIrpjq6pz
/wzd6U2irOMDbTf9JxHpK5oAERknIn4uc6kdVeW9GvAR7cTqISKT0D4HNrYDvUSkq3X8M7YEqxN7
H/iniIRZebewBNEqsc792Do3UnTY4SFS5kuwCn29X7SOq4zPgQkiMszSds1AO95vrtGVKYdoG/b2
lEUerOpdWQL0E5GxVtmPUjbjWj7fALTz/61oE0nb3yPoWV439Lt0j9UWN+tadrCySETb7VfAmkFe
DPxVdOCQtmjzjmrfO9GBVq6x7m0BegBQWs1phksc05+dVWrTn01F+/d0ouwb0hH9zfsdgNKBRRaj
fVw9sfyP6vi9DkRrK9ItbZDj+k8HgP3AUyLiaX07r3ZI/wi4SXSQBXfRQSpGiojLiLcueBUtOHau
Yd1/RptX2vytVpXbtrUnG8i0rvXD1dShuv7RFS0cjp+MFjp/rOVYowLnax9i0BgBq2F5Cq3tmIF2
BM2z9qGUSgJuQA8m09DOjxPrsS7/B9wp2kTjLSoJoVsTLA3Mm+iQvKAH4HvQA9TtaNv7l8+othV5
Cn2djqKdThegB4c1Qim1mbLZLdDCTgiwD+2D8zmuTQS/Rn9wD6GdaJPRwpaN+WhtT6roCHauWI0e
lASjoyTZ6rQObab1Dton5gBay1XdjFa1VJW3df8moINEpKFt2b92OHcn+v6tQV+fVeWyfwg9O7cZ
7W/1PdqRuyY8gL6W29ADhL9gzSZas8efoE0751bRth3AHVbbktBOvdcpHRGwptgiK2WjO6jpSqmV
Vlql74pSKgH9nv7DKjuSygW7G9HP1jyl1CnbH1r4DUL7eaxB27q/jb6WKyibDZ0NTBWRNBFx9T7d
Y/0/CvxktaPS6+aAO/qbdAp9D/qhByUGQ1WY/uzsUaP+zJq4mwq85fgNsb5D71LRTHAU8Hk5E+3a
fq9fQZvQpaDN7L61JVjf6N9b5aQBT6BN1wqs9MPo5+A5dF95FD1or9FYVOn1meZTJtRVV/ef0QLU
6kq2Qd/PIdb5i4EvqqlDlf1jJawGeqG/908C11saNqj5WMMV52sfYgBEvw8Gw8WFiPwfcLVS6qpq
DzZUi4jMR5tVvNDA9bgbHQTEhIM1GAyXBBdyfyYiXwEblFIvNXRdGgIRmQbcaPqsSw+jwTJcFFhq
7wGWCrwrelZscUPXy3D2EO23cS96ds5gMBguSi7k/kxE+otItFX3a9Emgl81dL0MhnNNvQpYohep
2y96oboKC6SJyHWiFz/bLiKbHe1OKztXRGaKjvay3fob45AWIyL/E5HdIrLTsvU2XBp4o+2xs9Bm
AvMxazJcNIjIOHS484PooCAGg8FwsXIh92fN0aaDWegAErcrpfY0bJUMhnNPvZkIWo7pB9CLk51A
ryUxyfFFsxz0cpRSSkRigAVKqU5VnSsiM4FspdQr5crzQEcNmqKUihORUCBd6TDJBoPBYDAYDAaD
wVDv1KcGKxa9MN5hpddbmI9ekM+OUipblUl4/pQ571d7rguuBHYopeKsvFOMcGUwGAwGg8FgMBjO
JR7VH1JnmuG8QNoJoH/5g0RkAjpEdgQ6GktNzv2TiExFR46ZrvTCmB0AJSI/oBdMna+UqhAVxXKS
vxvA39+/T6dOncofYjAYDIbzjC1btiQrpcIbuh5ng7CwMBUdHd3Q1TAYDAZDNdS176lPAatGKKUW
A4ut9RL+QtlinpXxL+s4Zf1/Fbgd3ZYh6JDCucAKEdmilFpRrrx3sZzk+/btqzZvrtPSOAaDwWA4
h4jI0Yauw9kiOjoa0/cYDAbD+U9d+576NBGMx3kF7+ZUsQK0Umo10MZauK7Sc5VSiUqpEmsdh/fQ
5oSgtVyrlVLJSqlc9NoMjqthGwwGg8FwVrEivq0UkT1WgKUHG7pOBoPBYGhY6lPA+gVoLyKtRcQL
vajgEscDRKSdtVAeItIbHTknpapzRaSpQxYTgF3W7x+A7iLiZwW8GI5eDNBgMBgMhvqiGG2q3gUY
APxRRLo0cJ0MBoPB0IDUm4mgUqpYRO5HCz7uwPtKqd3WomsopeagV/SeKiJF6FXff28FvXB5rpX1
yyLSE20ieARrpWmlVJqIvIYWzhTwrVJqaX21z2AwGAwGpVQCkGD9zhKRvWg/YjPBZzAYDJco9Ram
/ULA+GAZDBcORUVFnDhxgvz8/IauiqEe8fHxoXnz5nh6ejrtt3xq+zZQtWqEiEQDq4FuSqnMcmn2
AEstW7bsc/ToReNSZjAYLlFKShWlSuHpXq/L6p41MnKLiDuRzrAONY9ZUde+p8GDXBgMBkNNOHHi
BIGBgURHR2NZFhsuMpRSpKSkcOLECVq3bt3Q1akV1rqOXwAPlReuoGKApXNcPcMlTH5RCT6e7g1d
DUMDk5pTyE/7TnNZx3BCA7zPKJ+krAK+33WK2csP0DEykG8eGHJeC1nZBcWsO5jMg/O3kV9Uyjd/
GkK3ZsH1WqYRsAwGwwVBfn6+Ea4uckSE0NBQkpKSGroqtUJEPNHC1Vyl1KKGrs+liFKKpOwCwgO8
z+k34lBSNtn5xfRoEXLW8y4uKbW3xd2tbm36yzd7+GDdbyy4ZyB9oxufzeqdN5SWKkQwfQNamP5q
ezzdm4XQJSqI0lLFgs3HGdUlkocXxLH6QBKD24Uy984Btco3r7AEH083Zi87wOs/HXRK25+YxWvL
DjCkXRgiMKhtGFn5Rew4kUFogBedmgSdzSbWig2HU1h3MJl5m46RnF1o37/xt1SXAlZRSSmLt8aT
kJHPHy9ri8cZCI1GwDIYDBcMpgO9+LnQ7rEVqOk/wF6l1GsNXZ9LlW93nuKPn22lWYgvb/+hN20j
Alh9IIlvdybwxqRelKq6CylVMfLVnwE49NcxVeZ/NCWHlJxCerdsVKN8d5/M4JrX1wJwWcdwPrgt
tpozKpKeW8h/1v4GwPT/xrHkj0MI9vO059/Iz4uoEN9a53u+MeHtdaTnFfHzo5fV+tyiklJeXLqX
oe3DGNExwuU9PJGWi5tIja/Vqv2neXD+djpGBvLe1L72a14dCRl5vPHTQRZuOUHTYB/Sc4vo1TKE
P1/dic5NqxZSsguK+XTDUZbuSGBnfAY+nm7c2Kc5q/YncSItj0FxJ1l/KAWAA4nZNaqPjUNJ2fbn
HCA2ujFpuYX8ejqbx67uyMvf7+dfqw7xr1WHAFg343Kuf3sdiZkF+no8MoLoMH+Xecen5zF3w1Ha
RwYwoVfzWtWrJtz83gZKFXi4CZ2aBDJ1YDTvrj7EX77Zw+6TGcy6PgYvDy1EHU/N5d65W9gVrw0Q
lu09xeB2YXUu2whYBoPBYDDUncHAFGCniGy39j2hlPq2Aet0yXEoSQ8aU3IKuO6tdU5pozpH8vRX
u+jeLJhZ18fQMtSPk+l5NA32OWsC/YCXVrDh8ZEVBuilpYrknAKG/30VAJufGkVYDcyzdsVn2H+v
3J/EvlOZTpqAT/53hLdWHuLxMZ24rmczl3nsTcgC4KqukfywO5FHF8YxbURbnli0k32ndNrndw+g
f5vQ2jT1vCC7oJjcgmIignyIO5FR/QmVsC8hiw/XH+HD9UcAmHV9dybGtnQ6ZvK/N3IkJZdXb+rB
DX2chYDvdyUQHuhDWIAXkUE+PP/NHj7beAyATUdSWXMwibExUdXWY0ncSR6cvw1bWIRgX0+OpuSy
an8SR1NyWfnIiCrPf3zRTr6OOwnAtT2i2HA4hU83HMPfS5uGrj+UQoifJ9fGRPHZpmOUlircrGdV
KcWJtDxaNPZzyvPH3adIzyvixaV7nfa/d0tfPN2FxMwCWof50zUqmFve32RPHzzrJ6fjp/83jjdv
7kXT4IoC6nNLdvPjnkSAKgWszzYeIyLQm1FdIqu8Do5sO5ZGqYKOkYF8cmcsEYE+APRsEcIbP/3K
oq3x5BWW8ML4brzx00E+XH+EAG8PHr2qIxl5Rby7+rBd2KoLl7SAlZeXx759++jUqVNDV8VgMJzH
pKSkMHLkSABOnTqFu7s74eHaSXbTpk14eXk5HZ+amsqCBQuYNm1alfkWFxcTFhZGenp6hf3e3t50
794dAHd3d9566y0GDKi5WcdTTz1FWFgYDz30kMv0bt260bNnTz799NMq8zl8+DCbNm1i4sSJAGzc
uJH58+cze/bsGtflYkYptRa4sNRu5xHFJaUkZRe4HHzVhqSsAkL8PBnXI4qP/6cDiEzo1YzF2+J5
6HMt964/lMLXO04SHerPHz/byr/+0JvR3ZtWlW21+Hm5k1tYQlJWAf9Ze5i7h7W1p+1NyOS+uVv5
LTnHvm/PycwaOdjnFpYA0LtlCFuPpfPIf+NYOG0Qm35LZarDYPbB+dsJ8vHksk4RFfJIztYahOlX
dsTH052vtp+0D2ZtvPzDfr64d1DtGl2OXxOzeHvVIW4bHE1M84qmkrviM2gW4ksj/7LvZF5hCb5e
7jzz1S6+2ZHA5AGtePiKDjUu86/f7uWzjceYM7lsudPiklK7SdexlFxeWLqHO4e2IbZ15aaRh5Od
tTn/+vmQk4CVmJnPkZRcQAsKvVqG0CY8AICT6XlM+3RrhTwb+Xny0vUxTPt0C9n5xdW2JbugmDmr
DhEd6s9bN/cm0MeDFo39SMku4Nklu/lmR4L9erli36lMvo47yX0j2vK7vi1oFepHQXEpiZn5tGzs
x3/W/sYLS/cyokM4bcL9KSlVpOUW2v2wbNrfj2+PtT+b6bmF3P3JFgACvT3458SeZOUX87u+Lewa
n9ZhWoQY3iGcGaM7cTw1l7mWcAmwYvpwth5N49GFO7jzo81M7NeCyQNaOU1qpORosz1P96o/oU8s
3gnAL0+OIjyw+gmKhIw8Jr67AYC/Xt/dLlwBdIkK4l+T+zDn50P87ft9rD2YTFZ+MRGB3vzj9z0Z
ZGmtft+vBS0b++H1t2qLc8klLWDt2bOHCRMmsHfv3uoPNhgMlyyhoaFs364HaTNnziQgIIBHHnmk
0uNTU1OZM2dOtQJWVQQGBtrLXLp0KU8++SQrVqyoc36O7Ny5Ew8PD1auXEleXh6+vpUPbg8fPsz8
+fPtAlb//v3p37//WamH4eIiv6iE2z/8hUeu6lhjU7iXf9jPu6sPV6nZScoq4P7PthIV4ku/6Mbc
3L9lhWNOZ+UTEejNLYOimb/pOA9d0Z57h7fldFY+6w6mcMvAVizbk8j+U1l8tT0egEXb4s9YwPL3
9mBcjyhOpOXx0fqj3D2sLTkFxfh7e7Bwywkn4QpgZ3xGjQSs5OwC3N2EhdMG8f46PUB+d/VhXlt2
wH5Mr5YhbDuWzhdbT1QpYIX6e/GP3/ekd8tGLNuTyIm0XCb0ak6pUvxzxa+8t/owUwa2qlMgjPTc
Qu75dAuHk3IoKC7h7T/0cUpPyylk7Bva1PGfE3syNiaKxxftYMHmEwT5eJBpCSBzNxxld3wGiVn5
PDGmM4PaVm2ate2YnpRyFHBe/HYvj17VkfdW/8a3OxPYn5jFj3sSeen67kwqp5XaeSKDp77cadd+
7X3+av6x4gAfrD3ipN2JO67LeW5cV55dspvfvfM/Wof5M6Z7Uw4kai3gsA7hnM7MZ9+pLBr5ebL0
gaEE+OjhdVYNBKwrX/uZkxn5/N+oDnSJKtNShgZ4M6Z7U77ZkcChpOxKgzL88lsqAFMGtrJPVPh4
utMqVJvlTR0YTUZeERNjW7L1aBoAp7MK2Hw0jT6tGnEkRT+jL323j/aRATQN9mXVfu0H26NFCO9N
6UNEkE/5Yp2YNlxPLNw2uDWg8Pf2oGmwL23DA0jNKeSl7/bx9Fe7iQjy4aquTezn2Z5Rtyo0yflF
JfbfvyXn1EjAenvlIe1Pdd8gelXyLZo2vC1BPp48sXgnUcE+/PjwcAK8y8SitpYgXVfO35Af54jC
wsLqDzIYDIZKePnll+nWrRvdunXjjTfeAGDGjBns37+fnj17MmPGDDIzM7n88svp3bs3MTExfPPN
N7UqIzMzk0aNGtl/V5bX888/T4cOHRgyZAi//vprpfnNmzePqVOncvnll/P111/b9x84cIDLL7+c
Hj160Lt3b44cOcKMGTNYuXIlPXv25PXXX2f58uWMHz8egOTkZMaNG0dMTAyDBg1i1y697vtTTz3F
HXfcwfDhw2nTpg1vvfUWAFlZWYwePZoePXrQrVs3Fi5cWKvrYDi/2X0yg/WHUvgmLqHG56zcdxqA
lOyKfXF+UQmnM/Pp9+JyNv6WyuJt8faZ7PIkZRUQHuhN2/AAdj13FfcOb4uI8PrEXjx6VUfuGd6W
jk0CWRJ3kgOJ2QR6e7BsT6J9AF0XlFJk5BYR4ufFZZ0iiE/PY9onW+j67A98tvEYi7fFM7R9GHue
v4pv/jSE3i1DeGvlQf767V5KSl0HkswpKGb6gjiW7zlNqL8Xbm7CuJ7axMwmXN0+uDW/PDmKxfcN
ZmyMHoBvPZZWIS+bkNbIzwsR4ZZB0Xx6Z39WPXoZD45qz+2DWzOgTWNe/HYvb5YLXLDhcApPLt7J
tzsT2GQN4F3x077THE7SA/SDpyv69sSn59l/Pzh/O/d/tpUFm08A2IWrrlFBpOQUsmLfaXbFZ3LP
J1vILqhaMEnLKcTH041AhwHxB+uO0OWZH5i9/AD7LeGnY2Qgi7dpgXpXfAav/LCf7IJi/jRvKzvj
M3B3E3q0CMHXy51mIb4UlpSSnFNgz/N4mq7/2Jim+Hq6k5xdyC9H0nju6z3M23Sc/q0b89Ft/Zh1
QwwAfaPPCJZjAAAgAElEQVQbExXiS4CXByKQlV9UZTsKiks4mZFPbHRj/nhZ2wrpnZoEAvDnL3bw
/a5TJGZWXKYky7pWjfy8KqQBeHm4Mf3KjjQL8bVPYvzt+33c88kW+r6wnJ8tYerg6Sym/GcTP+1L
5JH/xtE23J9F9w6qVrhypF1EAO0iAp000ncNbcPaP19GoI8Ha39NBiAlu4AXvtnDUUs7WFBc6vRO
JGcX8H+fbyc1p5DTmWX3Y29CJsUlpU5lbj2WxnurD9u3D57O5pMNR7m2R1SlwpWNm/u3ZNMTI/np
kRFOwtXZ4JLWYPn4+NC+ffuGrobBYKgDM2fO5LnnngPg2WefZebMmU7p06dP57XXdMyBV155henT
pzul33333bz33nsAvPPOO9x99921rsPGjRuZO3cuv/zyC8XFxcTGxjJixAhmzZrFwYMH7RqooqIi
vvzyS4KCgjh9+jSDBw9m7NixVeadlZVFz549yc/P59SpU6xcuRIAX19fl3lt2rSJL774gri4OAoL
C+nZsycDBw50mfeCBQv4+eef6dSpE++99x6/+93vAJg0aRIzZ87k2muvJT8/n9LSUmbNmsWbb77J
l19+CcDy5cvt+Tz99NP079+fJUuW8OOPP3LrrbdiW1vwwIEDrFixgvT0dDp37sy0adP49ttviY6O
5rvvvgMgI6PuvhOG8489J7W/wrbjFQf7lWEz6SoqN2jam5DJ6H+ucXnOj7tPcaXDLDhAUnYBfazB
lM2ECbQW4I+XtQPgnuFtCfb1pE14ADf3b0nfF5az7lBynSMA5hWVUFhSSrCvJ0Pba43L97tPAdqk
qXkjX54Z2wU/Lw+6NQvm1d/15PFFO3h39WGOJOfwzpQ+FXzAth5L44utJ5z2hTto9ibFtuTJazrb
fb1ujm3JNzsSWLYnsYLWMDmrkMaWkOaKYD9P5t89kLFvrCHuhLOg+cxXuziQmG03+Toy6xqXeeQX
6fs2vmcUS3cmOJnpgRZ8Af47bSA3zfkf3+06RSM/T2aO60qpUoT4at+lOz76hWu6N6V782AenL+d
+LQ8OlrCRXni0/M4lZnPgyPb89Co9qTmFPLi0r2M6BTB578cI7+olC2WpmZg21A+XH+ED9b9xnNf
67W/T2dps79nr+3CpNiWeFjXJ8oSCk6m59tNyk6m5+Hr6U5jfy8+uK0fCRl57DyRyfvrfqN9RABv
3NwLEaFH82Cev64ro7tpjaibmxDgVaahq4z0XC2AjesZ5TJiXZvwAKZf0YF3Vx9m2qdbCPD2YPsz
Vzgdm51fjIeb4O1Rvc6kY5NAgn097Roq0L5iUcE+zLohhqnvb+L2DzcT6u/Fu1P7npXAMG5uQvNG
fjQL8SUhI4+T6XmMe3OdXXsVFezDyYx8cguLCfTRAUF+3p/E4m3xHErK5skxne15PbtkN6cy8/nz
1WWuPfd+uoXEzAKu7taEFo39OGz5Y94+uGZLfdRGgKwNl7SA1bVrV77//vuGrobBYLhAWbt2LTfc
cIPdxG78+PGsWbOGK6+80uk4pRQzZsxg7dq1uLm5cfz4cZKTkwkJqXxg52giuHbtWqZOncrOnTsr
zWv16tX2uvj6+nLttde6zHfDhg00a9aMZs2aERERwV133UVGRgalpaUkJyfbz/Pxqb7TWbt2LUuX
LgXgyiuv5NZbbyUnR89mjx07Fi8vLyIiImjcuDFJSUnExMQwY8YMZsyYwbXXXsvgwYOrLcNw4bAn
QQtYu09mUlhc6iToVIZtcOuoscjKL3ISrtqE+fPtg0MZ8reVJGcXcPcnW5wG/EopTmcWVDtQGtAm
lAEOAR2aBPnw8vf7ubprE7tfTW2wDY5D/DzpEBnIZ3f153BSDu+v+42M3CKWPjCUYN+yCHKtw/yZ
d9cApi+IY9E2HQq6fGQ6m+8VYPdJEhE+uK0fHm7C0PbO5oWD2oXRLMS3gmZj2Z5Evtweb9eAVEWz
EF9+2J3IzweS6NI0iG3H0kjNcdYo2swey1NYrOsb2zqUL7efZOwba1n6wFD7wNxWr6gQXwa1DWX9
oRQ+vC22glD7v8e1j+u6g1rDkZZbuXXR377bB2hhQUQIDfDmtd/3BGBcD63tu+eTzfRoEUKvFo34
cP0Ru3AF8M2OBFo29rPM2cpoGaqDPBw6nU2HyAAW/HKcA4lZRIXoYCi2Z2dCL3jm2i5O54oIUwdG
O+0L9PGo1kTQ9gxVpn0C+NPI9tzYtzmjXv3ZHi3wVoe65xQUE+DjUaOALY39vfj2waEs232KER0j
WLozgb//sJ/k7EKGdQjn+eu68vP+JP40sv0Zm8iVJzLIh+V7T7N8rw6C8ehVHTmZnkeovxev/3SQ
3MISu4DlYflk7TiRwb1ztRmop7tQVKJYbz0joDWtvpZp6zc7Erh3RFtOWlrTZo0aNkLmJW8iaDAY
DPXNxx9/TEZGBlu3bmX79u2EhYWRn1/R1KMyhgwZwsmTJ0lNTT3jvObNm8euXbuIjo6mffv2ZGZm
smjR2V+6ydu7bNbd3d2d4uJiOnfuzObNm+natSszZszgr3/961kv19Bw7DmZiYebUFhcyt6EmkXf
sg2kHAeitkGnjSfGdMbH052Xb+zuMo+sgmIKikudND014bbB0YAemLniWEpulSZeNv+qJsFasBvU
NozJA1rx1R8Hs/bPlzsJVzZspnpQ5kfkSGaeLm/NY5fxwMgyC5vLOkZUEK5shAd62zVFNj7dcJSC
4lKeGtvF5TmO2DRfd328mUcXxnH3J1tIzi7kqWs689bNOojES9/tJS2notBTUKw1WNdYvmz7TmXZ
B7gHT2czY5E26QwP8OafE3ux7P+GVakxDLFCmq/an8S0T7aQW6ifi8TMfL7ZcZInFu/ku10JdIwM
ZHS3JpXm886Uvtw3oh0D24ay8pERTBnQintHaBO83MIShrSv6OPVNjyAQB8PNh9N5YN1R5j59R7W
/JpMs0Z+FY6tCYE+nmz8LYX31/5m9yMqKVUcT821H2MTJEOqCeXeNNiXXc9dRbdmQSzd6fy8ZhUU
4+9Vc31JsxBfbh3cmugwf24dFE2bcH+7X+PUgdH859Z+9KyHdd1OpOl2B3h78O6UPvzxsna8OKG7
fXIjx2GSJaegbKIhNaeQu4e1YedM3f4y/7YiJr67wR6EZPayA/T5yzJmWsJ0qH/lQuu5wAhYBoPh
gmTmzJkopVBKVTAPBHj11Vft6eXNAwHeffdde3pdzAMBhg4dyuLFi8nLyyM7O5uvvvqKoUOHEhgY
SFZWlv24jIwMIiIi8PDwYNmyZcTHx9eqnN27d+Pm5kajRo0qzWvYsGEsXryY/Px8MjMzXfp5lZaW
snDhQvbs2cORI0c4cuQIixYtYt68eTRq1Ijw8HC7T1Z+fj65ubkV2lK+/XPnzgW06WCzZs3w93e9
3glAfHw8AQEBTJkyhenTp7N1a8UIXIbzm6z8IpbtSUQpZx+i4pJS9p3K4gorjPKukzUz//S0TJ0c
BZlMh9+/vjjaHpp5RIcIYpoHVzjeJlzUxPndkXuGt6VH82BWH6i4sHVhcSnD/r6SaZ9uqfT8n/ad
xtNdiC23gG+gj2elEd8AOjUNJNTfi1nf72VnuRDjGZaAFeRTs7WTACKDvFnza7JTMICkrAJGdoqg
Xw0WF751cDSv3NSD0lJlNx17fHQnJg9oRZtw/T5/uuEY/V9aUcHXq9ASsPy83fn0Dh38Jj5dm4G9
tmw/oINAeHm4ER7oTfvIqjVqNk3OnJ8P8f3uU/T5y3IOJGZx9ydbuP+zbXy28RhFJYoh7cNqHGK/
dZg/fxnfjfstU1EoEwgdcXcTBrYJZd6m4/xjufZ3u3NIax67qmONyinPibRcTqTl8fw3e3hx6V6U
Ujw4fxtDX17Jml/1dU6voYAFWjhvEuRLtoPwAdpEMNCnbgZp/t4erHh4ODPHda3T+bVhyoBWgI4E
6Gji62e9K47aW5tg/dUfB/PtA0PtkyxNgnxIzdHvyPHUMv8+AB9PN1pZWshgX88GX1PxkjYRNBgM
hjMhNjaWSZMm0a9fPwDuvfdee2j1Pn360L17d6655hoefvhhrr32Wrp3705sbGyNfD9tPlg2Pv74
Y0SEKVOmuMwrNjaWCRMmEBMTQ2RkJLGxFRcmXblyJa1btyYysmwtkcsuu4zJkyeTmJjI3Llzueee
e3jyySfx8vLiiy++oFevXpSUlNCjRw/uuOMOunQpmxF//vnnuf3224mJiSEgIIAPPvigyjbFxcUx
Y8YM3Nzc8PLyYs6cOdVeB8P5xRdbTjDz6z08dnVH7htRNmBNySmkoLiUQW1DWbH3NMccZumrwhae
2dFEMDNP/x7WIdwugIH25bhraBv+NG8bCRn5dnMimxN8RC0FLIDY1o15b81vHEvJtZuIAXafpHUH
U1yeF5+ex9yNR7mySxOXpnNV4e3hzr9v6cu9n27lro83s+GJkfa0zLwiRKjVgDnAW1+Hxxbu4PVJ
vQDtk9a9kqhzrupzY5/mbDuWxtyNx/jL+G72wXCnJoH8c2JPfj6QxKKt8Vz/9non88yC4lLcRJt6
RoVoTZ4tPDbUfpHk8qZyeUUlvPz9PuKOp9OtWZB9XSKboF0b/L09eP66rjRv5FvpArL3XdaOVfuT
6NkihNcn9bJrJ+vCiE4RLN2RwOhuTfhkw1G6RgXZtaV3f7yFzk0D2WppMUOqMBF0boO7k6YHIKew
+IwCNJwrQeTWwa25ZVB0hfJs749ju2zCVpeoIKdvQGN/L5bvPc3gWT/RNqLMhLF3yxC+uHcQIkJy
dkGFa9QQXNICVk5ODmvWrGHIkCENLukaDIYLg/Lasscee4zHHnuswnELFixw2t64caPL/MqvgQXg
4eFBSUmJi6MhIiKi0ryeeeYZnnnmGZdpACNHjrSv5+VYVmKiXhsnMjKSVatWVTiv/L5Ro0YBEBYW
xpIlSyoc/8ILLzht79unfSaaN2/OmDFjKq2f4fzHZsq3vZx5m81nJyzAm+aNfTmWUjMBy93NpsFy
9sECePTKipqDptaANyEjnw6WNiQpu24aLNC+Q++t+Y37521lyf1D7Ptt0QWjQ/0oKVWM+ecaWjT2
5b2pfRER3lqpo+49PqZu62j2atmIKQNb8fcf9jutcZSZX0ygt0elgSlcMW14G77YesIeYCMjt8ge
VbE2/Hm0XrS4X3RZsAwR4bqezbi6WxMWbY2vkGdhifa1ExGXa5nVZJFdRxw1f6O7NeG7Xafsppi3
DIymS1QQW4+l232takt5P6ny9GwRwt6/XH1Wgju8elMPXr4hBnc3YfWBZcxYtBN3N6Fvq0Zs/C3V
LlwF+XjU2JzN39vDrt2xkZ1fXGMBraFxNda23fPHF+1kbExT+kQ3JrewBC93NyfhCsDPMoWMT89z
ilCZU1BizzsswLtGi3nXN5e0gLVv3z6GDRtGfn6+k7+AwWAwGAyGitjM98oHQbD554T4edGysZ89
/HJ1lFqhmW2+R7oMPYAM8q04RGlsDURTHUJp19VEEGBkpwiign04eDobpZR9kGYbvBUWl3IqM5/9
iVnsT8zicHIObcMDWPNrEsM7hNO8jv45oJ3+QUe1s61ZlJFXRJAL362qaB8ZyKNXdeTvP+wnv6iE
P/xHa5CqMlN0RZCPZ6WL8np7uHNtjyh2xTubNBYWl+Lt4W4vb3C7UDpEBnL74NY0b+Rbp8nrTk0C
OZSUzSNXdaRUKX7YrSeAGvl50TUqmK5Rtdde1YazIVwBTuuKPT6mM68tO8Dfb4yhS1QQr684yHU9
o+jZIoRSpWq8Bpm/l3uFEPZZBcU0b1z357ChsYXaP5ycw+vWcgHdmwW7fH5tmt37L2vHmyvLlhbI
L3Y9IdmQXNIClo3CwkIjYBkMBoPBUA02H6FT5aLWpVmBKRr7e2mH/PiaBbnIs/yGfj6QxIzRnRAR
uwYr0IUfUqg1M+24btbprHy83N1cBpWoDjc34c6hbXj+mz0kZxfahTRboIaTGfnsdAhh/r9DKXwd
d5LjqXncNbRNrctzJDJIl3XbB7/Qp1UjXr4xhr0JmXUSFG0+POm5RfZrXxP/q9rg7eFm97myUVBc
4hQtcu6dA864nEX3DcLL3Q0PdzcnAbaRf+3v7/nC5AGt+EP/lnaB86XrXQdsqQ5/bw/yi/SaUTmF
es20w0k5VQb8ON9pFxHArOu70z4ygBv+9T9AL8gd5cI8857hbRncLowBbUIZ3C6MdhEB/P2HfUwZ
EH2Oa109l7SA5efnR+/evSs46xoMBoPBcCmilKJUVT6LbxOwTmcWOGl8Ui1n/Ub+njTy8yQ9t9Ap
vTJsgRn2ncoiISOfk+l59pDarvyQgnw88HQXUhw0aDZzuLqa+ttMDX9NzCK7oJiiklK71gRg2qdl
wVh+3JPI6gNJXNczion9WtapPBs2Ddbh5BwOJ+cwpH0Y+05l8fKNMbXOy+a7NO7NtQDcM7xNpdqo
uuLl4UZBOU1BQXEpXi7WbzoT/Bwi4o3p3oT/rP0NgGDfC8MMrjLOhiuKzdfqiy0neGPlrxxPzaN9
RIB9rbcLERFhYqx+lw7/dQydnv6ewpJSlxqsAG8Pe7j8gW31/5dv7HHuKlsLLmkBq3PnzqxZ43oh
Q4PBYDBcXIiIOxCJQ9+nlDrWcDU6v1h9IIn75m7F011YN+Nyp4Huyv2nCfX3sgtYhSWlpOUW2U32
7CaCvl409veiuFSRVVBcbTS8vKISIoO8ScwsYMvRNJ5YvNOeVt7/AvRgrJGfF6mWBmvj4RQWbY0/
o5DMHZpoZ/m9p7K4d+5WextvHRTNFV0imfbpFpQCQV8jX093XhjfrUbrfFVF80a+BPt64u4mpOYU
8uD87bRo7MuEXs1qnZdNg3XaMpcc2SmyqsPrhLeHGwVF5TVYpXh71l9A6j6tyoTERjWItHexY3sn
H/tiBwBdo4KYd/cAp3f1QsbNTWje2JfDSTkXfJsu7NobDAaDwVADRORPwLNAImAbJSqg9uqCi5Sp
72+y/159IJmrHcyObvvgFwA6Nw2y70vPLbQLWMnZBQT6eODl4WZ3uE/LKaxUwFoSd5KP1h/heGou
IzpGsGxPIn+at82e/spNlc9KN/b3smuwPvrfEQAnjVZtCQ/wJsTPk798s8dp/yNXdSTA24O4Z64k
p7CYcW+uI6ugmFahfi7NF2uLn5cH256+guV7E7n7Ex0Ofs7kPi4Fy+oIcdDubHpyJBGBdY9+Vxne
Hu4UlDgLWIX1oMEqT9eoIHafzKyTCejFhr93mVbn/Vv7MrBNWK197c53+rcO5XBSTq0CvZyPGAHL
YDAYDJcCDwIdlVKu424bnEjKKvOxcvS72ZuQSViAN8nZBXZND8DRlFz7GjQ2TUNiZgE/7k7k5v4t
K4Qyn7lktz1QRqcmgXRqEsjhpBxKShWv/q5HlaHPQwO8SMnRJooHErMB+PC2fnVuq4gwoHUo3+8+
hY+nG3+7IYauUUF2cyw3NyHQx9MefKLpGYTuLo+bm9CsUVn0vboGcHD0T6oP4Qq0iWBhcamT6afW
YNXvAP+zuwZwOCkbj3oW5C4EHBcUHtEh4oIXQlzx1DWdCfb1pEcdQvGfT5in1WAwGGqIiDB58mT7
dnFxMeHh4YwdOxaAJUuWMGvWrLNe7m233cY777zjtO/LL79k9OjRAAwaNKjK848cOUK3bt2qPeaz
zz6zb2/evJkHHnigjjU+LzkO1Gz124uc/6z9jYEvrXDyPy7vi2yLzAc4hUMG+L8r9Npr6U4CVo49
El4jS6s144sdvPjtXuZtqmiF6Sban+KKLpHcOaQN06/syFt/6M2cKX2qXVcq1N+b1JxCfjmSxsHT
2bx8YwwjOkbUpOmV8urvevD2H3qza+ZVXNezGe0iKi6IG2IJWGeyNpIrOkYGctfQ1qx8ZESd82gS
5MMDI9uz7P+Gnb2KlcPbMoksdNBiFRaX4F3Pgk+wrye9Wjaq/sBLgM5RWoPcOsz/ohSuQAfymDG6
E6NdLAZ9IXFJa7CysrL47rvvGDhwICEhIQ1dHYPBcJ7j7+/Prl27yMvLw9fXl2XLltGsWZm/xLhx
4xg3btwZl1NcXIyHR9nnedKkSbz00kvcc8899n3z589n0qRJAKxfv/6My7QJWDfffDMAffv2pW/f
vmec73nEYWCViCwF7NKDUuq1hqtSw2AzhcvIK0IQ5v1yjBEdw52OSXKI0nfotNYSebgJvzw5ijQr
oEVGbhG74jNYtDWeIym5XBOjB0TNQrRG5rC1flG6FWHwrZUHCfHz5A/9W5FXWMLE2JY8PbZs4eqa
0thf+2DtsKL7jep85v5G/t4ejKlmQGfzuWoSVHG9pzPBw92NJ6+p/XVwRER4+IoOZ6lGrrEJWAUO
odkLikvPaJFbQ+1oFuLLDw8NO2P/P0P9c0nfoQMHDjBmzBgOHDjQ0FUxGAwXCGPGjGHp0qUAzJs3
zy7kAHz44Yfcf//9ANx666088MADDBo0iDZt2rBw4UJAawoeffRRunXrRvfu3fn8888BvZjv0KFD
GTduHF26OA+2Ro4cyb59+0hISAD0IunLly9n/PjxAAQEBFSZtyNHjhxh6NCh9O7dm969e9uFsxkz
ZrBmzRp69uzJ7NmzWbVqlV0zl5qayvjx44mJiWHAgAHs2KEdrGfOnMntt9/OiBEjaNOmDa+//vpZ
uML1xjFgGeAFBDr8XVL8aC1GC9qE76Xv9jLru308tXiX03HJ1uK9ydkFPP3VLiKDvNn+7JU08vey
+8IcTcll3JtreX+djvJmi8YXGeTDG5N68fAVHQgP9OZwcjYlpYq//7CfJxfvQilFXlEJvnU0LQv1
9yKroJh9p7II8fO0+4HVN4lWaPoBbc5udL4LBZuAlVNQzNIdCeQXlbAvIavefbAMznRsEkjrMP+G
roahGsy0A3odLIPBcIGxfETFfS1/Bx3ug+JcWDWmYnqbW/VffjKsvdE5bdSqGhU7ceJEnn/+ecaO
HcuOHTu4/fbbK41GmpCQwNq1a9m3bx/jxo3jxhtvZNGiRWzfvp24uDiSk5Pp168fw4Zps56tW7ey
a9cuWrdu7ZSPu7s7N9xwAwsWLODBBx/k66+/ZsSIEQQFBTkdV1XeNiIiIli2bBk+Pj78+uuvTJo0
ic2bNzNr1ixeeeUVvvnmG0ALfDaeffZZevXqxZdffslPP/3E1KlT2b59O6AXbF+5ciVZWVl07NiR
e++9F0/P888ZXSn1HICIBFjb2Q1bo3PLtmNpRIX4svVY2ZpOp7PyWX9Iu6RtPprmdPyyPYn0/+ty
RnWOJCEjnw9u7WfXVNgErNnLnScnHf2Hru0RBcD+xCxW709i9YEke1phSSmlqvYL4dpoHKAFqiXb
T9KtWVA1R589nr+uG+sOJtPfChN9qWHTWn2/65Q9lD5AYlZ+ZacYDJcsl/S0Q2BgIFdffXWFQYrB
YDBURkxMDEeOHGHevHmMGeNCiHNg/PjxuLm50aVLFxIT9bo6a9euZdKkSbi7uxMZGcnw4cP55Rcd
oS02NraCcGVj0qRJzJ8/H3A2D3SkqrxtFBUVcdddd9G9e3duuukm9uzZUyEfV/lOmTIFgMsvv5yU
lBQyM/Viptdccw3e3t6EhYURERFhb+f5hoh0E5FtwG5gt4hsEZGuDV2vc4FSiglvr+ea19eSkVc2
obj/VBbHUnPpF+3avyUxs4C5G7X/VNeosn7Sw92NIIc1qoZ30OaFrmbVbxsUTVZBMV/vOAlo36v8
Qu3DU1cNVmMrSmFhSalTpMP6pmeLkAt6vaEzxWaWti8hC4C24f6EB3rzp8vbN2S1DIbzkktag9Wh
Qwe+++67hq6GwWCoC1VpnDz8qk73CauxxsoV48aN45FHHmHVqlWkpFQelM7b29v+uyYLmvv7V272
MWjQIBISEoiLi2P9+vV2Yau2zJ49m8jISOLi4igtLcXH58wc9h3b6O7uTnFx8RnlV4+8CzyslFoJ
ICIjgPeAqiOEXARk5ut7kpxdQHpuEc1CfIlPz2P7ca3NumNIG16fFMzJ9Hz+veYwD43qwH83H2fN
r8nsT9SDaVvodRsdIgPZfDSNrlFBvH9rPwqLS10uTmwLeHEiVQfK8PPyIM9aXLiuGqzerRpxWcdw
RnSMYPKAVnXKw1B7bCaC+xOzCPX3YsX0EQ1bIYPhPOaS1mAZDAZDXbj99tt59tln6d69e63PHTp0
KJ9//jklJSUkJSWxevVqYmNjqz1PRPj973/PLbfcwujRo10KRjXJOyMjg6ZNm+Lm5sYnn3xCSYke
7AYGBpKVlVVpnefOnQto08GwsLALUfPvbxOuAJRSq4BLwpHhpEMUwIy8IqJCfAjw9mDHCR1UsUmw
D02DfenTqhH/mtyHjk0CeWpsF67toYM+uLtJBad6m0YrPNAbdzepVFiymRUeS80FtFBlF7DqqMGK
DPLhg9tiuWVQtEuhzlA/2J6B7cfTjQ+QwVANRsAyGAyGWtK8efM6hzCfMGECMTEx9OjRg8svv5yX
X36ZJk1qZuY0adIk4uLiXJoH1jTv++67j48++ogePXqwb98+u9YsJiYGd3d3evTowezZs53OmTlz
Jlu2bCEmJoYZM2bw0Ucf1aHlDc5hEXlaRKKtv6fQkQUveuLTygSs9Nwign29CPHztAs9la3rZFuf
qaS0ovZ1SHttFngqo2r/G1vI9VNWgIiSUkVeoRawfOp5/STD2cXmgwVwXc+oBqyJwXD+IzUxW6lz
5iJXA/8E3IF/K6VmlUv/A/BnQIAs4F6lVJyIdAQcw1+1AZ5RSv1DRG4CZgKdgVil1GaH/GKAd4Ag
oBTop5Sq9Ovft29ftXnz5sqSDQbDecTevXvp3LlzQ1fDcA5wda9FZItSqs5x40WkEfAcMMTatQaY
qZRKq/ys+uFc9z2fbDjK01/qKIHNQnwZ2DaUZXsS7QsFH/rrGJeaoN0nM7jm9bUAHJl1jVNaaani
yYkhuxgAACAASURBVC93MrpbU4Z1CK9wruNxbZ741r7t5+XOJ3f054Z/reej22Pt/luG85+tx9K4
/m0ddXT/C1c7CVwGw8VKXfueevPBEhF34C3gCuAE8IuILFFKOXpU/wYMV0qlichotI18f6XUfqCn
Qz7xwGLrnF3A9WhByrE8D+BTYIolpIUCRVRBeno6CxcupG/fvkRHR59Zgw0Gg8Fw3mIJUhfVysk1
JSO3LLDF6ax8gn097cLVpNiWlZrZdWlauRmom5vw0vUx1Zbt5ib4ebmTa2mt8otK7BqsupoIGhqG
Hs1DePWmHvSNbmSEK4OhGuozyEUscFApdRhAROYD1wF2AUsp5bg65gaguYt8RgKHlFJHrXP2WvmV
P+5KYIdSKs46rnLPc4tDhw5x00038eGHHxoBy2AwGC5CROQfSqmHRORroILJhlLqzFeGPs+xCVMA
RSWKVqF+9GgeTNyJDGaOq3yBWxHh+4eGnvE6R/7eHnYBq1TB4WQdId8IWBcW7m7CDX1cDdMMBkN5
6lPAagYcd9g+AfSv4vg7AFch/SYC82pQXgdAicgPQDgwXyn1cvmDRORu4G7Hfedx1CuDwWAwnBmf
WP9fadBaNCDpuc7GHJ2aBPHBbbEcTcmpVhPRqcmZBzOxCWi+njrAxTNf7QYgwOeSDmRsMBguYs6L
r5uIXIYWsIaU2+8FjAMer0E2Htb5/YBcYIVlN7nC8SCl1LtoU0RCQkLUqFGjaNXKhHk1GAyGixGl
1BbrZ0+l1D8d00TkQeDnc1+rc0tGXhGdmgRSUqr49XQ2HZsEEuzrSWN/r+pPPguk5mgTxc5NA+0L
HbcJ9zeR6AwGw0VLfQpY8UALh+3m1j4nrMAU/wZGuzDrGw1sVUrVZOXKE8BqpVSyle+3QG9gRWUn
tGvXjoULF9Yga4PBYDBc4NyCDrrkyK0u9l10pOcVEezryad39ic1p5BgX89zWr4tLHv/NqF2AesB
szitwWC4iKlPAesXoL2ItEYLVhOBmx0PEJGWwCJ0YIoDLvKYRM3MAwF+AB4TET+gEBgOzK76FIPB
YDBczIjIJHTf01pEljgkBQKpDVOrc0tGbhGtQv3wdHcjMujMFpauC3Mm9yErv8gpLHu7iIBzXg+D
wWA4V9TbOlhKqWLgfrTgsxdYoJTaLSLTRGSaddgzQCjwtohsFxHHkOv+6AiEixzzFZEJInICGAgs
tXyubBGiXkMLdtvRmq+l9dU+g8Fw6SEiTJ482b5dXFxMeHg4Y8eOrfK8zZs313ndLIA2bdqwf/9+
p30PPfQQf/vb32qU94cffsj9999f5TGrVq1i/fqyuENz5szh448/rnOdzyPWA68C+6z/tr/pwFUN
WK96YemOBL7bmeC0LyOviBC/c6u1cuTqbk24qW8Lu+asabCPfaFig8FguBipVx8spdS3wLfl9s1x
+H0ncGcl5+agha/y+xdTFrK9fNqn6FDtBoPBcNbx9/dn165d5OXl4evry7Jly2jWrFm15/Xt25e+
fWu+jEZxcTEeHmWf54kTJzJ//nyeffZZAEpLS1m4cCHr1q2jVatWtcq7MlatWkVAQACDBg0CYNq0
adWccWFgRaA9aq27eNK2NqKI+KJN1480YPXOOn/8bCsAB14YjZeHnkNNzzv3ZoGuGNQ2lHen9KFf
dGNXkYANBoPhoqHeNFgXAikpKXzwwQds27atoatiMBguEMaMGcPSpVo5Pm/ePCZNmmRP27RpEwMH
DqRXr14MGjTIrnVatWqVXcuVmprK+PHjiYmJYcCAAezYsQOAmTNnMmXKFAYPHsyUKVOcypw0aRKf
f1629vrq1atp1aoVrVq1qlHejnz99df079+fXr16MWrUKBITEzly5Ahz5sxh9uzZ9OzZkzVr1jBz
5kxeeUUH3tu+fTsDBgwgJiaGCRMmkJam1+YdMWIEf/7zn4mNjaVDhw6sWfP/7N13eFRV+sDx75ue
UAMEFAKKSBEkAYkoIEUEFdQootIUkEVEf+jqioriSlksu8paUBHWRdRVQEGKZVcBRURUIEpLqGLo
NUAIBFLf3x8ziZM+ASaTSd7P88zD3HPvOfe9SZiZd86553x/Xn7GHvIxjgXoc2QBn5yPhkXkRhHZ
IiLbRWTMubb33spElm89XOp6rtOxT/gsnuxs5UxGFmcysqkZVjYTWhQnwN+P61tdQHgZTa5hjDHe
Ui5mEfSWxMREhg0bxoQJE2jbtq23wzHGuOsjD337PbDAMkkF9O/fn4kTJ3LzzTezfv16hg0blptY
tGjRgu+//56AgACWLFnC008/zbx58/LUHzduHG3btmXBggV88803DB48mLVr1wKQkJDAihUrCA0N
zVOndevW+Pn5sW7dOqKjo5k9e3aexM6dtnNcc801/PTTT4gI77zzDv/4xz+YPHkyI0eOpGrVqowe
PRqApUv/mB9o8ODBTJkyha5du/Lss88yYcIEXn31VcDR27Zq1Sq+/PJLJkyYwJIlS0r8GXpJgKrm
rrirqunOmWrPiYj4A2/iGNK+B1gtIotUNaH4mkUbt8gxjXniizeVql7CvhO5zz/8eRcxF4fTqUkd
AKqXgx4sY4ypLCp1gpXD1sEyxrgrKiqKxMREZs2aRe/evfPsS05OZsiQIWzbtg0RISMjo0D9FStW
5CZd3bt3JykpiRMnHB+MY2NjCyRXOQYMGMDs2bNp1aoVCxYsYMKECaVqO8eePXvo168f+/fvJz09
ncaNGxd7vcnJyRw/fpyuXbsCMGTIEO68887c/bfffjsA7dq1IzExsdi2vOywiMSq6iIAEbkVOHIe
2m0PbFfVHc52ZwO3AmeVYKmWnOQX5t0ffufNb7fnKXty7gYWjuoEQE1LsIwxpsxU6gSrVq1a3HLL
LbRp08bboRhjSsONniZPio2NZfTo0SxbtoykpD9Wl/jrX//Ktddey/z580lMTKRbt26lardKlaLX
Berfvz/XX389Xbt2JSoqinr16p1V7A899BB/+ctfiI2NZdmyZYwfP/6s2skRHBwMgL+/f3n/smok
8KGIvAEIsBsYfB7abeBsK8ce4KriKmzZsqXA38Zdd93Fgw8+SFLyydwy12OGDh3K0KFDOXLkCHfc
cUeeutn+Qey68s8A1Az15/hpx7To6VnZDPzLJLioK5vWxXFLdH22bNnC/fffXyCmZ555hh49erB2
7VoeeeSRAvuff/55OnbsyMqVK3n66acL7H/11Vdp06YNS5YsYdKkSQX2T5s2jebNm/PZZ58xefLk
Avs/+OADGjZsyJw5c5g6dWqB/XPnzqVOnTrMnDmTmTNnFtj/5ZdfEhYWxltvvcXHH39cYP+yZcsA
ePnll/n888/z7AsNDeW///0vAH/729/y9N4C1K5dO/eLi6eeeooff/wxz/7IyEj+8x/H7d+PPPJI
gV7jZs2aMX36dABGjBjB1q15J01u06ZNbo/w3XffzZ49e/Ls79ChAy+88AIAffv2zfOaA3Ddddfx
17/+FYBevXpx+vTpPPtvvvnm3J7pwl6Tcv72UlNTC3xpBMX/7QE88MAD9OvXj927dxcY3gzw2GOP
ccstt9jfnv3tkZ8v/O2drUp9D1bjxo2ZOXNm7jewxhjjjmHDhjFu3Dhat26dpzw5OTl30ovC3ogB
OnfuzIcffgg43njr1KlD9eolz6jWpEkT6tSpw5gxYwodHuhu264xvvfee7nl1apVIyUlpUCbNWrU
IDw8PHcY5AcffJDbm+VLVPU3Vb0aaAlcpqodgYIX7CEiMkJE1ojImsJ6NnOcOFP0vqKkh0XkPm8e
EZZnX1rVCwCoUqm/TjXGmLIlZzscoSKIiYnRNWvWlHygMcbrNm3axGWXXebVGKpWrcrJkyfzlC1b
tiz3m8kff/yRIUOGUKVKFW666Sb+85//kJiYyLJly5g8eTKfffYZR48eZdiwYezYsYOwsDCmT59O
VFQU48ePz3MPVGFeffVVxowZw8GDB6lRo0aB8xfV9syZM1mzZg1vvPEGCxcu5NFHHyU8PJzu3buz
evVqli1bxtatW7njjjvw8/NjypQpLF26NDeetWvXMnLkSFJTU7nkkkt49913CQ8Pp1u3brz88svE
xMRw5MgRYmJizsswwcJ+1yISp6rnPF2iiNQE+uJYG+syVa1/ju11AMar6g3O7acAVPWFouoU996z
af8Jer3mSGY/f+gaDp9M47Ul25g7sgMB/oV/J/pV/AHu/yAOgD5tGzD/170A+AlcUD2Efcln+O7x
blxUu+geUmOMMQWd7XuPJViWYBnjE8pDgnW25s2bx6JFi/L0GJmine8Eyzkl+604kqq2OBYZvg1Y
rqrZxdV1o+0AYCtwHbAXx1qMA1U1vqg6xb33/LQjif7TfypQvmrsddStVvgiwXNW7+LJeRsAeL5P
aw6lnKFqcAAzVyay55hjyM7mv92YZ6FfY4wxJTvb9x4bNGCMMR60aNEixo4dy4wZM7wdSqUkIh8B
nYGvgSnANzgmpVh2PtpX1UwRGQV8BfgDM4pLrkriOtW6qxOnM6lbrfA6R0856nw0/Co6NKmdu8bU
si2H2XPsNHWrBVtyZYwxZahS34N1+PBhpkyZwuLFi70dijGmgoqNjWXz5s25C/iaMtcSOAZsAjap
ahZwXoduqOqXqtpMVZuo6nNn287n6/excK1jeF9keN7ZJItKvACOpaYTHOBHx0vr5FnA9+I6jvux
GoQXPjOlMcYYz6jUCdauXbt4+OGHmTVrlrdDMcYY4wGq2ga4C8ewwCUisgKoJiJnNw2jB4366Fe+
3HAAgOjImnn2nSgmwTp6Kp1ahSzeGxnuSLDCy8Eiw8YYU5lU6gQrRzmfWtgYY8w5UNXNqjpOVVsA
fwbew7Eg8Eovh1ak/LMJFteDtfVgCnWrF7w/q2uzCNo3rsXTvX3z3kVjjPFVlTrBioiIYNSoUfTo
0cPboRhjjCkDqhqnqqOBi4Ax3o6nKO0uCgcgPMyxQHBRCdaKbUdYvyeZPm0KToZ42YXV+fj+Dlxa
t6rnAjXGGFNApZ7kolGjRkyZMsXbYRhjjClj6phCd7m343AVGujP6QzHIsGjrr2UvldEUq96CM2e
+W+RCdaHP++kbrVg+rdvVJahGmOMKUal7sEyxpjSEBHuvvvu3O3MzEwiIiK4+eabAceMgS+++KLH
zr927VpEhP/9739n3UZRk20MHTqUuXPnnnVcX3755VnHZBwysx0zxterHkyAvx8Na4URFOBHcIAf
p9IKH8q+P/kMzepVs1kCjTGmHLEEyxhj3FSlShU2btzI6dOOtYUWL15MgwYNcvfHxsYyZsy5jzor
6r7QWbNmcc0115zTxDwrV57/247Ke4IlIn4icpe34yhOZlY2GVnKwKsa8fUjXfPsCwrwIy2z8OW6
DqekUbd6cFmEaIwxxk2WYBljTCn07t2bL774AnAkPAMGDMjdN3PmTEaNGgU4eoQefvhhOnbsyCWX
XJLbO6SqPP7441x++eW0bt2aOXPmALBs2TI6d+5MbGwsLVu2LHBeVeWTTz5h5syZLF68mDNnzuTu
e//994mKiiI6Opp77rkHgIMHD9KnTx+io6OJjo7OTayqVq2a296oUaNo3rw5PXr04NChQ7ntxcXF
0bVrV9q1a8cNN9zA/v37AejWrRtPPvkk7du3p1mzZnz//fekp6fz7LPPMmfOHNq0aZN7PeWJczHh
J7wdR3HOOBOoi2uHUcN531WO4AD/AgnWvuOnWbv7OIdSzhS5ALExxhjvqNT3YB04cIDnn3+eRo0a
5Rn2Y4zxAd26FSy76y548EFITYXevQvuHzrU8ThyBO64I+++ZcvcOm3//v2ZOHEiN998M+vXr2fY
sGF8//33hR67f/9+VqxYwebNm4mNjeWOO+7g008/Ze3ataxbt44jR45w5ZVX0qVLFwB++eUXNm7c
SOPGjQu0tXLlSho3bkyTJk3o1q0bX3zxBX379iU+Pp5JkyaxcuVK6tSpw9GjRwF4+OGH6dq1K/Pn
zycrK4uTJ0/maW/+/Pls2bKFhIQEDh48SMuWLRk2bBgZGRk89NBDLFy4kIiICObMmZNnoeTMzExW
rVrFl19+yYQJE1iyZAkTJ05kzZo1vPHGG279DL1kiYiMBuYAp3IKVfWo90L6wxnnvVeFDfULDvAj
PV+C1W/6j+w+6uhJrWc9WMYYU65U6gRr7969jB07lmuvvdYSLGOMW6KiokhMTGTWrFn0LiyJc3Hb
bbfh5+dHy5YtOXjwIAArVqxgwIAB+Pv7U69ePbp27crq1aupXr067du3LzS5AkdvWf/+/QFHkvf+
++/Tt29fvvnmG+68807q1KkDQK1atQD45ptveP/99wHw9/enRo0aedpbvnx5bhz169ene/fuAGzZ
soWNGzfSs2dPALKysrjwwgtz691+++0AtGvXjsTERLd/buVAP+e//+dSpsAlXoilgNwEK6BgghUU
4Ed6Vt4E60DyHz2YUZE18lcxxhjjRZU6wcph62AZ44OK63EKCyt+f506bvdYFSY2NpbRo0ezbNky
kpKSijwuOPiPngXHpHXFq1KlSqHlWVlZzJs3j4ULF/Lcc8+hqiQlJZGSklL64EugqrRq1Yoff/yx
0P051+Tv7+9Tr52qWnjmWk6cyXAkUMGBBUfuB/n7kZ6Zlacs5qJa/LgjiTcGtqXdRbXKJEZjjDHu
qdT3YNWrV48xY8ZY75UxplSGDRvGuHHjaN26danrdu7cmTlz5pCVlcXhw4dZvnw57du3L7bO0qVL
iYqKYvfu3SQmJrJz50769u3L/Pnz6d69O5988kluopczRPC6665j6tSpgCNBS05OztNmly5dcuPY
v38/3377LQDNmzfn8OHDuQlWRkYG8fHxxcZXrVo1jyR755OIhInIMyIy3bndVERu9nZcOYodIhhY
cJKL4EA/oiJrcHNUwfWvjDHGeFelTrAiIyN54YUXGDFihLdDMcb4kMjISB5++OGzqtunT5/cCSm6
d+/OP/7xDy644IJi68yaNYs+ffrkKevbty+zZs2iVatWjB07lq5duxIdHc1f/vIXAF577TW+/fZb
WrduTbt27UhISCgQR9OmTWnZsiWDBw+mQ4cOAAQFBTF37lyefPJJoqOjadOmTYkzD1577bUkJCSU
20kunN4F0oGceer3ApO8F05eOQlWaCEJlqMHK2+ClZ6ZTXBApX4LN8aYckvcGbZSUcXExOiaNWu8
HYYxxg2bNm3isssu83YYpgwU9rsWkThVjTnbNkVkjarGiMivqtrWWbZOVaPPMdxSK+y9Z8W2I9z9
75/5+P4OtG+cd8jfwH/9RHpmNnMf+GMNs75TVxIa6M9/hl9VJjEbY0xldLbvPfb1lzHGmMogXURC
cUxsgYg0AdK8G9IfUs5kAIX3YAUXMslFWmYWQdaDZYwx5ZJNcmGMMaYyGAf8D2goIh8CnYChXo3I
KTtb+WjVLkIC/Whar2qB/UEBfqRl5EuwMmyIoDHGlFeVOsHat28fTz31FFWqVOGZZ57xdjjGGGM8
RFUXi8gvwNWAAH9W1SNeDguAH3ck8f22I3RsUrvQSS6CAvwL6cGyBMsYY8qrSv3qvH//fl588UXe
eustb4dijDHG87oC1wHXAp29HEuugycca1pNiG1V6P7CFhp2THJRMBkzxhjjfR5NsETkRhHZIiLb
RWRMIfsHich6EdkgIitFJNpZHiIiq0RknYjEi8gElzpzRGSt85EoImtd2lrr8sgWkTbuxJmRkXG+
LtkYY0w5JCJvASOBDcBG4H4RedO7UTkcT3W8B0VUCy50f1BAwWna0zKzCl0zyxhjjPd5bIigiPgD
bwI9gT3AahFZpKqucwX/DnRV1WMi0guYDlyF48bj7qp6UkQCgRUi8l9V/UlV+7mcYzKQDKCqHwIf
OstbAwtUdW1xMV544YWMGjWKqlULjnk3xhhToXQHLlPn1Lki8h5Q/AJfZeR4ajoiUC0ksND9Qf5+
pOVbaDgtM5sgf0uwjDGmPPLkq3N7YLuq7lDVdGA2cKvrAaq6UlWPOTd/AiKd5aqqJ53lgc5Hnvnk
RUSAu4BZhZx7gPN8xapfvz5PP/30Wa9nY4ypXEQkz8LkmZmZREREcPPNxa9Xu2bNmvPyOvPqq68S
EhJSYNFgdxUXx8UXX8yRI2d3S9KCBQsKrLNVDm0HGrlsN3SWed2x1AxqhAbi7yeF7i9siGBaZrb1
YBljTDnlyVfnBsBul+09zrKi/An4b86GiPg7h/8dAhar6s/5ju8MHFTVbYW01Y/CEy9EZISIrBGR
NYcPH3bjMowxxqFKlSps3LiR06dPA7B48WIaNCjuZc0hJiaG119/3e3zZGZmFlo+a9YsrrzySj79
9FO32zqXONzlIwlWNWCTiCwTkW+BBKC6iCwSkUXeDOxYajrhYUFF7s+Zpj1n3crMrGyystXuwTLG
mHKqXHz9JSLX4kiwnswpU9UsVW2Do1ervYhcnq/aAApJokTkKiBVVTcWdi5Vna6qMaoaExERcd6u
wRhTOfTu3ZsvvvgCcCQ8AwYMyN23atUqOnToQNu2benYsSNbtmwBYNmyZbm9XEePHuW2224jKiqK
q6++mvXr1wMwfvx47rnnHjp16sQ999xT4Ly//fYbJ0+eZNKkScya9cdLX1ZWFqNHj+byyy8nKiqK
KVOmALB69Wo6duxIdHQ07du3JyUlJU8cSUlJXH/99bRq1Yrhw4fnfngH+M9//kP79u1p06YN999/
P1lZjuFpVatWZezYsURHR3P11Vdz8OBBVq5cyaJFi3j88cdp06YNv/3223n7WZ9nzwK9cEzXPh7o
7Syb7Hx4zfHUDGqGFT48ECAkyB9VOOOcqj1nRkGbRdAYY8onT74678UxBCNHpLMsDxGJAt4BblXV
pPz7VfU48C1wo0udAOB2YE4h5+1PEb1XxpgKQsQzDzf079+f2bNnc+bMGdavX89VV12Vu69FixZ8
//33/Prrr0ycOJGnn366QP1x48bRtm1b1q9fz/PPP8/gwYNz9yUkJLBkyZI8CVSO2bNn079/fzp3
7syWLVs4ePAgANOnTycxMZG1a9eyfv16Bg0aRHp6Ov369eO1115j3bp1LFmyhNDQ0DztTZgwgWuu
uYb4+Hj69OnDrl27ANi0aRNz5szhhx9+YO3atfj7+/Phhx8CcOrUKa6++mrWrVtHly5d+Ne//kXH
jh2JjY3lpZdeYu3atTRp0sStn2NZU9Xvint4M7bjp9OpGVp0glWnimPyiyMnHesi56yJZQmWMcaU
T55cB2s10FREGuNIrPoDA10PEJFGwKfAPaq61aU8AshQ1eMiEopjooy/u1TtAWxW1T352vPDcV+W
W9Pv7t27l4ceeoj09HReeeUVwsLCSn2RxpjKJSoqisTERGbNmkXv3r3z7EtOTmbIkCFs27YNESl0
htIVK1Ywb948ALp3705SUhInTpwAIDY2tkAilGPWrFnMnz8fPz8/+vbtyyeffMKoUaNYsmQJI0eO
JCDA8XJeq1YtNmzYwIUXXsiVV14JQPXq1Qu0t3z58tyhhjfddBPh4eEALF26lLi4uNy6p0+fpm7d
ugAEBQXl9oC1a9eOxYsXl+InZ4qSfDqDpnWrFbm/TjXH8MEjJ9NoWCuMM84JL4JsiKAxxpRLHkuw
VDVTREYBXwH+wAxVjReRkc79b+MYnlEbeMsxZwWZqhoDXAi855yJ0A/4WFU/d2m+qF6qLsBuVd3h
ToyHDx/mjTfeAOC5556zBMsYX6Fa8jEeFBsby+jRo1m2bBlJSX90vP/1r3/l2muvZf78+SQmJtKt
W7dStVulSpVCyzds2MC2bdvo2bMnAOnp6TRu3JhRo0ad9TUURVUZMmQIL7zwQoF9gYGBOF+r8ff3
L/JeMVM6x52TXBSlTtWcHqx0AE6nOxKssCBLsIwxpjzy6PgCVf1SVZupahNVfc5Z9rYzuUJVh6tq
uKq2cT5inOXrVbWtqkap6uWqOjFfu0Nz2shXvkxVr3Y3PnEZEpSenn6WV2mMqWyGDRvGuHHjaN26
dZ7y5OTk3EkvZs6cWWjdzp075w65W7ZsGXXq1Cm0h8nVrFmzGD9+PImJiSQmJrJv3z727dvHzp07
6dmzJ9OmTctNdo4ePUrz5s3Zv38/q1evBiAlJaVAMtSlSxc++ugjAP773/9y7JhjQtfrrruOuXPn
cujQodz2du7cWWx81apVIyUlpdhjTOGyspWUM5luJliOIYKpzgQr1BIsY4wplyr1AO769evz+uuv
8/bbb5f4AccYY3JERkYWOt35E088wVNPPUXbtm0LJDQ5X+iMHz+euLg4oqKiGDNmDO+9916J55s9
ezZ9+vTJU9anTx9mz57N8OHDadSoEVFRUURHR/PRRx8RFBTEnDlzeOihh4iOjqZnz56cOXMmT/1x
48axfPlyWrVqxaeffkqjRo4ZzFu2bMmkSZO4/vrriYqKomfPnuzfv7/Y+Pr3789LL71E27Zty90k
F86F7NcX9fB2fCdOO4aRFjfJRe2qjiGCh044EqwzGc4EK9ASLGOMKY9EvTzUxptiYmJ0zZo13g7D
GOOGTZs2cdlll3k7jLMyb948Fi1a5FYyZQr/XYtIXM4oh9IQkYucT//P+e8Hzn8HAajqmLON82y5
vvf8fuQU1768jH/eFc3tV0QWXWfSYnpcVo8X+0axfOthBs9YxdyRHYi5uFZZhW2MMZXO2b73eHKS
C2OMqfQWLVrE2LFjmTFjhrdDqZRUdSeAiPRU1bYuu8aIyC9AmSdYrpLd6MECaFAzlB1HTnHwxBkb
ImiMMeVcpR4iaIwxnhYbG8vmzZvp2LGjt0Op7EREOrlsdKQcvAceT3Xc/1vcPVgADcJDWfX7Ua56
fik/bD8C2BBBY4wpr6wHyxjjM1Q1z+Q0puLx4LD1YcC7IlLDuX3cWeZVOT1YNUKDij0uMvyPtejr
DQAAIABJREFUWW4/+Mkx6UhYkL2FG2NMeeT1b++86cCBAwwdOpSBAweyefNmb4djjClGSEgISUlJ
nvwAbrxMVUlKSiIkJOS8tutcI/FSVY0GooFo58y1v5zXE52FPxKs4nuwYqPrFyizIYLGGFM+Veqv
v5KTk3NvOr/vvvto0aKFlyMyxhQlMjKSPXv2cPjwYW+HYjwoJCSEyMiiJ3s4G6qaLSJP4FhTMfl8
tSsiLwG3AOnAb8C9qnq8NG0cT3Uvwbq8QQ0e7dGMV5ZszS2zIYLGGFM+VeoEy3WoUVpamhcjMcaU
JDAwkMaNG3s7DOO7lojIaGAOcCqnUFWPnkObi4GnVDVTRP4OPAU8WZoGkk9nUCXIn6CAkgeUXNO0
Nq8s+WPbnTrGGGPKXqVOsC644AJee+01AgICCiwYaowxpkLp5/z3/1zKFLjkbBtU1a9dNn8C7iht
G8dTM0rsvcoRHlb8fVrGGGPKh0qdYFWvXp17773X22EYY4zxMFX1dPfnMBy9Y4USkRHACCB3UWeA
5NPp1HAzcarpcly/mIZnGaYxxhhPq9QJljHGmMpDRC4HWgK5s2io6vsl1FkCXFDIrrGqutB5zFgg
E/iwqHZUdTowHRwLDeeUJ51Kp1YV93qwalUJYvKd0XRuWoe61c/vRCDGGGPOH0uwjDHGVHgiMg7o
hiPB+hLoBawAik2wVLVHCe0OBW4GrtOzmOJy3/HTdG4a4fbxfdud3wlAjDHGnH92h6wxxpjK4A7g
OuCAqt6LY7r2GsVXKZ6I3Ag8AcSqampp62dkZXMoJY36NUPPJQxjjDHlTKVOsA4fPswdd9zBbbfd
xtKlS70djjHGGM85rarZQKaIVAcOAed6I9MbQDVgsYisFZG3S1P5QPIZVKFBTRvuZ4wxFUmlHiJ4
6tQp5s2bB8Att9zi5WiMMcZ40BoRqQn8C4gDTgI/nkuDqnrpudTfddTR6dWgZti5NGOMMaacqdQJ
lus6WJmZmV6MxBhjjCep6oPOp2+LyP+A6qq63psxxe9zrHncsn51b4ZhjDHmPKvUCVZERARTp04l
ICCA6Ohob4djjDHGQ0TkA2A58L2qbvZ2PAAb956gQc1QalWx9a2MMaYiqdQJVlhYGHfcUep1IY0x
xvieGUBnYIqINAF+BZar6mveCmjrwRSaX1DNW6c3xhjjIZU6wTLGGFM5qOq3IrIcuBK4FhgJtAK8
kmBlZmWz4/ApujZzf4p2Y4wxvsESLGOMMRWeiCwFquCY2OJ74EpVPeSteHYfO016VjZN6lb1VgjG
GGM8pFJP026MMabSWA+kA5cDUcDlIuK1Bai2HUwBoKklWMYYU+FU6gTr6NGj3HDDDVx33XW8++67
3g7HGGOMh6jqo6raBbgdSALeBY57K57th08CWA+WMcZUQJV6iGBaWhpff/01AB06dPByNMYYYzxF
REbhmOSiHZCIY9KL770Vz/ZDJ7mgegjVQwK9FYIxxhgPqdQJlq2DZYwxlUYI8E8gTlW9/oIfv/cE
LS60GQSNMaYiqtQJVnh4OO+88w4BAQFcfPHF3g7HGGOMh6jqyyJyDXAP8K6IRABVVfX3so7lUEoa
Rw6mcMPlF5T1qY0xxpSBSp1gBQcHc+ONN3o7DGOMMR4mIuOAGKA5jvuvAoH/AJ3KOpbk0xmEA9e1
qFvWpzbGGFMGKvUkF8YYYyqNPkAscApAVfcBXhmjF+AnXNGoJtENa3rj9MYYYzzMowmWiNwoIltE
ZLuIjClk/yARWS8iG0RkpYhEl1RXRP7mrLNWRL4Wkfouba11eWSLSBtPXp8xxhifka6qCiiAiFTx
ViDZqoQE+nvr9MYYYzzMYwmWiPgDbwK9gJbAABFpme+w34Guqtoa+Bsw3Y26L6lqlKq2AT4HngVQ
1Q9VtY2z/B7gd1Vd66nrM8YY41M+FpFpQE0RuQ9YArzjjUCyFUItwTLGmArLk/dgtQe2q+oOABGZ
DdwKJOQcoKorXY7/CYgsqa6qnnCpUwXnt5H5DABmlxRgcnIynTp1IjMzk169ejF+/Hh3r80YY4wP
cU5y0RM4geM+rGdVdbGXYrEeLGOMqcA8mWA1AHa7bO8Brirm+D8B/3Wnrog8BwwGkoFrC2mrH46E
rAARGQGMAKhduzbbt28HoFmzZsWEZowxxtc5E6rFACLiJyKDVPXDso4jW7EEyxhjKrByMcmFiFyL
I8F60p3jVXWsqjYEPgRG5WvrKiBVVTcWUXe6qsaoakz16tVzy20dLGOMqXhEpLqIPCUib4jI9eIw
CtgB3OWNmBw9WOXi7dcYY4wHeLIHay/Q0GU70lmWh4hE4RgH30tVk0pTF0eC9SUwzqWsPzDLnQCr
V6/OihUrCAgIoE6dOu5UMcYY41s+AI4BPwLDgacBAW7z1n26dg+WMcZUbJ5MsFYDTUWkMY7kqD8w
0PUAEWkEfArco6pb3akrIk1VdZvzuFuBzS7t+eH4RrKzOwEGBATQqVOZL4FijDGm7FzinEgJEXkH
2A80UtUz3gooW5XQIEuwjDGmovJYgqWqmc5hGF8B/sAMVY0XkZHO/W/jmAGwNvCWiABkOofvFVrX
2fSLItIcyAZ2AiNdTtsF2J0zOYYxxphKLyPniapmicgebyZXOeweLGOMqbg82YOFqn6JYwifa9nb
Ls+H4xiy4VZdZ3nfYs63DLj6LMM1xhhT8USLSM7sswKEOrcFUFWtXnRVz7EEyxhjKq4SEywRqe1y
b5QxxhjjM1S1XGYyNsmFMcZUXO68wv8kIp+ISG9xjuOrKE6dOkVUVBQtW7bknnvu8XY4xhhjKomQ
gHKZ9xljjDkP3Bki2AzoAQwDXheRj4GZ+Sal8EmqyoYNGwDHmljGGGNMWagSbAmWMcZUVCX2YKnD
YlUdANwHDAFWich3ItLB4xF6kGuHXFpamhcjMcYYU5mEBXn0FmhjjDFe5NY9WMDdwD3AQeAhYBHQ
BvgEaOzJAD0pNDSUNWvWEBYWhuuiw8YYY4wnVQm2BMsYYyoqd17hf8SxUONtqrrHpXyNiLxdRB2f
4OfnR7t27bwdhjHGmErGhggaY0zF5U6C1VxVtbAdqvr38xyPMcYYU+FVsSGCxhhTYbkzi+DXIlIz
Z0NEwkXkKw/GZIwxxlRoNkTQGGMqLncSrAhVPZ6zoarHgLqeC8kYY4yp2MKCbIigMcZUVO4kWFki
0ihnQ0QuAgodMuiLWrZsScOGDalduzaZmZneDscYY0wlEBxgCw0bY0xF5c4YhbHAChH5DhCgMzDC
o1GVoV27dnHq1CkATp8+TbVq1bwckTHGGF8iIo8BL+MY8XGkpOP9RPIsE2KMMaZiKTHBUtX/icgV
wNXOokfceQPxFWFhYZZgGWOMOSsi0hC4Htjlbh1/P0uujDGmInP3Lttg4Kjz+JYigqou91xYZWfl
ypUEBQURGhpK7dq1vR2OMcYY3/IK8ASw0N0KIYE2PNAYYyoydxYa/jvQD4gHsp3FClSIBOvSSy/1
dgjGGGN8kIjcCuxV1XWlGfJ3ce0qngvKGGOM17nTg3UbjrWw0jwdjDHGGFOeiMgS4IJCdo0FnsYx
PNCddkbgvH+5UaNGJRxtjDHGl7mTYO0AAgFLsIwxxlQqqtqjsHIRaQ00BnJ6ryKBX0SkvaoeKKSd
6cB0gJiYmAozE68xxpiC3EmwUoG1IrIUlyRLVR/2WFTGGGNMOaaqG3BZE1JEEoGYijQJlDHGmLPj
zp22i4C/ASuBOJdHhTBo0CDCw8MJCwvjiy++8HY4xhhjjDHGGB/mzjTt74lIKNBIVbeUQUxlKjU1
lePHjwOQlmajII0xxpSeql7s7rFxcXFHRGSnB8M53+oAvt4z5+vX4Ovxg11DeeHr11DW8V90NpXc
mUXwFhwLKAYBjUWkDTBRVWPP5oTlTXBwcO7z9PR0L0ZijDGmMlDVCG/HUBoiskZVY7wdx7nw9Wvw
9fjBrqG88PVr8JX43RkiOB5oDxwHUNW1wCUejKlMTZs2jaNHj3Ly5EnuvPNOb4djjDHGGGOM8WHu
THKRoarJ+db4yC7qYF9To0YNb4dgjDHGGGOMqSDcSbDiRWQg4C8iTYGHcUx4YYwxxpiKb7q3AzgP
fP0afD1+sGsoL3z9GnwiflEtfjkOEQnDsaDi9YAAXwF/U9Uzng/Ps2JiYnTNmjXeDsMYY0wJRCTO
F8bdG2OMMe7MIpiKI8Ea6/lwvCc7OxtVxd/f39uhGGOMMcYYY3xUiZNciMi3IvJN/kdZBFcWJk6c
SEBAAP7+/jz33HPeDscYY4wxxhjjw9yZRXA08Ljz8VdgLeDWuDoRuVFEtojIdhEZU8j+FiLyo4ik
icjofPtqishcEdksIptEpIOzPNpZZ4OIfCYi1Z3lgSLynrN8k4g85U6Mfn5+ZGVlATZNuzHGmMpH
RGaIyCER2ehSVktEFovINue/4S77nnK+r28RkRu8E/UfRKSh88vgBBGJF5E/O8t96RpCRGSViKxz
XsMEZ7nPXAOAiPiLyK8i8rlz29fiT3R+jlwrImucZb52DQU+P/vSNYhIc+fPP+dxQkQe8aVrADcS
LFWNc3n8oKp/AbqVVE9E/IE3gV5AS2CAiLTMd9hRHJNmvFxIE68B/1PVFkA0sMlZ/g4wRlVbA/Nx
JH4AdwLBzvJ2wP0icnFJcQYFBeU+z8zMLOlwY4wxpqKZCdyYr2wMsFRVmwJLnds438f7A62cdd5y
vt97UybwmKq2BK4G/s8Zpy9dQxrQXVWjgTbAjSJyNb51DQB/5o/Pa+B78QNcq6ptXO759LVrKOzz
s89cg6pucf782+D4PJ+K4/O+z1wDuDdEsJbLo44zM3RnbvP2wHZV3aGq6cBs4FbXA1T1kKquBjLy
nbMG0AX4t/O4dFU97tzdDFjufL4Y6JvTHFBFRAKAUCAdOFFSkI8++ijp6elkZ2fz4osvunFZxhhj
TMWhqstxfOHp6lbgPefz94DbXMpnq2qaqv4ObMfxfu81qrpfVX9xPk/B8YGyAb51DaqqJ52bgc6H
4kPXICKRwE04vgjP4TPxF8NnrqGYz88+cw35XAf8pqo78bFrcGeIYByOIYFxwI/AY8Cf3KjXANjt
sr3HWeaOxsBh4F1nV/M7IlLFuS+ePxK1O4GGzudzgVPAfmAX8LKq5n/DQERGiMgaEVlz+PBhAgMD
CQwMRPKu82WMMcZUZvVUdb/z+QGgnvP5uby3e5xz5Epb4Gd87Bqcw+vWAoeAxarqa9fwKvAEeddK
9aX4wZHULhGROBEZ4SzzpWso6vOzL12Dq/7ALOdzn7oGd4YINlbVS5z/NlXV61V1hYfjCgCuAKaq
alsciVPOPVzDgAdFJA6ohqOnChzZahZQH8cf2GMickkh1zNdVWNUNSYiIsLDl2GMMcb4NnWs51L8
mi7lgIhUBeYBj6hqnhEsvnANqprlHBYVCbQXkcvz7S+31yAiNwOHVDWuqGPKc/wurnH+DnrhGGra
xXWnD1xDcZ+fAZ+4BgBEJAiIBT7Jv88XrqHEadpF5Pbi9qvqp0Xs2ssfvUvgeMHY62Zce4A9zm9v
wNE7NcZ5vs041uRCRJrh6I4GGIhjzGkGcEhEfgBigB1untMYY4wxDgdF5EJV3S8iF+LoVYFze2/3
GBEJxJFcfejyucSnriGHqh4XkW9x3E/iK9fQCYgVkd5ACFBdRP6D78QPgKrudf57SETm4/jy3peu
oajPz750DTl6Ab+o6kHntk9dgztDBP+EYyznIOfjHRy9SLcANxdTbzXQVEQaO7PQ/sAid4JS1QPA
bhFp7iy6DkgAEJG6zn/9gGeAt53H7AK6O/dVwXGj6+aSzpWdnc2ZM2c4ceIEKSkp7oRnjDHGVHSL
gCHO50OAhS7l/UUkWEQaA02BVV6IL5c4xvj/G9ikqv902eVL1xAhIjWdz0OBnjg+w/jENajqU6oa
qaoX4/i8942q3o2PxA+Oz44iUi3nOY4v8zfiQ9dQzOdnn7kGFwP4Y3gg+No1qGqxD+Br4EKX7QuB
r0qq5zy2N7AV+A0Y6ywbCYx0Pr8AR7Z9AjjufF7dua8Njnu/1gMLgHBn+Z+dbW4FXgTEWV4VRzdi
PI4/psdLiq9du3Y6f/78nG5GvfXWW9UYY0z5A6xRN9537FH6B44PMftxTDi1B8cXq7VxzNS1DVgC
1HI5fqzzfX0L0KscxH+N8318PY6lZNY6P3/40jVEAb86r2Ej8Kyz3GeuwSWubsDnvhY/cAmwzvmI
d/nc6jPX4IypwOdnH7yGKkASUMOlzKeuISc5KZKIbFLVy1y2/YB41zJfFRMToxMnTuSmmxyjDHv1
6sWXX37p5aiMMcbkJyJx+se0ycYYY0y5VeI9WMBSEfmKP7rp+uHIHCuEnHWwatasSe3atb0cjTHG
GGOMMcaXlZhgqeooEemDY159gOmqOt+zYZWdrl27kp6eTmBgoLdDMcYYY4wxxvg4d3qwAH4BUlR1
iYiEiUg1dSzm5/MssTLGGGOMMcacLyXOIigi9+GY5nGas6gBjpvmjDHGGGOMMca4cGea9v/Dsb7B
CQBV3QbU9WRQxhhjjDHGGOOL3Emw0lQ1PWdDRAIo56snl1Z6ejr79u1j3bp1pKamejscY4wxxhi3
iUhtEVnrfBwQkb0u20H5jv0qZ72nYtrbk7MuVyHlc1y2+4vIO+fpGiaJyCPnoy1jvM2dBOs7EXka
CBWRnjjWmvrMs2GVrQ4dOtCgQQPatGnDxo0bvR2OMcYYY4zbVDVJVduoahvgbeCVnO2cL8nFwU9V
bzjH++ivclnItlzIuTZvx2FMDnf+GMcAh4ENwP3Al8AzngyqrEVGRuY+//33370YiTHGGGPM+SEi
l4pIgoh8iGPx3Atde6dE5DMRiROReBEZ7mazk4GnCzlXnh4oEdksIpHOGDaKyAcislVE3heRG0Rk
pYhsExHX9e3aishPzvJhLm2NEZFVIrJeRJ4t6tpK/QMyxkOKnUVQRPyB91V1EPCvsgmp7F166aXU
qFGDpk2b5q6LZYwxxhhTAbQABqvqGgARcd03RFWPikgYsEZE5qnqsRLamwWMEpHGpYihOXAXsBnH
zNRnVLWjiPTF8UX+Hc7jWgMdgerALyLyBdAOaARcBQjwpYh0BA7lvzZjyotie7BUNQu4KP/43Yrm
xRdf5NixY6xevZo+ffp4OxxjjDHGmPPlt2ISkEdFZB3wIxAJNHGjvUwcvVhjShHDdlVNUNVsIAFY
6izfAFzsctwCVT2jqoeA5cCVwPVAL+BXHMnZpUAz5/HFXZsxXuPOOlg7gB9EZBFwKqdQVf/psajK
mK2FZYwxxpgK6lRhhSLSA+gCXK2qp0VkBRDiZpszgSeArS5lmeT94t61rTSX59ku29nk/SyafxI1
xdFrNUlV/50v/ksp4tqM8TZ37sH6DfjceWw1l4cxxhhjjPFNNYCjzuSqFY7eIrc4J854HfizS3Ei
juF8iEh7oOFZxHSbiASLSATQGVgDfAX8SUSqONuOFJE6Z9G2MWWmyB4sEQlQ1UxVnVCWARljjDHG
GI/7AhghIgnAFuDnUtb/F3knu/gEuFtENgI/4RgBVVobge+A2sA4VT2I456rFsBPzvvHUoCBZ9G2
MWVGVAtf0kpEflHVK5zPp6jqQ2UaWRmIiYnRNWvWkJqaytatW8nIyCAkJITWrVt7OzRjjDEuRCRO
VWNKPtIYY4zxruLuwXKdZqaTpwPxpoSEBK680tEzfsUVVxAXF+fliIwxxhhjjDG+qLh7sArv2qqA
XCe5yMjI8GIkxhhjjDHGGF9WXA9WCxFZj6Mnq4nzOc5tVdUoj0dXRqpWrUpUVBSBgYE0bdrU2+EY
Y4wxxhhjfFRxCdZlZRaFlzVp0oR169Z5OwxjjDHGGGOMjysywVLVnWUZiK/Jzs5GRPKviG6MMcYY
Y4ypxNxZB8vkEx8fzyWXXELbtm05evSot8MxxhhjjDHGlBOWYJ2FBQsWcOrUKTZv3sz8+fO9HY4x
xhhjjDGmnCjuHqxcIhIKNFLVLR6OxysyMzP58ccfc2cQ7N69e7HHHzlyhCNHjgBw4sQJj8dnjDHG
+JK4uLi6AQEB7wCXY1/mGlORZAMbMzMzh7dr1+6Qt4Mpr0pMsETkFuBlIAhoLCJtgImqGuvp4MrK
6dOn6dKlC+CYUTAlJaXIYzds2MDvv/+eu52enu7x+IwxxhhfEhAQ8M4FF1xwWURExDE/P79Ks+yL
MRVddna2HD58uOWBAwfeASpMLnC+ufOt0nigPXAcQFXXAo09GFOZCwj4I88saR2sP/3pTyxcuBCA
AQMG0LNnT4/GZowxxvigyyMiIk5YcmVMxeLn56cRERHJOHqnTRHcGSKYoarJ+WbLq1AvmIGBgXTq
1InAwECCg4OLPXbnzj8mV+zVqxdXXHGFp8MzxhhjfI2fJVfGVEzO/9s29LcY7iRY8SIyEPAXkabA
w8BKz4ZVtgICAlixYkWJx6lqniGBgwcP5u6777ap2o0xxhhjjDGAe9nnQ0ArIA34CEgGHnGncRG5
UUS2iMh2ERlTyP4WIvKjiKSJyGiX8hARWSUi60QkXkQmuOwbLyJ7RWSt89HbWR4oIu+JyAYR2SQi
T7kTY2mICPHx8XnKevXqRe/evc/3qYwxxhhzDvz9/du1aNGiZc7j6aefvuBs2unbt+/F7777bvj5
iOmDDz6oGRcXF5Kz/cgjj9RfsGBBtfPR9i233NK4WbNmLSdMmFC3NPWOHDni/+KLL0acjxh8TVhY
WNuyPF+/fv0ucv39n4tJkybVveSSS1rFxsaW+radiRMn1k1JSbEeKA9ypwerhaqOBcaWpmER8Qfe
BHoCe4DVIrJIVRNcDjuKo0fstnzV04DuqnpSRAKBFSLyX1X9ybn/FVV9OV+dO4FgVW0tImFAgojM
UtXE0sRdkvr16zNixAimT58OwFdffXU+mzfGGGPMeRAcHJy9efPmhJKPPL8yMzPz3NvtasGCBTUz
MzOT27Vrdwbg1Vdf3Xc+zrlr166AdevWVdm1a9fG0tZNSkry//e//113zJgxh92tk5GRQWBgYGlP
VeGV9HOZM2fOziJ3ltK///3viCVLlmxt0qRJ8ZMHFGLatGn17rvvvqPVqlXLdrdOcX/XpiB3flKT
ReQCYC4wR1Xd/c/bHtiuqjsARGQ2cCuQ+2KnqoeAQyJyk2tFVVXgpHMz0PkoaSy3AlVEJAAIBdIB
j8yhPnjw4NwEK0dWVhb+/v6eOJ0xxhjjs4YNo+HGjYSdzzYvv5zUGTPYXdp6SUlJ/u3atbts4cKF
26Kjo9NuueWWxt26dUt57LHHjoSFhbUdMGDAke+++656RERExrx583bUr18/07X+woULq40ZM6Zh
VlYW0dHRqe+///7O0NBQbdCgQevY2Nij3333XfVHHnnkQEpKiv+7774bkZGRIRdffHHa3Llzf//p
p59ClyxZUvOnn36q9ve///3CefPm/fbss89eePPNNyffe++9x4pr+6677kr66quvamRmZsqcOXN2
tG3b9oxrXD169Gh26NChoBYtWrR89dVXd8XHx4fkP3+1atWyd+/eHTBs2LCLdu3aFQzwxhtv7Hzt
tdfq7d69O7hFixYtu3btemLq1Kl7Hnjggchvvvmmhojo448/vv++++479vnnn1cbN25c/Ro1amTt
2LEjJDExsdTJXFGGLRzWcOOhjef3b6Tu5akzbp1R6r+Rffv2Bdx7770X7d27Nwjgn//8567rr7/+
1Lfffhv26KOPNkpLS/MLCQnJnjlz5u/R0dFpr7/+eu0FCxaEp6am+mVlZcm4ceP2TZw4sX6tWrUy
tmzZEtq6devUBQsW/O7n50f79u2bv/zyy7u7dOmSGhYW1vZPf/rToa+//rpGSEhI9ueff769YcOG
mfHx8cEDBw5sfPr0ab8bb7zx+DvvvFMvNTX1V9cYBw4c2GjPnj3BvXr1ajpo0KAjXbp0OVlYbJmZ
mTz44IOR3377bQ0R0SFDhhxRVQ4dOhTYtWvXZuHh4Zk///zz1mnTptWaPHnyBaoqPXr0OD516tS9
4OjhGzRo0OHly5dXf/3113fdcMMNJwv7mZmCSuweVNVrgWuBw8A05xC8Z9xouwHkefHb4yxzi4j4
i8ha4BCwWFV/dtn9kIisF5EZIpLTbT8XOAXsB3YBL6vqUXfPt3TpUhYtWsS8efM4c+ZMocd8+umn
TJ48mUceyTtCcty4cXYfljHGGFOOpKWl+bkOEfzXv/4VXrt27axXXnll15AhQxpPnz49/Pjx4wGP
PfbYEYDTp0/7xcTEnNq+fXt8p06dUsaMGVPftb3U1FS5//77G8+ZM+e3rVu3JmRmZvLSSy/lDq2r
Xbt2ZkJCwqYRI0YcGzRo0LGNGzdu2rJlS0Lz5s1Pv/7663V69ux5qkePHscnTZq0Z/PmzQmtWrVK
c7ftOnXqZCYkJGwaNmzY4RdffLFe/mv97LPPtjds2DBt8+bNCTfeeOPJws4PMHLkyEadO3dO2bJl
S0J8fHzCFVdccWby5Ml7cupOmzZtz/vvv19zw4YNoZs2bYpfunTp1meffTZy586dgQAJCQlhb731
1q7zmVyVN/fff3/Dv/zlLwc3bty4af78+b+NHDnyYoDo6Ogzq1ev3rxp06aEcePG7X3iiScic+rE
x8eHLVy48LfVq1dvAdi0aVPom2++uXv79u3xu3btCl68eHHV/Oc5ffq0X4cOHU5u2bIloUOHDien
TJkSATBq1KiGDz744KGtW7cmREZGFto79dFHH+2qW7duxnfffbd13Lhxh4qKbfLkyRG7du0KSkhI
iN+6dWvC8OHDk5555plDOXV//vnnrYmJiYHjx49vsGzZsq0JCQnxv/76a5UPPvigZk6MV1111akt
W7YkWHJVOm719anqAeB1EfkWeAJ4FpjkycBUNQtoIyI1gfkicrmz92wq8DccPVZ/AyYZ5Fl0AAAg
AElEQVQDw3D0mGUB9YFw4HsRWZLTg5ZDREYAIwAaNWqUW3733Xdz4MABAPbu3Uv9+nleVwGYOXMm
n332WYHy4cOH4+dnQ1mNMcaY/M6mp+l8KGqIYJ8+fU58/PHH4U888cRFcXFxuTdW+/n5MXz48KMA
w4YNS7r99tsvda23bt26kMjIyLSoqKg0gKFDhya9+eabdXF8EczgwYOP5RwbFxcX+uyzzzZISUnx
P3XqlH/Xrl2Ti4u1pLYHDhx4DKB9+/apixYtKvF+sKLOv3Llympz5879HRwTfNWuXTvryJEjeYbf
fP/999XuuuuuowEBATRs2DDzqquuOrlixYqwGjVqZEdFRZ1q0aLFeV8A9Gx6mjzlhx9+qL5t27bQ
nO2TJ0/6Jycn+x09etS/X79+jRMTE0NERDMyMnK/We/cufOJevXqZeVst27d+lTO0L1WrVql/vbb
b0H5zxMYGKj9+/dPBmjXrt2pJUuWVAf49ddfq3799dfbAYYPH540fvz4yPx18ysqtm+++ab6yJEj
D+cMW3SNMceKFSuqXH311Sk5vbX9+vU7+t1331W95557jvv7+zN06NBj+euYkpWYFYjIZc6JJTYA
U3DMIFjiLxvYCzR02Y50lpWKqh4HvgVudG4fVNUsVc0G/oUjsQIYCPxPVTOcQw9/AGIKaW+6qsao
akxExB/3dLqOK83MzMxfDSBPz9btt9/OjBkz+O6774iMdOfHYYwxxhhvy8rKYuvWrSEhISHZSUlJ
RX7RXNqRKa73s4wYMaLxG2+8sWvr1q0JTz755L60tLRz+hY2JCREAQICAjQzM7PEwM73+XOEhYW5
fc+Or1JVfvnll02bN29O2Lx5c8KhQ4fW16hRI/vJJ59s0LVr15Rt27bFf/bZZ9vT09Nzf6b5fy7B
wcG5t7X4+/tT2O8sICBAc76cDwgIKPQYdxUX27kICgrKtvuuzo47v4AZOBYZvkFVu6nqVGcCU5LV
QFMRaSwiQUB/YJE7QYlIhLPnChEJxTFRxmbn9oUuh/YBcrqpdwHdncdUAa7OqeOOHj16cMstt9Cn
T58i18K64447ePTRR3nggQd4/vnnuffee+nSpYu7pzDGGGOMl02cOLFes2bNzsycOXPHsGHDLk5L
SxOA7OxscmYLnDlzZu327dunuNaLjo4+s3fv3qCNGzcGA7z//vu1O3funFLwDJCamurXqFGjjLS0
NJk9e3atnPKqVatmnThxosBnr9K07Y6izt+pU6eUnKGHmZmZJCUl+deoUSPr1KlTuTF16dIlZe7c
ubUyMzPZt29fwKpVq6p27tz51NnG4muuueaaEy+88ELuTIwrV64MBThx4oR/ZGRkOsC0adPqeOr8
bdq0OTlz5sxwgBkzZtQq6fjiYrvuuutOTJs2rU5GhmOk4cGDB/0BqlSpkpWcnOwH0Llz51M///xz
tf379wdkZmbyySef1OrWrZsNBzxHJaalqtrhbBpW1UwRGQV8BfgDM1Q1XkRGOve/7Zw8Yw1QHcgW
kUeAlsCFwHvOmQj9gI9V9XNn0/8QkTY4hggmAvc7y98E3hWReECAd1V1vbvxvvvuuyUeM2LECHeb
M8YYY4wX5dyDlbPdvXv35Pvvv//IBx98UCcuLm5TeHh49ty5c1PGjBlz4SuvvLIvNDQ0e9WqVVVe
euml+rVr18749NNP89xiEBYWpm+//XbinXfe2SRnIorRo0cXOvPemDFj9rVv3/6yWrVqZV5xxRUn
T5486Q8waNCgow888MDFb7/9dr25c+f+djZtu6Oo80+dOnXX0KFDL2rWrFkdPz8/3njjjZ09evQ4
1a5du5NNmzZt1b179+SpU6fuWblyZdXLLruslYjohAkT9jRq1Chz/Xq3P1L5jDNnzvjVq1cvKmf7
gQceODh9+vTdw4cPb9SsWbOWWVlZctVVV6V07Nhx15NPPnlg+PDhjf/+97/X79mz53FPxTRlypTd
gwYNavzSSy9d2L179xNVq1YtMKwvv6Jie/TRRw9v3bo1uEWLFq0CAgJ0yJAhh59++unDQ4YMOXLj
jTc2q1evXvrPP/+8ddy4cXu7du3aLGeSi7vvvttj11dZiGPCvkJ2iHysqnc5hwa6HvT/7N13eJRV
9sDx70mvJLSE0KR3UCDSLCgIUlSwIrpWEEVhUVHXXfitICgW3AUURKSKBUFlRVSQFQsrFkooUqUL
EgJIGqTP/f3xzkxmJj0kmRDO53nyzFvu+74nQTM5c+89V7AK/XXI98ILSGxsrNm4cWOpr3/00UfZ
tm0b6enpLFq0iLZt25ZhdEoppRxEZJMxJs+wb1U5bd269dCll156yttxlERISEhHz2ptSlW0lJQU
n9DQUJuPjw9z5syp/uGHH9b4+uuv9xd9ZcXaunVrrUsvvbSRt+OorArrwRpjf72hIgK50AwfPpx5
8+Y598+c0TmASimllFKq9H744YeQMWPGNDTGUK1atZyFCxce8nZMquQKTLCMMcftm48aY/7mek5E
Xgb+lveqi8OBAwfckiugwNLuSimllKr8tPdKVQb9+vVL3bNnT4UvkK3KVnFKg/QhbzLVP59jF7Rv
v/2WP/74g4yMDPr160dMTEyeNpMnT8Zms7F7t3vtjB9//JE2bdrkaa+UUkpdpGw2m018fHzyn4eg
lLpg2Ww2Aap8RcnzUVh50pHAo0ATEXGd2RiOVQK9Spk0aRJr164FYM2aNfkmWC+99BJnz+YtpNOt
W7dyj08ppZS6gPx68uTJNrVr107SJEupqsNms8nJkycjyK3irfJRWA/W+8CXwBTgWZfjKcaYP8s1
Ki8ICMhdAy4zM+8aesYY0tLSnPsLFy4kJCSE6tWLXO9PKaWUuqhkZ2cPj4+PnxsfH9+O4i0Jo5S6
MNiAX7Ozs4d7O5DKrLA5WElAEjAUQESigCAgTETCjDFHKibEinHNNdcQGRlJYGBgvr1XxhgmTZpE
eno66enp3HfffV6IUimllKr8OnfunADc5O04lFLKGwos0+5sIHIj8C+gLpAAXALsMsZc8DXJz7dM
u1JKqYpRlcq016pVyzRq1MjbYSillCrCpk2bThljapf0uuIUuZgMdAP+a4zpKCLXAn8p6YOqoilT
prBkyRKysrIYN24cd999t7dDUkopVck1atQI/XBPKaUqPxE5XJrripNgZRljTouIj4j4GGO+EZFp
pXlYVXP8+HEcK5ufOnVBraeolFJKKaWUKgfFmXiaKCJhwPfAeyIyHchbSu8i5O/v79zOysryYiRK
KaXKmojMF5EEEcm3WpZYZojIPhHZJiKdKjpGpZRSlU9xerAGAenAE8DdQATwfHkG5Q0bNmwgLi6O
jIwMunfvTmys+1D/5ORknnvuOfz8/IiMjGTcuHE88cQT3Hvvvfj5+VGnTh0vRa6UUqqcLATeAN4p
4Hx/oLn9qyvwpv1VKaXURazIBMsY49pbtagcY/Gqjz/+mJdffhmAF198Md8Ea9o0a2RkvXr1GDdu
HPXr16d+/foVHqtSSqnyZ4z5XkQaFdJkEPCOsapF/SQikSISY4w5XiEBKqWUqpQKW2g4BXAtMSj2
fQGMMaZaOcdWoQIDA53bGRkZec7n5OQ4t/38itPxp5RSqoqrB/zusn/UfixPgiUiI4ARAA0bNqyQ
4JRSSnlHYetghVdkIF6RfgJSD0JYY2JjYxk+fDiBgYFcfvnleZpGRkby2muvkZ2dTXh41f/RKKWU
KjvGmDnAHLCWCPFyOEoppcpRsbpiRORKoLkxZoGI1ALCjTEHyze0CnDuKJyJg7DG3Hjjjdx4440F
No2IiODJJ5+swOCUUkpVcseABi779e3HCmWzlVs8SimlKoEiEywReQ6IBVoCC4AA4F3givINrYKk
xZf60gULFjBp0iSys7N58MEHmTBhQtnFpZRSqrJbAYwSkSVYxS2SijP/auvRPVyz8Jryjk0ppZSX
FKcH62agI7AZwBjzh4hUnTFy6aVPsFJSUjh40OrI+/PPP8sqIqWUUpWAiHwAXAPUEpGjwHOAP4Ax
ZjbwBTAA2AecAx4ozn21B0sppaq24iRYmcYYIyIGQERCyzmminUePVi6DpZSSlVdxpihRZw3wGMl
vvHplizu/S0NGhTdVCmllPfIA1Kq64qTYC0VkbeASBF5CHgQmFuqp1VG6ScA2Lt3LytXriQjI4MW
LVpw6623ujU7cOAAU6ZMwc/PjyZNmvD0008zdOhQ+vbti7+/vxa+UEopVWz79qEJllJKVVHFWQdr
qoj0AZKx5mH90xizptwjqyj2IYLbt29n7NixANx88815Eqz4+HjmzrXyym7duvH0008TGRlJZGRk
xcarlFLqgrd/P1x7rbejUEopVR6KVUXQnlCtARARHxG52xjzXrlGVlGyUgAICgpyHkpLS8vTTNfB
UkopVRZErB4spZRSVVNhCw1XwxpbXg+rUtIa+/5TwFagaiRYOecAaNq0KaNGjSIsLIw2bdrkadas
WTPmzJlDdnY20dHRFR2lUkqpKiIgQBMspZSqysSao5vPCZFPgTPAj0BvIAoQYIwxZkuFRViOYpuI
2Tg1Cm454e1QlFJKFUJENhljYr0dR1mIiIg1TZpsJC7O25EopZQqTGnfe3wKOdfEGHO/MeYtYCjQ
Bri+qiRXTtl5hwMW1+rVq4mKiqJ69eoMGTKkDINSSilVVQUFWT1YBXy+qZRS6gJXWILlrDtujMkB
jhpj0ss/pApmHyLo8J///IcmTZrwxBNPFHlpdnY2J0+eJDExkZSUlPKKUCmlVBUSGAipqXDypLcj
UUopVR4KS7AuFZFk+1cK0MGxLSLJFRVguTM5YMtdw+rmm2/m4MGDTJs2jV27dhV6qes6WNnZ2eUW
olJKqaojMNB61XlYSilVNRVY5MIY41uRgXiF+AA2yD6H8a/GlClT3E67VhP86aefmDZtGn5+fnTr
1o1Ro0bRs2dPTpw4gZ+fH4GOd0yllFKqEK4JVo8e3o1FKaVU2SusB+u8iUg/EdkjIvtE5Nl8zrcS
kR9FJENEnirOtSJyu4jsEBGbiMS6HA8QkQUisl1EtorINUUHaP/2c9IQEZ5//nnnqRdffJGOHTs6
9w8dOsSHH37Ie++9x7p16wAIDAwkKiqKGjVqEBoaWuyfi1JKqYtXYCD4+FhrYSmllKp6yi3BEhFf
YCbQH6tAxlAR8ax//ifwV2BqCa79FbgF+N7jXg8BGGPaA32A10SkiO/PkWBZ87DCw8OdZ4YNG4aI
OPddhwDqOlhKKaVKSwQaNtQhgkopVVWVZ6bQBdhnjDkAICJLgEHATkcDY0wCkCAiA4t7rTFml/2Y
5/PaAGsd9xWRRCAW+KXACB35l72S4NixY8nOziYsLIzg4GC3pldeeSXvv/8+2dnZNGrUqFg/AKWU
Uio/zZppgqWUUlVVeSZY9YDfXfaPAl3L8dqtwE0i8gHQAOhsf3VLsERkBDAC4NJGAdZBew/Ws8/m
GcXo1KhRI02slFJKlYmWLWHxYqtUe97PC5VSSl3IqtJYt/lAa2AjcBhYD+R4NjLGzAHmAMS2CDeQ
CTnuxSyOHDlCZmYmV199NTfccANBQUGEhYXx1VdfuQ0P3LlzJ507dyY93apeb7PZ8utZU0op5WUi
Egw0NMbs8XYsAK1aQXIyxMdDTIy3o1FKKVWWyjPBOobVg+RQ336sXK41xmQDzsWrRGQ9sLfQp3gM
EQR45ZVXWL58OQAffPAB27dvB6x5V76+7oUV/f39nckVwKpVq+jfv3+hj1RKKVWxRORGrLm+AUBj
EbkMeN4Yc5O3YmrVynrdvVsTLKWUqmrKs4rgBqC5iDQWkQDgTmBFeV0rIiEiEmrf7gNkG2N2FnaN
c1yGy2LDAQEBzu0///zTuR0WFpand6pZs2YAXHvttcycOZM2bTxreCillKoEJmDN7U0EMMZsARp7
MyDXBEsppVTVUm49WMaYbBEZBawGfIH5xpgdIvKI/fxsEamDNaSvGmATkceBNsaY5PyuBRCRm4HX
gdrA5yKyxRhzPRAFrBYRG1Zv1z1FR2nvkXIZIti1a1dycnIICAigXbt2/Prrr/z444+MGDECEcHf
3585c+Zw//33IyJkZWVx8OBBmjdvfv4/NKWUUuUhyxiT5PEhmSnOhSLSD5iO9YYx1xjzksf5COBd
oCHWe+pUY8yCou5brx6EhWmCpZRSVVG5zsEyxnwBfOFxbLbLdjzW8L9iXWs/vhxYns/xQ0DLEgXo
eLPNzu3BeuKJJ3jiCedIQ+bNm8ejjz6KMdZ7cVZWFjt35naM+fn5aXKllFKV2w4RuQvwFZHmWMuD
rC/qIpclQ/pgFVvaICIrPEZHPIZV4fZGEakN7BGR94wxmYXf2+rF2rWrtN+SUkqpyqpcFxqu/HIX
Gi7IypUrycrKcjvmORdLKaVUpTYaaAtkAB8AycDjxbjOuWSIPWFyLBniygDhYnWPhWGt75hNMbRq
pT1YSilVFV3cCZa4LzScH89Fhe+44w7+8pe/lGdUSimlypAx5pwxZpwx5nJjTKx9O73oK/NdMqSe
R5s3sCrY/gFsB8YYY2yeNxKRESKyUUQ2njx5ErASrN9/h9TU0nxXSimlKquqVKa95PKpIuhq1apV
ZGRkuB378MMP87SbPHkyCQkJJCYmMnv2bEJCQso8VKWUUqUjIt+Qz5wrY0yvMrj99cAWoBfQFFgj
IuuMMckez8pdIiQ21kBuoYu9e6FTpzKIRCmlVKWgCRa4DRHcsmULmzdvJjMzk5EjRzqPd+nSheef
fz7f28ycOZP4+HgApkyZogmWUkpVLk+5bAcBt1K8YXzFWTLkAeAlY03U3SciB4FWeCxynx/XSoKa
YCmlVNVxcSdY5B0iuGLFCp577rk8LTds2MD111+f710iIyOdCVZiYiL16nmOIFFKKeUtxphNHod+
EJEiEyBclgzBSqzuBO7yaHME6A2sE5ForGJLB4oTV7Nm4OurhS6UUqqqubgTrHx6sFzXwXI1duzY
Am8zduxYUlNTiYyMJEZXjFRKqUpFRGq47PoAnYGIoq4rznIjwCRgoYhsBwT4mzHmVHHiCgyEJk00
wVJKqarm4k6wHD1YLmXaO3TokKfVoEGDmDp1KlOnTgVg06ZNdHIZzzF8+PDyDVMppdT52IQ1B0uw
hgYeBIYV58JiLDfyB9C3tIG1bQs7dpT2aqWUUpWRVhEEtx6sAQMGOLeHDBnCjh07nImVQ6qWfFJK
qQuGMaaxMaaJ/bW5MaavMeZ/3o4LoH17q8hFWsGrhSillLrAXNw9WJK3B+vMmTPO7WXLlrF48WIS
EhLcLqtWrVqFhKeUUqr0ROSWws4bYz6pqFgK0qED2GzWMEEtdKGUUlXDxZ1gIdaLSw9WREQE+/fv
Z+fOnSQkJODv709ERASDBw/m7Nmz+Pr60rRpUy/Fq5RSqgRuLOScAbyeYLVvb71u26YJllJKVRUX
d4LlHCKYu96kj48PTZo0ITo6mhUrVvDRRx9RrVo1li9fXuBtPv/8cz755BNSU1O5+eabufPOO8s7
cqWUUkUwxjzg7RiK0qwZBAXB9u3ejkQppVRZubgTLEcPlsm7HMqJEye46y6rGm+jRo04ePBggXfZ
tm0b8+fPB6Bx48aaYCmlVCUjIgOBtljrYAFgjMl/ccMK5OtrFbrQBEsppaqOi7zIRf4JVnx8PO3a
tXPuF1S63SE0NNS5rQUwlFKqchGR2cAQYDTWJ2u3A5d4NSgX7dtbQwSVUkpVDdqDBWBzT7B8fX1J
cynp1KdPn0Lv0rt3b95++23CwsJo1apVmUeplFLqvPQwxnQQkW3GmIki8hrwpbeDcujQARYuhIQE
iIrydjRKKaXO18Xdg+Xg0YMVFhbm3A4MDOSNN94o9PK2bdty9913k5qayubNmzl79my5hKmUUqpU
HJ+YnRORukAWUGlWhXcUutBhgkopVTVc3D1Ykn8PVlBQEBs3biQsLMxt+F9hxo0bx7///W8AtmzZ
wowZM8o0VKWUUqW2UkQigVeBzVgVBN/2bki5HOvbb98OvXt7NxallFLn7+JOsAoociEidO7cuUR3
GjJkCKdPn2br1q2sXbu2rAJUSilVSiLib4zJMsZMsh/6WERWAkHGmCRvxuYqKsr62rrV25EopZQq
Cxf5EMGCqwiWVEhICO+9916h1QaVUkpVqGMiMldEeotYQxaMMRmVKbly6NQJNm/2dhRKKaXKwsXd
g5V6wHq15U2wjhw5wqpVqwgICKBhw4b06tWr0Fu1bNmS3377jZ9//pn9+/fz4osvMnDgQC699FIA
UlJSSE5OJiUlhSZNmhRZmVAppdR5aw3cBowHFonIx8AHxpifvBtWXp07w5o1kJYGwcHejkYppdT5
uLgTrGx7SfV8erC2bt3Kww8/DMCAAQOKTLACAgJo3LgxTZo0cTvuSLBiY2PZu3cvADt37qR169bn
G71SSqlCGGNOA28Bb9mLW9wO/FtEooAlxphxXg3QRefOkJNjDRPs1s3b0SillDofF/kQQTtbVp5D
jzzyiHN71apVpbptYmKiczs8PNy5nZKSUqr7KaWUKh1jzB/APOBNIAUY7t2I3MXGWq+bNnk3DqWU
UudPEyzItwcrIiLCue3ohSqpq666CoBPP/2UTZs2ER0dTfPmzUsXo1JKqRITkSARuV1EPgH2Ab2A
Z4G63o3MXf36ULu2JlhKKVUVXNxDBB3ymYN13XXXsWvXLgDuueeeYt/qu+++4+zZs+zevZvvvvuO
lStXMmfOHDp27Ei9evUYMmQIXbp0KbPQlVJK5U9E3geuA74D3gPuMsakezeq/IlYvVgbN3o7EqWU
Uufr4k6wfIOBNKsHy5jcdbGAf/zjH4wdO5bq1au7De8rTP369UlMTCQ9PZ3vvvuOK6+80nkuLi6O
uLg4xo2rNEP+lVKqqlsFPGyMKfW4bBHpB0wHfIG5xpiX8mlzDTAN8AdOGWN6luZZnTvDV1/BuXMQ
ElLaiJVSSnnbxZ1gRbQBiQNjs77E13mqTp06Jb5dSkoKZ8+eBazFij1ddtlldNPZy0opVSGMMe+c
z/Ui4gvMBPoAR4ENIrLCGLPTpU0kMAvoZ4w5Yi+gUSquhS66dz+fyJVSSnmTzsESe45ZBmthBbvU
1v3oo4/czo0aNYq4uLjzfoZSSqkK0wXYZ4w5YIzJBJYAgzza3AV8Yow5AmCMSSjtw7TQhVJKVQ3l
mmCJSD8R2SMi+0Tk2XzOi4jMsJ/fJiKdXM5FishHIrJbRHaJSHf78Q9FZIv965CIbLEfDxCRBSKy
XUS22odsFC7jVG4FwTJIsHbs2MHOnTuZPHkyL73kPoqkevXq/Pbbb2zevJlDhw6d97M85eTkcPz4
cZKSksjMzCzz+yul1IVIRHxEpEcpL68H/O6yf9R+zFULoLqIfCsim0Tk3gLiGCEiG0Vk48mTJ/N/
WD2Ijoaffy5ltEoppSqFchsiWJyhFUB/oLn9qytW+dyu9nPTgVXGmNtEJAAIATDGDHF5xmtAkn33
Ifv59vYhGl+KyOXGGFuBQRobYKztfApdlFRKSgpt2rTJc7xFixZs27aNFi1aAPDXv/6V6dOnn/fz
XB0/fpwGDRoAULduXY4dO1am91dKqQuRMcYmIjOBjuX0CD+gM9AbCAZ+FJGfjDF7PeKYA8wBiI2N
NfndSAR69IAffyynSJVSSlWI8uzBKs7QikHAO8byExApIjEiEgFcjbVmCcaYTGNMouuFIiLAHcAH
9kNtgLX29glAIhBbaITi8u2XQQ+W67pXDv/85z/59ttv6du3r/NYefQwpaWlObdDdHa0Ukq5+lpE
brW/b5TEMaCBy359+zFXR4HVxpizxphTwPdA6db2wEqw9u+HhFIPNFRKKeVt5ZlgFWdoRUFtGgMn
gQUiEicic0Uk1OPaq4ATxpjf7PtbgZtExE9EGmN9otiAQrl8+2XQg+Wa5AC8/fbbtG/fnqVLl7J0
6VLq1KlDu3btiImJOe9necrOziY6Oprw8PAiqx4eP36cCRMmMGbMGH7Uj0qVUlXfw8AyIFNEkkUk
RUSSi3HdBqC5iDS2j6S4E1jh0eZT4Er7e08I1iiMXaUNtId9MKP+alZKqQtXZa0i6Ad0AkYbY34W
kelYC0P+n0uboeT2XgHMB1oDG4HDwHogx/PGIjICGAHQ/JLauSeK04NlDHzTF/wj4KqP8pxu3749
v/zyC2lpaQQHB3P55Zfj+oHpypUrGThwYNHPKYXWrVuzfv16MjIyyMzMJCcnB19f33zb/vWvf3UW
4Vi4cCFHjx4tdil6pZS60BhjSvULzhiTLSKjgNVYZdrnG2N2iMgj9vOzjTG7RGQVsA2wYZVy/7W0
sXbqBP7+sH49DPIc86GUUuqCUJ4JVnGGVhTUxgBHjTGOqb4fYSVYAIiIH3ALVi8VYL0RAk+4tFkP
uI2Bt7fLHQd/WWuDTwrY0ouXYKUdg/j/Om7ktm4WQFhYGJdffrnbsVq1anHq1CnAKtNeni699FJS
U1MBSEpKolq1avm2mz59OuHh4QQFBfHmm29y8OBBOnToUK6xKaWUt9iHBt4NNDbGTBKRBkCMMeaX
oq41xnwBfOFxbLbH/qvAq2URa1CQVa59/fqyuJtSSilvKM8hgsUZWrECuNdeTbAbkGSMOW6MiQd+
F5GW9na9AdfiGNcBu40xRx0HRCTEMYxQRPoA2R4FNfLyC4WQuta2o5pgYdJdBsUXpz3w2WefcdNN
NzFv3jzq1fMcIXn+1qxZwxtvvMHChQudyRUUPs+rbt26pKWl8dlnn3HHHXfonC2lVFU3C+iOVVId
IBWrCFOl1KMHbNgAWhBWKaUuTOXWg1WcoRVYnwoOAPYB54AHXG4xGnjPnpwd8Dh3J+7DAwGigNUi
YsPqBbunWIE61sEqzhyscy4dcDnnwDegwKZjxowBICAggGXLlhEQUHDb8/HBB0mqdFYAACAASURB
VB+wYMEC536rVq0ICAjAmHyLVLldp5RSF4muxphOIhIHYIw5Y39vqZR69IB//Qvi4qBr16LbK6WU
qlzKdQ5WUUMrjJUFPFbAtVsooAqgMeb+fI4dAlrmaVwYWwakHrTfoJhDBB1y0oDIfJtNnz6dGTNm
OPdfeOEFkpKS2LNnD1lZWVSrVo327duXKNSC7N2bOwqyZcuWfP3111SrVo2wsLAyub9SSlUBWfal
QwyAiNTGmi9VKTkKXfzwgyZYSil1ISrXhYYrPwFTgoWG047nbuekFdhs3rx5bvv+/v789NNPdO3a
lSuvvJInn3yyNMHmq2nTps7tPXv2UK9ePZ566qkyu79SSlUBM4DlQJSIvAD8D5ji3ZAKFhMDzZrB
t996OxKllFKlcXEnWFLCMu1ZLlV9s88V2Cw0NLei/KBBgxAR/P39c2+TVbz5W8Xx+OOP5zkWHBxc
6DVNmzaldu3axMTEcObMmTKLRSmlKiNjzHvAM1hJ1XFgsDFmqXejKty118J330H2+a8gopRSqoJd
3AmWzYDjzas4PVjZuUUkCuvBeuyxx3j11VeZNWsWs2bNAqB69erExsbSvXt32rZtex5Bu4uJieHB
Bx90JnUxMTHUrFmz0GtOnTrFqVOniI+P5+9//zvff/99mcWjlFKVjYgsNsbsNsbMNMa8YS+tvtjb
cRWmVy9ITrbmYSmllLqwSFHFEKqyWBGz8UmsYu/XrYOoKwu/4IehcHiJtd37W4juWc4RlszevXtJ
TEwkMzOT9u3bExER4Xb+9ttv548//mB9PvV/bTab25pdSilVmYjIJmNMvvNyi3HtZmNMJ5d9X2C7
MaZNmQVYArGxsWbjxo2FtomPt4YKvvwyPPNMBQWmlFLKTWnfey7uHiyAHPv6k8XqwTrrcl3BPVje
MmzYMLp27cpVV13Ftm3b8pzftGmTM7maMGGC27nff/+9IkJUSqkKIyJ/F5EUoIOIJItIin0/AfjU
y+EVqk4daNMG1q71diRKKaVKqlyrCF4QfOoBu4u3rpXbEMGC52AV5eeff+bIkSOcOHGCe+65J09P
U0lMnjyZgIAAwsLC3Hqg8lsHKyUlxbn98MMPs3v3bgYPHky3bt2oW7duqWNQSqnKyBgzBZgiIlOM
MX/3djwlde21sHChtR5WOa30oZRSqhxogpUhkA6MmAgPpMAttxTc1rUHK7vgHqxjx47xzDPPEBgY
SIMGDZg4caLb+eHDh/Prr78CcMUVV9CxY8dShW6z2Xjuueew2axqwyNGjCA9Pd2ZcHlau3YtSUlJ
JCcnU7NmTV0LSyl1sRgnIn8BGhtjJolIAyDGGPOLtwMrTK9eMHOmtejwFVd4OxqllFLFpQnW6cOw
Alj5g/VV2Jw01x6sT76BoxuhZk148kkICXGeevfdd3n//fed+xMnTiQ9PZ3169dz8uRJZ3IFEB8f
X+rQT5065UyuqlevzltvvVVo+7CwMA4dOkRycjLbtm2jc+fOpX62UkpdQGZirXvVC5gEpNqPXe7N
oIrSsyeIwFdfaYKllFIXEk2wdmVD3ulK+cuyJ1gpwNPzc5Oxpk1h6FBnsw0bNuS5NCEhgd69e7sd
69SpEzExMaUI2hIQEMCrr75KfHx8sQpUfPjhh/z979YomWeeeUYTLKXUxaKrMaaTiMQBGGPOiEil
H3RXs6a10PCXX4LHQAillFKVmBa52OYxV6mwRUdyzsJpYD7uPV0ea0m1a9fOue2Y2+S6DpbDCy+8
wGWXXVbSiJ0iIyN56qmnmDp1Kq+++mqR7V3ner3yyis670opdbHIslcONAAiUhurR6vSGzDAGiKY
kODtSJRSShWXJliedu3Keyw9ATaPhcxEeB3wHLWfnu62e9ttt/Hmm2/yzjvvsGLFCgBCQkLo1KmT
W7t+/fqVYeBFa9asGV26dHHuHz9+nAYNGlCzZk2WLFlSobEopVQFmgEsB6JE5AXgf8CL3g2peAYM
sF5Xr/ZuHEoppYpPhwgCCBDjD39kwdNPw6pV7ucPvgO7/2Vt/5bP9UlJsGYN9OkDWD1Yrr1YYPUe
/fLLL+zdu5ekpCTS0sq+zPu+ffs4cuQImZmZtGrVikaNGjnP/fLLL4wcOZJz59yrHx49ehRwrzCo
lFJViTHmPRHZBPTG+o0/2BiTz6dplU/HjlbJ9i++gHvu8XY0SimlikN7sACqA9dHW9sew/0AyPjT
eq1/M4QF5z3/4YfQty/Yk5WC+Pr60rp1a7p168a11157fjHnY/r06dx8Q2/69+/v7DlzOH36NPv3
7+f48eP06tWLc+fOcf/99zvPl0fCp5RSlcgJYB2wHggWkU5FtAdARPqJyB4R2ScizxbS7nIRyRaR
28ooXgB8fKB/f6sHq7AR7EoppSoP7cECK8G6IgwWkP9Ad0f1wOhroOYmSD1i7fv7Q1YW1K8Pe/ZY
28Wwd+9evvvuOzIyMmjZsiV97D1fJfXBBx/wyy+/EB4ezqBBg+gWc5DX50KHZyHdY9hicnKyc7tG
jRoEBwfzyiuvMHHiREJCQqhWrVqpYlBKqcpORCYB9wP7sc/Dsr/2KuI6X6xqg32Ao8AGEVlhjNmZ
T7uXga/KNnLLgAGwYAH89BNceWV5PEEppVRZ0gQLIAioY19wuLAEyy8MoqLgsD3BatLESqx8fa19
j6SmIOvXr2fEiBEA3HvvvaVOsL766isWLlwIwCWXXEKPelYSddfAS7nkkkvc2t5www3s3buX5ORk
QkNDAahdu3apnquUUheYO4Cmxpi8K7AXrguwzxhzAEBElgCDgJ0e7UYDH1NOZd/79LEWGl6+XBMs
pZS6EOgQQYBAoEZDCAyEc+fgrMuCwjmZcGSpte0XCoFB1nZL4AH7gPiDB63XIobZff755yxatIjl
y5c7j61du5ZPP/2Ud999t8Dr4uLieO2113jjjTdYu3at87jrvKnw8HBa9x0PwLPPz2LIkCFu9wgN
DaVx48bs37+fdevWMWPGjEJjVUqpKuRXILIU19UDfnfZP2o/5iQi9YCbgTdLHV0RIiKsJOvjjwtf
qlEppVTloD1YYCVYOalW79Tvv8PJk2Dv5WHHZMi2J1x+YbnJ1z1ALXvZc8fcqyISrBtuuMFtf8SI
EcyZM4fBgwcjIgwdOpQ//viDkydPkpaWRvPmzYmKimLOnDnMnj3beZ2xv8M++OCDdO/enZSUFNq3
bw8+J6wGtowCY3BNvEaNGoWPj+bYSqkqbwoQJyK/As5fkMaYm8rg3tOAvxljbIWtRygiI4ARAA0b
NizxQ269FT7/HDZtgtjY0oaqlFKqImiCBVaCdXw1RDW3EqyEBHBU4Ns/N7eda4IVCDiWqYyIsMZv
XF746JDLLruMLVu2ADBjxgxGjx7NnDlzACtpGjt2LLt27eKrr6xh/O+++y533323M6HyNGDAAAY4
avgCxC20b+SfNPn5uf9zN2vWjAMHDjifX5zFipVS6gK0CGuO1HZKtv7VMaCBy359+zFXscAS++/P
WsAAEck2xvzHtZExZg4wByA2NrbE/VCDBoGfn9WLpQmWUkpVbtp9AVailJUM1e29VidP5t/O3yXB
CgD87e+ROTnQooWVZBVi4cKFDBs2jIkTJzJs2LA856dPn86OHTuc+45CFddcc03xvg9f+/DFqKsK
bOK6sPHBgwepXr06gYGB3HnnncV7hlJKXXjOGWNmGGO+McZ85/gqxnUbgOYi0lhEAoA7AbcSrcaY
xsaYRsaYRsBHwKOeyVVZqFEDevWCjz7SYYJKKVXZaYLlKsL+48iv0AVYPViOdaQCAX/7B6Fnz8KG
DWDvnSrIpZdeyty5c/nnP/9JSEgIAG+99ZZbm2PHcj8cXbZsGQB33HEHZ86cITMzs8DeLIDU5FPk
EMCyjz7m+++/dzv34IMPUrduXWcPmq+9MEdiYiKZmZl51scqqXXr1nHddddx1113ER8fX+r75OTk
cOTIkfOKRSmlPKwTkSki0l1EOjm+irrIGJMNjAJWA7uApcaYHSLyiIg8Ut5Be7r1Vti3D7Zuregn
K6WUKgkdIuiqZrj16ppgpR3P3fYcIphln3uVaR/Sv3kzuPQQFeXAgQPOIYL5Wb16NT/99BOdO3cm
MDAQf3//Qu+XcmwDYWQyd9IdBDW+iauvvhqwhv8tWLDA2W7FihXceOONrFmzhr59+wKcd4L1yCOP
sHOnVVirQYMGvPzyyyW+R1JSEh07dqRGjRps3LjxvOJRSikXHe2v3VyOFVmmHcAY8wXwhcex2QW0
vb+U8RXLLbfAY4/Bu++W6K1GKaVUBdMEC8AnBDgHkfYhdo4hgsfXuLdb8wNkZoIA/sD+qdbx+oFw
6FxukYvUVAgOzi3f7uH06dO88847fPHFF2zatKnQ0JYtW0ZYWBhXXHEFS5YsoX///s5zw4YNw9/f
n6CgIKZOnUqAWIledASccCkZn5CQwKBBgwgNDWXDhg3O8uw9e/bk9OnTBAcHExQUVOSPqTA7d+5k
5MiRbN++nZEjR5bqHitWrODgwYPUqFGDI0eOlGoiuFJKeTLGlP3K7l5QqxYMHAjvvQcvvWTNyVJK
KVX56BBBAP8a1mukfQ7Va69ZiVRS7nwomj0CP9h7VQy5SRZAln1plXPnIDsbwsNhzJgCH3fq1Cme
fPJJ/vvf/zqP+fj4cOONN9Kxo/VB61VXXUWfPn3o1q0bmZmZJCcnM378eAYMGMCyZcvIyclh/vz5
vPXWW0yfPh1fX198QusD0L1rZ666KnceVnR0NP/5z38YOXIk3bt3p2vXrgAEBAQ4Fx0+nwIXNpuN
22+/nY0bN5KSkpJnDa7iOnz4MACbNm1i5syZpY5HKaU8ichAEXlGRP7p+PJ2TKVx330QHw9r1hTd
VimllHfo518AgbWAo9AyPPfYr79CkH3oX+unoeMr8O5frf1rGsOUg9DZ3vbPbOs1LQ0SE63tmTPh
jTfyf1xgoNv+hAkTaNq0KX/5y1/ytP3555/p3Nl60ObNmwH45ptv6NGjh7PNjPt8kW3jqd57AXwS
zcgRD0KLRzl8+DD79+/nzJkztGvXjq5duxIREcGsWbNISEigS5cuDBw4sPg/Jw/t2rUjMDCQqKgo
Pv74Y+e8stJq3bo1Q4YM4ffff6dt27bndS+llHIQkdlACHAtMBe4DfjFq0GV0oABVsGLRYvAZUCD
UkqpSkQTLIBQ+48h8gB06mTNpTpzBqLsPVM+9oTIMQTw2r7w3FtWLxaAY/mrc+fgzz+LfFz16tUZ
M2YMgYGB1KpVi6efftp57sSJE+zZs4e0tDTq1KnjHM7nKj09nRMnTrBw4ULS0tJ4pNpI2PEitHrS
amCz4p42bRrTpk0DYOrUqYwdO5Yvv/ySv/3tbwA88MADpU6wMjIy3CoeXn311YgIGzZsKNX9AG69
9VZuvfXWUl+vlFIF6GGM6SAi24wxE0XkNeBLbwdVGoGBMHQozJ1rfZ4XWZrlk5VSSpWri3uIYPXq
1mtPe9W7+K+gQbS1feZM7oK9Pvahg455TQ27QQsgy+Ve//gHTJ4MGRnQuzf8+GOBj42IiGDatGm8
/PLLbskVwMqVK+nZsyf9+vXjsssu4/HHH89zfVhYGDabjfvuu49HHnEpZLXuNus1KNr+7VV3njpz
5gwARx2LIoOz8EV2djapqakFxpufU6dOue3v2rWLuLg4bLaSLDGTvx9//JEJEyYQGxvL4sWLz/t+
SqmLnmNS6jkRqYv12zvGi/Gcl/vvt95q9NejUkpVThd3ghVuHxJosnOPhdkLU5w54+wJwtfeg+VI
sL78EvYCGS73iomG5cuhQweYMQO6uRarKr7g4GC3/c8++wyA//u//2P06NHs2bOHlJQUYvNbaTLh
W6g/GBoNBWDevHnOU0uXLmXOnDkMHTqU2rVr4+Pjw8KFCwkICMDf3z/fnrLCREdHc+DAAX7++WcA
OnXqRE5ODv7+/rRp06ZE9/L0ww8/MHHiRDZt2sS9995bJkmbUuqi9pmIRAKvApuBQ8D7Xo3oPMTG
Wuvaz5qla2IppVRlVK4Jloj0E5E9IrJPRJ7N57yIyAz7+W2OdUlEJEhEfhGRrSKyQ0Qm5nPtWBEx
IlLL5djf7ffaIyLXFxlgZCR8+y0Eu/wBH2r/kZw5AzkePViOIYL2pIdMl3sF+UPdutb2m2/CI6Vb
IqVmzZp5jg0dOpTnn3+eGTNm0KJFC06fPp27HpbjNbwF2bFv8+WJ65g/fz6zZ892W4/qt99+4+GH
H2bLli0cOXKEw4cPc++995KVZXXDpaenlyiR8fPzo3HjxnTp0oWePXsSYF9k2WazkVDQOmLF5FgM
2c/Pj+joaI4fP17EFUoplT8R8QG+NsYkGmM+Bi4BWhljLsgiFw6PPQa7d8M333g7EqWUUp7KLcES
EV9gJtAfaAMMFRHPro3+QHP71wjgTfvxDKCXMeZS4DKgn4g4u4REpAHQFzjicqwNcCfQFugHzLLH
UDB/f+jZE/xdxvqF5FiviYm5QwQ9e7AyM6Em0JPcWWz+PuAoK/7GG/DWW1BEwvLZZ58xYsQIRo8e
7awoePnll7u1adKkCaNHj3buT5w4kVq1ajFgwAArybJlQmgjaDGK7IZ/ofaeUez4eBgjR44kMzMT
TxEREQQFBVG/fn1ExFlBMCwsjHSX0u4lMWvWLLe5UyUdbgiwYcMGGjRowIIFC6hWrRpTp05ly5Yt
xMfHU69evTzts7OzSUpK4qxjXTKllMqHMcaG9V7k2M8wxiR5MaQycccdVrGLWbO8HYlSSilP5Vnk
oguwzxhzAEBElgCDgJ0ubQYB7xirO+YnEYkUkRhjzHHA8Ve6v/3LdSDEv4FngE897rXEGJMBHBSR
ffYYCp4MlZMDw4bB99nwT8AX8DttnXMdIug5Bysnx6pFNRD4yH6vXt0gxmNIf1aWNSM5H9OmTeOJ
J55w7sfExHDdddfl6UUaMGAA3bt3d+6vXLkSgFWrVtGmTRs6d+5Mu3YP8+yg0QQaQ4sYqGefejV1
6lTS0tJYtGgRN910E0lJSbRs2dLt/mfOnCEgIOC8yrS3adOG1q1b06dPH8LDwwkNDS3xPUaMGMHR
o0d58MEH6dWrF19//XWBbcePH88LL7wAwAsvvMA//vGPUsde1r7++mvOnTtH9erV6dChA9WqVfN2
SEop+FpEbgU+MaZqDKoLDrbevv71LzhyJPfzPaWUUt5XnglWPeB3l/2jQNditKkHHLf3Pm0CmgEz
jTE/A4jIIOCYMWarR1JQD/gpn3u5EZERWL1lNImJgfnzrRMJ0RBzwmOIoL0DzLOKIFh9f0nkDhNM
S7ZWfdy2zVpHa9EiaxZyAQmWr8cixI7S7bVq1WLlypXMnDmToKAgOnTo4Gxjs9lw/dtg9+7d7N69
m169evHss88iIohvIB3aNWLUqD6MHDmSkJAQxo8fn28Mrs89XyJC8+bNS319SkqKc7t169aFtnWd
p1aa3rLyNH78eH76yfrP8KOPPtKqiEpVDg8DTwLZIpKOVQPWGGMu6E9ARo2Cf//bSrLsBWOVUkpV
ApW2yIUxJscYcxlQH+giIu1EJAT4B1Z/U2nvO8cYE2uMia1eq1buiRcTrT6yBlda++fOFVzkAuAr
4FFyS7UfPGi9tm9vzUAGayihw8aNsGqV9VEjeYtZ9OzZ07k9cOBAvvjiCz755BMeeugh53FH4QdX
TaNh6oBtcNLqqAuPqMF1117N66+/ft7rUhXm/fffp1GjRrRs2ZLnnnvuvO+3Y8cOdu/ezWuvvcar
r75aYLsNGza4JYzn0/NWHhzVGgFuu+22fIdpKqUqljEm3BjjY4wJMMZUs+9f0MkVWL1Wd98Nb78N
HoVdlVJKeVF5JljHgAYu+/Xtx0rUxhiTCHyDNa+qKdAY2Coih+ztN4tInWI+z51rL9KfGdZ6Vv72
OVjnzhVcpn3dOnCs8+voUPrzZO69goMhLMwaIghw9KhV8ql/f7jlFgDuv/9+srKyMMZgjMm/KqCH
sLAw5/Ytt9zCp0vnsvs1fzrGnIKspNxYbUX8UW8M7HzFubtx40bGjRvH3/72N5YtW+Y8npWVxQcf
fJDvPKfExEQOHz7M3r17OXnyZJ7zJRUYGEjLli158sknCQ4OxhhDUlISBw4c4Nix3H/G/fv3O7dv
v/1251DByqJ3795u+zpHTKnKQUSqi0gXEbna8eXtmMrCM89Yb1evv+7tSJRSSjmUZ4K1AWguIo1F
JACrAMUKjzYrgHvt1QS7AUnGmOMiUtteUhcRCQb6ALuNMduNMVHGmEbGmEZYwwA7GWPi7fe6U0QC
RaQxVuGMXwqN0GOYHonAqc+tbdceLM8hgi+/DI4RbY5OotCA3PsMGwYpKblzshwfLfbuDddcA1gV
8vz8Ch6hOX78eHr06EHfvn35xl4mKjw8nJCQEKKiomjVqhU3XdUYP8mCyPZQyz5PK7oXRLQt9Nsm
PR52vADHv4KdL1Nj50hefPFFXnnlFVatWuVslpmZyZLX7iJhUT1I/s3tFhkZuTXqy2qYoauZM2cS
GRlJ06ZNmTx5svN4cnKyczsiIqLMn3u+Zs6cSf369QkNDSUqKqrUhUOUUmVHRIYD3wOrgYn21wne
jKmstGkDgwdbCZbLr0ellFJeVG5zsIwx2SIyCuuNzBeYb4zZISKP2M/PBr4ABgD7gHPAA/bLY4BF
9nlYPsBSY8zKIp63Q0SWYhXRyAYeM8bkFBnoqFHQvTuY1ZD1Dhy3JxKpiZBjTxw8e7BWroQ72wI7
oBZWLUOT5X7fzCT4eZiVzBy0l15/9lm47roiQ+KP1Zw+vIHQ0FDWrFnDww8/DMAjjzzivrDwkY+t
1+6LIcCebHSz5pStXbuWxYsXExERQc+ePbn55ptzr8tJg6xkSDsOKb9Rx3ef81R2trUmWHJyMp9/
/jmfPgmQBBsfg15fOdsNHz6cwYMHk5mZ6Ux0rrzySuLi4sjMzGTTpk1u88dKqpbL8M3Tp087t6+7
7jqWLFlCUlLSec35Kk9HjhypdEMXlbrIjQEuB34yxlwrIq2AF70cU5kZPx7+8x9r+u/EPIuaKKWU
qmjlWeQCY8wXWEmU67HZLtsGeCyf67YBHYtx/0Ye+y8AJRsz5hhX8e638DnQ1Z6TnTkCNns5vvzm
YN3+PDwfDXcOhiOn4FxukQY2bIAXxkK3ddZPOBh4/Xlo2zZ33aqC/gA/vRG+7ceb/eGd7EVkZWUV
XInOxw8T1gwJzF0767vvvmP79u1MmjTJuR6VzWZzT7Ayz1q1Ffd+AxmHCGhqY9KkSfj5+dG+fXvA
6pW66667GPqedYnJTsU14vDwcMIdCzXj+PGkc+7cOcC9h6sojkIVrkMga9WqRWhoKDVr1qR69erO
402aNKFJkybFvrc3aHKlVKWTboxJFxFEJNAYs1tEWhZ92YWhc2erbPtrr8Gjj0J0tLcjUkqpi1ul
LXJRobKz4Z63rZLrjpF+R8/CnP3WHCufACsxck2wkpOh+RUQ6G/tp7lUs/vjD/h0HUwGngaygNuv
tuZu+fvDvn0YY3jssce48cYbueOOOzh69Kh17Z+5RSzuu+8+jh07xtVXu08V+OWXX4iOjia4+Z1c
/WodCKmfe/LnEbT+Y7TbYr95htJ98z28AUxcBC99h9/yNMaPH8+zzz7LwIEDAQgICHCbF7Zt27Yi
FyJ2LDYMlKi4w6JFiwgPD6dOnTpMmTIFsOYypaamcvjwYd566y239jabjeTkZHbs2MH69euL/Zzy
dPLkSd5++20+/fRT4uLivB0OAEOGDKF169a0bt2aYcOGkeZaBVOpi8tR+7Dz/wBrRORT4LCXYypT
kyZZb1GVbFqqUkpdlDTBAve5WK6dDx+fsN6CfQKt+Veuy6fYi1UQZO/dWro295wj0XDUN9gG/Pwj
BAVZa2ht20ZcXByzZs1i5cqVLFu2jPQ/fwNbNuTkJnHbt29n165deeY4+fv7k5CQQHp6Ov/73/9Y
uzb32WEh/jSJsrYbN27M1KlT6devn/v3e/y49erjKElvDQs8e/YsrVu3JigoiGnTpjFo0CDnJWdT
zxb5B/qqVatISUkhIyODHj16FNrW1b591hDFEydOOI+JiDWPzWV4oMNvv/1GREQE7dq144orrij2
c8rTzp07GTFiBIMHD2b06NHk5OSwePFi5/dWnjIyMoiLi+PPP/90O37o0CF2797N0aNH+eyzz1i9
enW5x6Iubjabjc2bN7N58+ZK80EDgDHmZmNMojFmAvB/wDxgcHGuFZF+IrJHRPaJyLP5nL9bRLaJ
yHYRWS8il5Zt9MXTooU1/ffNN2HXLm9EoJRSykETLLCG6020T//y7KTJwBoi6Jg9HB4OcXHgGLYX
Zq9ysXozOHpTXHpyANgPDPq7VVkQ4LbbqL57t/P0K0Oh2a+94JvrrflRdu3atcu3EIZjyODfboSP
H8etyl71qEZEVw9g3LhxLFy4kLFjx+aThNiH9kU6hh5GgDEsXryY3bt3k5GRwZNPPskHH3zgdlVR
a05Vq1aNsLCwEi9cnJaWhr+/1RPYtGnT3BNt24JrKX27qKgo58+lPApslIZrj2FYWBgnT54kMzOT
li1b8vvvvxdyZf6KO8Ty9OnTXHLJJXTq1Im6devy3//+1+18YGAgH374IQkJCQweXKy/J5UqtfT0
dDp37kznzp0rxYcfIhIkIo+LyBsi8rCI+BljvjPGrDDGFNnNbp8HPBPoD7QBhopIG49mB4Gexpj2
wCRgTll/H8U1aZL1NjN6tPvngUoppSqWJlgO1SLAn9yy6w4ZWEMEHQlWdDRcdlnu+cf75G47/qCw
Jwtu9wCrmMbMmQCEHToEwOTb4ekb7OdPrIWcc0WGeskll9CuXTsubQjtG0Dt2rWd55q06kyIfyaT
n5+YZ2ihU06Q9VrDnryEdwERWrZsybhx42jRogUAEyZMcF7i5+/vNgTwlAbcnQAAIABJREFU8ccf
p2bNmtStW5d33323yJgLM3v2bNLS0jh48KB7b1tQEHgMb3zwwQe5++67yc7OJiAggFatWp3Xs8tK
/fr1uf/++xk4cCD/+9//iImJYfjw4dhsNtatW1eiez3xxBPcfvvtxWr72WefOXv+MjIyWLJkifPc
kiVLiIuLc/57KnW+jDGsXr2aV155hcOHL4gRdouAWGA7VpL0Wgmv7wLsM8YcsCdkS4BBrg2MMeuN
MY4F8H7CWiLEK6KiYPJk+Ppr+Ogjb0WhlFKqXItcXFDuug2ipuU9noo1RDDFPvzK1xeSknL/8K8d
lfcaz7lK5wBfAXMGRoyAzZup3b07xhh436On5+yRIkP18/Pjo48+oub2vyCZf1Kto0s9kAB7QYjs
5NxtT2ftfwtERlrh/fknfx8zhuzsbJo3b877779P06ZNiYyMhPetpl0uvxxcik0kJSU5h6SVxWK6
vgKNqp+DwNyCHkkhIfwRFMS4W26hZ8+ejBkzhm+//ZaD9kWdf/3110pTSbB79+50726Vyk9NTXUW
AKlbt26h5fg9paamMmPGDPr168ehQ4do1KhRnjb/93//x9GjRwkICODuu+/mX//6F0uXLqV3794M
GzbM2a5x48bn900p5WHdunXOD0Hmz5/Pzp078fHJ/ZxOROho/30UFBTklRg9tLH3LCEi8yhq6Y68
6gGuXdBHga6FtB8GfFnCZ5SpRx6BefPg8cetorXVC3gbUEopVX40wXIIqp3/8bNYJdCT91j7e/bA
Dz/AgAHWfoBHhb+sLOtjRFdpQKCBla3g5j9g7tyC40h2GTxvywEf33ybtWzZEg7YoFYr95JRke3h
krvAFFKQImGb9RoRDEDgme28N38jp1Ph2muv5fHHHwdg9erVXO/4FtLTCXa5RUHrYBljyMzMxMfH
xznsr1h+ug8O2UsW9vwMgusRvGcPrdPT+e/y5SQlJXHDDTc4kyuAmjVrFnCzktu5cye///47Xbp0
cataWBphYWFW8lwKiYmJ2Gw2duzYweDBg9myZUueNitWrGDbNuvfsG7dulxyySX8+OOPQO68qxo1
alCjRo0SJXeV0YkTJ5xDU6Ojo90qTaqKt8tlcs+ePXtISkpy+/8lODiYzZs3eyO0gjjXz7AvHVJu
DxKRa7ESrCsLOD8CGAHQsGHDcovD1xfefhu6doUxY+Cdd8rtUUoppQqgQwQdjqXCm8Ahj+MnfeFw
PLgOXQsNzd329/iD7847wSUJACA7HAIFslPg7GGr0EXfvvCb++K9AJx2+YDVVsQitRknIdAjMazT
G654j4dGPcudd97Jww8/7DY/CMhdMNn+h5FvRgYh9hzJsQ4WwKyZr+decs59/tXcuXNJSEjg6NGj
zrk9Dz30ED4+PgQFBbF48eKC47blWOXoUw/kHkvclrv93Y2wqhMB9qqNDYHf1q7lxWbNuA1o3bIl
CxYsIDIykuzsbOLj49m2bZvbIsQl8e6779K2bVv69etHgwYNOO4oAuIFKSlWuf/Dhw8XWFTEtcdw
woQJDBs2jJwca3mB5557jtatWxMdHc0777xDRkYGx48fZ//+/W5z9UrqxIkTDBkyhIceeogXKrBM
2VNPPUWzZs1o1qwZMTExfP311xX2bJVXeHg4119/PU888QRPPfXUhZDAXyoiyfavFKCDY1tEivML
4xjQwGW/vv2YGxHpAMwFBhlj8lbmAYwxc4wxscaYWNdh3eWhc2cYNw4WL7bWx1JKKVWxKv27Y4XJ
EPgf4Dmq6ncfWL0aXIfB2YfWAeAX6t7+k0+sXixXp4GgAHgzA649DkdyYM0a2LYNxKfg3qac9Nz7
H18DZw9CsxG55yPaWD1WLrKzs1n2+iO8fPk8+r4Emw5af3QD8OKLubV8AWrYe4Cy4Pnn/sFZn7rU
q1cPMs/AgUXc0/GQ8741bHt59O6rmLbgawICAggJCSEkJMTt2b4u1RgLLdJw+H348V4Aes2+nDMp
WXz9+DFqeHR4Zd3UEP8VR3iyRQtO1KrF3+1FRHocOMB9992HiNCvXz/WrFkDwBdffEH//v0Lfm4B
XBPQs2fPsmrVKh544IFCrig/zZo14+DBg6SkpBRYFn/Kiy9S4/PPWTVvHv5+fjRo0IBz48cT3qMH
Z86ccbarUaMGy5cvZ+jQoQDccccdfPjhh6WKKyEhgaVLlwLQtm1bxo0bV6r7lNY999xDzZo1SUxM
dDtujGHr1q34+fnRtm1bXYOsnL333nts3bqVTz/9NP8CM2knwJYBoQ2xZaVz+rfVPP3cvxl01xj3
tfgqiDEm/yEAxbcBaC4ijbESqzuBu1wbiEhD4BPgHmPM3vN8XpkZNw4++wyGD7cSrgYNir5GKaVU
2dAeLIdw+1C/M//P3nmHR1WscfidJJveIJBAQofQiyCINAWUrgiCCCICykUvAhb0ingVBWyIFb0o
ClLVSxUEFJQqCIROqCGEEko66XV35/4xZ7ObEJASDOud93nOk9N35uzJznzzffP7iu23uMPLLxfd
5xhC5lZCyNKuXcY9je2MDEjMUwZcVCRUqqT2x8WBq4OB1v8SPJoDnkbIn4OiIGf/C5FvFf2cTr9A
g3FFdrm4uPDV17Mo7wt+RkxfYR6s5cuL5vIKMkQu8mHYkEHk5eWxc+dOfv/6Ydj7Av3rHS5y76eb
bOfnnx2mF2RlKUNx7VpYu5ZmcXFUQ8nIXzVnVm6i+tt1O5u27mL//v3kZqZcdprpUTXK+6SrK0Pm
zMEQxufL994rDMELdgjHvMxTd428+OKLLF68mIYNGzJ9+nQ6dOhwXdcfOXKEkJAQatSoQe/evW+o
DDZMJhM1atSgSZMmNGtWstpzn9BQ7pk1i3eAt8xmRpw6hd9778FDD1EnIIDw8HCCgoIICgrCy8se
2HkzebAcvYMxMTE899xzfPjhh6xevfqG73ktBAcHU6tWLby8vJg2bRr9+vUrcnzAgAE0b96cJk2a
MHLkyCvcRXMjzJs3jwEDBvDyyy+TkJDA9OnTsVqtxMXFcfbs5XNFhw4dyrEvq/Pj5OZERETQp2cH
Ku7vwxc9NvPB+++WQQ1uHimlGRgNrAWOAouklIeFEM8IIZ4xTnsDCAL+I4TYL4TYXUbFLYK7O3z/
PeTlwaOPFh0j1Gg0Gs2tRXuwbNjmdhSP2jubC9Zij6mIB8tXJRReGgD70tS+uDj1tyHw4TtwMBDe
fw3OXILzJ6F9kAqUj4uDYAcb1xSgJONtXiuHnFiYsyA3DvJT4eJayLkIob3Av6jIg4uLC2Y8gDx8
PeC9996ze5pyi4UcljfCVPIBax5z5swhMjKSJs8CbSHdvxOfz99Ig1Do2woq+FqIcDRifv8d3noL
duwA4J/AM02aIA4e5ErExMRwbNUyeobBvFWH6Nq1G2vXri0MUSxCbLxSXty+nSpr17Jswwbo3Jmm
tWsX5vAKCQkhKCiI4ODgIiqH10v//v3p37//DV2bmZlZaNxVMGTls7OzSUpKIicnBx8fH6pUuXZh
sT179jBs2DDS0tJo2rQpq1atKnqCkRssMySEpW5udO/WDc+5cwmwWDAnJxMVZR9E37hxI5UqVcLL
y4ubCUtyFBPJycnhs88+A6Bjx46FyalvBR9++CEffliy8FtWVhZLliyhbt26lC9f/qbnzl0PZ86c
4eOPP8bLy4tx48YVfu9/F44cOcLQoUMLt81mM9u2bWOXMXiUlJR0mcBMTEwM/nfl4eeSR25uLvd2
6cvXG3fzj07gIsw4K1LKNcCaYvu+dFgfAYz4q8t1LdSrpwQvHn0Uxo2D6dP//BqNRqPR3DzawLJh
8/IcB57qBhvWquwmGRbAYj/vxRdVLiwbbr4qrPDlDLj4Txg3w37MAwipCC+MhmDDgNv1BQz8VAlh
XLwIP+XDEaAF0C8fPDzA1fA6OHqwLu1XoYQ/t1ChggAnv4YOP15mZA0a8g/gc54ZMZhOw8aosKnI
SZB4qGidbR6sAhdw9SoUEwgz+qn+LV/j1V7rWfr9LOAfBAV40Kye8qpYLBZcu3eHmjXBluD2gw8Q
f5Jc9NSpU+yJ2EbPvmA++A6jhvyTDh06EOD9JuDQCbMC489BVaFyjo0ZozxlANl2Kftp06ZdsQN+
vUgpsVqtHD9+nM6dO2M2m1m6dCn33nvvVa/LyckhvBKsegkC/SJheRWyM11o8I9YsvPgqaee4pur
CZuUQPzZQyRnXEHIwxiK9r37boYsW8auXbvwmz2bACC+WM6tTp06lcqcsuDgYLp06VIYjmmjks0b
WwakpqZSrVo1oqKiCAwMLBT6uBIrV64sTH774IMP0qJFixv+7Jdeeoklhg52bm4uH3/88Q3f63bE
ZDLRvn17tm7dCsD8+fPp1asXFouFoKAgDhw4wIIFC8jOzsbFxYVZs2aRn5+PmwtExUFzd3eGDBvB
0R93A8t59V/jrv6BmlvGgAGwcyd89BHUrq3UBTUajUZza9EGlg1PT5g9W3XeB3WAr9fC+BLOGz9e
eZls+NUGkz8UpEPGjKLnegO/H1MxGiF1ITYZ0oC8JKX8d+YMHDCMqJ3AokUwZAi4GvLGjh4s72qQ
fgy8q0D1AdDwFZWfq/gcMODZ58bDj5/Tq+u9YPNeHX5bqRk6YvPE5VvBvwGvvfYaCQkJNK70AXAJ
vMIQQtB/4FD44R94uhUouXagRYsWHDt6FF8/P7Zs2UKjRo1g6dKiz6YEcnJy2HQUwivBk21OQbtq
9B70MPzwb/tJwR0hdgtghWeGg18FGDuWladO8U39+li+/JJely4xatSoq865OXPmDOfPn6dNmzYl
njdhwgRSUlIoX748o0ePplmzZiQlJRU555133vlTA6tt27bsXPs15Q79g6ygzmCyUCHnV2oFw6FY
5c26HqoWrCdhhnp1npifevkJtlgfd3dcXFxwd3cnv/ixi79Cwmb7NZW6QMjV6/FnBAYGIoRASkmb
Nm248847CQ8PJzk5uVQVHefOncu0adPw8fFh8ODBjBkzpsTzwsLCOHPmDFLKP02CDbBixQpmz54N
qLxlN2NgPf7449x7770cPnyY3btvi4iwUiU8PJw1a9bQsWNH9u7dS3JyMhUqVMDDw4Nff/21iKFt
M7AWLVpE0PYm9HvkAXyaNMHHx4fgTl1g13J69ehylU/T3GqmToXTp9X4YNWqUCzSVqPRaDSljDaw
HBk+HPbtg7mrVdLhkli3DgYPtm+b/KHTOoj6Arw3w4izkFMdVp6BUGCZEmbgjtawOxJSAWu+mqd1
6BA45rCyqQoWN7Byc+HLY+BVDeZs/lMjBlMA1BwKvrXt+8z5UFyU0Ntbeczy8nhm2DCypeTJOw4R
5GFMRPMOU39dTCoXmDWPUyeP8cUHE4g6epBFBVApJcU+2f0aBAbq169Pj2EfcLEgEZgK+SlgNgwQ
N1/ovF6Jd3wbAmSza/2HLN+QxTvA85MmcerCBTh2jPotW171c2JjY6lbty75+fn85z//4Z///Odl
5yxevJjo6GgA+vXrd5lxNX/+/GvKJWUymShXpyeUW45PpfvAxYOff15DqhxDeLhXkXlif8azzz5L
i4L/8FRH6FAf9pfgEfxk6lSeB7bu2kUbi4V69eohGzWCw4f5ZOpUsJphS5+iSatPzoKHb9yTdfjw
YQYPHswPP/yAi4sLS5YsYcCAAUgp2bZt2w2LZ5TE+fPnOXRIeVs7duxIZmYmM2fOJCsrCzc3N159
9VUsFgtCCFxcXBBCFOYd+6sIDQ0tVM+88847S/XejnUrS/z8/Ojbt2+h7Lqnp2ehymVJVK9eHXaY
Ca5Uxa606lMLwh4EcbNaE5qbwdUVFixQebEGD1ZBGF27lnWpNBqN5u+LNrAcyc2FDz+EhQvhiyuc
8/jjRQ2sxG2wbRA0mWgYW18AZ8BIk8XHl6BTJ7jjDrWdjlLZyj4Jx9cVvfe5c+qvqxdIlGR7cAcl
JLHYmFT+8mFo3Pjq9TD5Qps59u2CDHgXKD7J2dMTPJWBNfD3ecxJhI42NfqARmBy6LSafCEvj48m
9Gd6nyMMrArZE8HXC4JytkJqLpYd57B++y3pH3yAKTAQf/9iOcJQKnkvjRmujMcfpyq59qC71UE3
H6hwl1o/3ADYQ/XDyrgCOH3hAi+inH2nikvhF2PmzJmFcubz5s0r0cCyJUqGy71MaWlpeHt7X7sM
tXcoePcp3OzxQB9iH+hzlQtKJjAwEHcHoZUA36LzyiwWCwf37AEg6vRp2rm4qDl2hjcy6cIFknev
pbElG0yBUP95iHxTeU1vEKvVSp8+fejduzcPPfQQUDTXl6NyYWng6I3y8fEhNzeXceNUiJmrqysb
flmKJWkP4e1H8tVXX0HaEWWkl7/zqkb+gw8+WDgXzpYM90aNmSZNmhATE0P58uVLzbjLycnh4d5d
yL+4ja5duvDKlK/Bp3qp3PtGCQ0NpV27duTk5NC6RjaWOz0JlpCdBztPQvfuPRgwYID9gmbvQDmH
QaPQbmrRlDleXrBypTKyevdWgre2dI4ajUajKV20geXI8ePKuALwR3mxHBXXvbzgySeLXpMZA9mx
cH4VBJag+la7KoS3BndDtjwHsOTB1NHw2ya1zxPlXbLlKcq0wvNAmynweZ9CUQMAiivlJSQouffG
jQvVCWfNmsU333xD7xbwz7YXCLwExJVQXy8vqBgEael0PAX3uKCMMHege7GwJzdfyEumYzVl6LWs
BeaaPrj5ZkHkcDjkxu41vWm9aBlNFi2i27BhfPvttyV8KLB7LCTvBM9giJkN/vXUfounmiwQHg7m
qlBlD5cqQnC8OiyB94EPXV3p865SJbNYLMTGxpKSkkJ2djbt26scn46d3jZt2pRYjBkzZpCUlERK
SgqtW7emWbNmZB44wOuDB+O/cyeEhUHDhiXXoTjpJyAzWoXiIWHvOKjcDcKuTwCiV69eJCx5x76j
IA3c7HL4BQUF2Ewui+G9AZRkGPDcqFFY6sPm11GewMZvICMnIaSZhQvm8NjgodctZZ6enk50dDRS
SiZPnszrr79eKCjh5+dXslz3TfDCCy8wcOBAsrKyCAsLw8ch75zFYmFIwz080QF6LVQDEtE/PUMd
l9/5MnY0j4x484rhin369Cn0OgF8/fXXjBs3jvnz5xcajteKp6fnNXk3r4fVq1fTM3QbY4YD/Apn
FkHd0ZBxQhla7gGl+nlXIjU1lYSEBPz8/HjyySd50vab94MnvbvmgeH5mHvuCe7s+TKNjQGfAwcO
4O//EDVDSve5aEqPoCBYv155r/r0USqDOlxQo9FobgFSyv/b5c4775RFiI+XEtRybLqU9SuodT8/
KaOipLRaZYmsbSPl+vulPP65lAtRSxukbICUF9apc+ZNU/dqgZTntkl5fwcp/VzVvvrGZ7q7S1mn
jr0MIKUJKSf0s29/8IGUrVtLGR4u5ZAhUtaoofZXrixlTo6UeXkyNDRUnv8cmTADKUdQ9H6Oy8mT
Uu7aJfNfflpaPITaNwNpXRZWpHpRUVEy/ttgKRcif/4XhXW0hpmkbI2U33tIuRB59PkBUoKsC7JX
r14lPysppdzysJSrGkmZeVrK+C1SXtyo7vl5qCrDwoVSnlst5UJkwrNVpAT5WZcucvHixdLi5ydz
nn668FYpKSkSZXtJf3//wv0rVqyQgwcPll26dJGzZ8++clkcKDh7Vlrd3Io+o0OHrulaGTlF1SE/
R8r+/aUMQcrKblLWqSnl559f2z2klBcuXJCxc+oWPuOVC96X06dPl6tXr1ZlLCiQB556SkqQx7p1
s1/YrZuUIAd4I1O/Nr6jrY9Jq9UqM2apbT8v5Pnz56+5LDZiYmIKn2/VqlWllFKazWZZUFBw3fe6
EaxWq3z++efla6+9JsPCwuQv411l/lz1nUsp5Yv9QqVciLynPvLw4cNFrk1MTJS7d++WkZGR8sKF
C0WOhYaGSkDOnz9f5uXl/SV1uRqzZs2S349xkWc/Q7avh5RZsVIm71bfZeyKW18Aq1XK11+XJ1q1
kgtBbgoLk4eaNZOpY8eqY/G/q//XmHmqTGcWFV66ZPFiOR/kZ37IYwd3SCmlzMzMlNPG3S9TZnnJ
Cc/2vuniAbvlbdBulMZyWdvzF3PpkpRt2qifuHffvXLTptFoNP/v3Gjbo/NgORIcrALV16yBeqNh
RzRs3Kg8W+HhVw4/8qiocjt5OMhgH0FlTYmMVNtexhyEHKDzYPjtd0OhEAg3Pjs/H4w5QYUUAMsc
Qglnz1ZenhMnYP58NXMZlCKhlxd4eDDd25vYZKjoD8QDrgK+fE+d5wIMrKAENWrVgpYtMU39knwv
41XIh/RsB9VE4OTJk8ScVZ6zSg4K9eRYlfctUI1g+5ZTE9eqBgbi61tCfjAbllwVBulTXYVAuhlz
zjINFQ4/v0KvTUVD0nDMxIn0798fFx8fPB1ybAUEBBR6ZNLT0ykwkjz37t2bBQsWsG7dOoQQbNmy
BfV/cmXcoqMRZjOULw82SfPi30cJTJ06lSmT/o3FCl+OnwBLlqjnftEM0aeuSxu5cuXKVAmxeyre
fvMVxowZw/z581UZ3dxoWr8+APUcQ0UND9ag9hUJsDm8/OsjhKDAqr6Xvg92LZxz9mecOXOG3bt3
c+LECVJTU2nVqhUhISHUrq3m9bkWFOAWHQ3mWy+/LYTg448/ZsqUKZw7d45u99+DqXL7wlxrybnK
w+VpUop+jqxevZqWLVvSpEkTXnnlFQBOnDjBihUruHDhAqCSGF9PiGBWVhbu7u408vfnXl9fnu/Q
gec7dOCt3r3hJsIln3zySQb2702VWk3YEJkP3lVYtkKpgy9dtPCq859KhchImDyZOrt28Rhw7/nz
NDpwgIDPPiNmnBsEt1f/r5V7QKPXwE95nt99913efOQRHgfGZMDM8XeTmpqKlJIN63+jnGcOWzau
vbVl11wXgYGwYQMMGgSvvgpDh6q0hhqNRqMpHbSBVZzBg6FHD7UeEAAdO8KFqfBbR/s5lnw4uxhi
5sHG7nB+pZo/5OUgWW1EBHLSCOnzMaIxLwFRp0EArkAdVGxeTAxERcGwYZeX6YKDQtrRo1cu+zPP
QHg43erV4709PYjNDII+wPzO8MDj6hx/YEgYPPJIkUvTLUaYVyZYCzyVmmJ2NhQU4OvrS6bRb61W
0a7+ITq5QlPAT8nEh1ZXxtBvy5fzww8/lFjEFStWcOzIfk6eucC6dYbhePxT9dfWN/bzA1fDSvA0
OvC2XFDennDqF9jYEzb2xCXydRo3bkyzZs3o1KkT58+fZ+rUqercP/6AhQsZM2YM9957L3Pnzr3y
swNlpALcdx8RxpymKUaS1auRk5ODt7ual+KabnSCGzfCPEk9q/SYGL7//vurf7YjZntHOtB4DEXm
OTmoCILq8P+yYQMAbgUOc8kCGgAgDdn/B3vcf81JlGfOnEmrVq2oW7cuP/30ExEREUT9/DMbR4yA
9HT1HjZoAIbRUurkX4J4BxXEs4vVfD0wDHTPQsO68/0qDHPIY/0uy/WVl5dXuG4LZZwxY0aRUMFW
rVpd+1w71POuV1DA4YwMNmdl8cnWrXyydSsTf/pJDcQUzzd3PQQ2QwTfg8mk3p2PPlXKpCt/XFRk
zuAtIU3l8UsPDeXFkBCeDw7mmHHImm0f1Ph911EefusIw1/8mFmzZrF69WocgxcLCtRcOXd3d/KN
f18X6bx5sP6ueHqqiPhJk9S4YosWYOiZaDQajeYm0QbWtWDNh9RI+/aF1bB1AOwYqpL+1h8HXbdB
hbZQ7znwrWWfuxV+n/pr82DZ5kJ5obw/bwHVfZXqVng41Klz+ednXN3zUkirVvD77/j88APLV6yh
amgF9RlN7oUcwztkArbFFcklBZDjaszseQ3KDT+tyuPjA+XKEX7pEmHVlBFV3ts+KU0+XAB3U2hg
ubilQMUKYFEesF9++YWvv/4as4OXY+fOnaQkXCDm9IXCpKWkHzcKYZzk4MGillDJjDt2VNvuVqgQ
CznnlXBDQToHDx5k//79bNiwgcmTJzN+vKGvP3MmOf/+N1mZmZQD1hp5tPbs2UO/fv0YOXJk0fxU
NgOrcmVOGEZV3IkTJDvOgSuBsWPH0iC8Ojn5EGqTxa9UmZTQACTgX1DAsUOHrnqPo0ePMnXqVGbO
nElelv3z+nVvyctjnmBgn84qaBEuM7Dc3NxIMYafZd7lkvD+5UIAKB+gjJKfVq7kxX88wNG9mwDY
sWMHc+fOJTw8nMmTJ7N+/XoSExMLrw+w5Yj77Tcl8pKeDtWrQ5XKEHVQqRZeB7m5uUyePJlHHnmE
adOmYbFYLj/p+HTYOcJe5z3PwZbeat2SY88VBzwx/Glw9ebxxwZRtWrVIrcpV64czZs3p2HDhoUC
F47iK6+//joRERGMGjWKtm3b0rhxYw4fPnzV8mdlZdHAWL8E7DGWfFDzJR2e3XXT9E1o9bl921AU
Lck7V+oYvwn+jRrxUVwcH8fFYcuqZs6mcKDhxIkofv9tOYu/n8Pvv/+On58fjmmWRR64u7tjMpl4
YZwywN+a+NqtLbvmhhACXn9dzcvKyoK771Y/tw7jEhqNRqO5AbTIxbXgFWrIieeAm5cStQAlBOFe
DryrQ+phJWde7znlkbH1GSsaEt0Vaxe7J2B1g9DuUMVhgn1Jkt6BqJ6cIyaTGip25Kmn7Mc2bFDq
gesAGQxNDeslBXgnHirNUsl7DarXDIV9KueS9HBBuHiojnxWFiEnTxLSewLseY6CvBxMLgVgBZEj
yT8C7mdOq6TMlb+DT4C0x5E5B9i3bx8TJkzAz8+PgQMHAqqTOH0T5Juh7aNGaGCe0SG1llMV9fcH
N5uscw688Ya9jo/eBZXOwoaO6rto0wqipyvBjI7d2LFjBwCrVq2i06lTmE0mJqFSmvUz1Omio6NZ
tmyZehwpKYwYMUJ5D22JakNDKfDygrQ0BnTrRnp6+uXfiQPlypUsGLgtAAAgAElEQVTDz9sV6eZF
J5taZLlyLM6dyCOMIRjIKZYAuDi7du0qDGEbMsetME3AP5rtBgzPzfZD0HbeZQaWyWQqFIj0sBkk
5VpCtf4AuHn4QTZ0vqcNCQkJHP1vXz7qZUUeWwO1DvLuu++ycuVKAN4wnvW//vUv6tSpQ3R0NC+/
/DLbtm1jUd26Su+5cmU49j54X4STF9k/vRb7/N9i+PDhV62jjdmzZxd+zpIlS6hRowb9+/cvPN6j
Rw+eqL+NPndkEn3oEE2aNIFqA+H4J8qYu3d10RsG1IdHS45veuSRR3ike3fYvBHMefDjYrpkpeLa
8g7cPX1pk5sLq1bhu2kTHkePcgz+1KCuUaMGC7/6Cp5+GreHH+aC4XWuP2oU7ufOlWqsVd/+jwFv
07tXFwIDA//0/JvCNghjDBIIIXDz84OMDApyYPfu3fTs2RO/ObNI9IOEdPjPpk08OmMG5YKDYd48
ACaMfRl3d3eEEPTs9RD8+j4dO7S9tWXX3BSdOsGBAzB6NLz5Jnz3nYps1lLuGo1Gc2NoA+tasOWD
yr2ovFOhPcE9SMkRC8MJuOUhCGoNDV5U2z5AFlDBGNutenfRezYA5s+Diu3Ap5p9f7EQJ4YAhyhq
YLm7w5132g0CgJo1VccuNVV1wHftgpxUmAtk/AyhddV5YcBZ7JLwKBnutIsxlDO2xW+PQPsf4LXX
4J13IDMTaj0PtYYx84sveMRlNMEAozAU7dT8IFyAz4AqnixeupzKmSv4dx/Yu3dvoYE1YMAA9tet
S25urgpXkxJyjRC8hibVsoeGgosRFmkp5pF56QOY6AoffcZl9OxJfHw8Ukr69OlDTuPGeNaowejH
H8dt4kRWGKqGyclJ+BkOkLBgP0g8C82a2UO7wsLwDAmBuDjuadYM8zXkOWo/cjXkJ8NSw9MZGMj9
93fBPSwMzp9nzNWkunITuMM8m4jPGnLk8BE83AyPkHcV5T2VFshLhsStnDp1il0LFjAAWL56NX0n
TMDFxYW+AwbAokXc2+YuIAICG9nv72ZUdu1dxIR8R8saKtxLICE5gkqVKhUpjoeHB++99x5vNGrE
kqFDlSG/eDGyVi2El5fy0iTtVO94ClTxjOXTLVuu2cBq27YtLVu2JCkpidOnT7N///4iBtbZs2fx
apDBiYsqkhZgx5FE7naVPD28H489+SJNmzbF11RQGEp3VUaOBIeQ1XbGAsDWrfDBBxhBpYxGSfSX
REFBAdnZ2ZhMJrwNI9cvJIQHH3xQnfDGG+r/6joTS9tIS0vDa/tDCL+auNz9Da6urox7ZSKcrkOv
oLsgIOSG7nutrPzvf+kNHDl1isqXLlGuXDmkpydkZGDNBS8v9R7dn2eBVDW189mAACrOmAGb7eGc
Id7l7fNVPYOh+iD1V3NbExSkVAWHD4dRo6BbN+jcGd5+W3m2NBqNRnPtaAPrWvAKVX8jJ6n8Un51
1FLknEqQGwd5xjyJp4F9LSDnD7A8CN5eRc8PBM6vhoiRMMBh8npxD1Y0yiACeLoDfPW7mqeVlWU3
sHx91RwugPbtYds2Fcb1idHRW7ZCLaA8Z+WBeLtuuxCCnOzcQgNLuvmojq1NqMIhL5Gvry+Zl6BI
KevWhaw0OB8PnwJDQkgsbyY4cyeD2sL0mAwlZ58ayd1V4O4QH6jUF7xCID9NGRGuPjB4lz33VoEx
58NczBvgWwNOGXL2nm7QojJcioWjIKNPMHjwYOLi4oiPj8ft1ClEs2YE1jWMy4QEqFCBJ6vOYVRh
ZOAc+GaOmv/l6QFt20FoKJ379lVDuunp1zY/J0AJT3Bpi/pbrhz15M8QaoLzULVPHzh7FoqFsAFw
eiFNPTdDLrTaBPyC8oYGGsb2mBHg8SyYM0hNTSXO8IadOHPG/vGGYZ6TlooHkJ0Sjff2YVC1L2Ta
coZJKuRsQYR4YJskGLFpGRcuCBo3bsyhQ4cICwujXbt2SCnxfvFFhjqW0/aOLVgAjeKUgRULFfwg
J/PqXh9H7rjjDhYsWMDSpUt54IEHqFatWpHjWVlZhAQoD0ktQ6L91y0HubsT7Ni0Eu+Elbx9Hoa/
+h2DBg1SnuWdI6D6QKjy4OUfaBP2aFET/DyVwZoepWTsfZWkeM7Ro3jFxPDm4MF4dup02S3Wr1/P
m/37k5uaSv3HH2d+kybqgM3rA/bkutfowcrNzSUiIgJQRu3s2bN5MXwz+85sJjWyNc8884x6D2oN
u6b73SxbfvmF3sD2gwfpkpGhPLOVKkFiInEZwYQag0Xl3NyQvhCUgEoRcfBgkfvMnrOeJ3sYYbp+
taHdd39J+TWlQ9eucPgwfPUVTJkCbdooY+uFF9Sx68zyoNFoNP+X6DlY10KFNtDgZWg6SW2fXQIp
+4qe4xmiDKx8w9V0JzBiL0QMgPxUODkDHNMFpQFv7bJ30PKSVacvNBNqOJy3HbD1XXs1V0aPj4+a
p1RYPocZELb5MokXIdXY54I6398H7kIJXSQYBpa1ABG7pFCDA8DqanQUSzCwHn/8cWqGN7HPlwJ4
6CHoZuSaOgHsiKVdu3bUa9ySapUDyc/P5+x3rWFLH7VsfwKOTVNiBemGaIdnRVgzE74epgzZo9PU
fnO2fR4OwLlVcHarWvcxw7Ox8KzaLEi8yJgxY/jvf//Lpo0bEYmJyiMYokb+Cw4epGH9WnhmGCF3
bn5qsVUvN0+FVnbpQnCoYVT/SXhgZmYm0lIAp3+Awwvg00/UAX9vSNgErR3mJ5Uwg3zdunVcOKU6
qGkVOvJrteqYg+tC/fZQpQrs28fOhUsAyMtMYvLkyYV5sKyO3hsjXPBsVBQAF88chVNz1fPOuVB4
WsTGRVQPtr+IB3as5pefV/Hq6H7ItOO8PX4opw5txM3VFQxRhderVOG3e+6xv1tJSZATB0+gElgD
LzztkGy2BMxmM+vXr2fZsmXExcVRr149JkyYQNOmTS8LfYuIiKBljTBa1biHKtnZEBWFJd+XmAQ1
F2niw9DrDpWLCmDtul/hzHd8OnkkK1asuPzD04xBj4VrYNMR2HQUtpyAX3cq8ZRVq/CaMAGACu7u
JSpgfvbZZ6xITeU/QHx8vN2IWrQIUPOTzhihhWsWL77qswCgoADzE09gvvfewuW5lSsJ/RFy8+11
A5S3MOPa1B+Ls3XrVrJKMPjMZjMpKSmkpKQUeuzcDK9cDqjk1UArQxSl2wk3mjz4INSrB3/8Yc+l
9uijqhfuwME9V08Crrn98fCAsWPVmMo776ixpu7dVVrATz6BuJLyKmo0Go2mEG1gXQsmP2g+VYXy
JW6DrY/A/mLqaZ6VVKfTkgPCFTr+DN33qkVaIGGL8lrZcKsGv0ZBvkXNK4n6AlbVgz+6wdvA0S3K
wGnTBv71ALwAhJnUvsxMNU/Jhm00HcBIAMsBBwOwmY8yFPZ/Cd2B5kBL45ozi2DrAAIcjD/hUV6t
lGBgubq6Iky+dsU/UF63u7rbt3+5wB21atG0eVt8PSTZ2dm4FSSxbBeskVOgxz6o9wIsD4V1tiTA
QdDnJRg1FyInwiHDmMUKWXZPDQdfA1sElvCDFh9hNfqiMi2T/aumQO/2ELsMvnsdHuusDBXA+vPP
XIpXnT9pCoQqfaDKYsCYN1VHwD+agNWqFqDg9985NnIkkydNUiIDZjOcPFm4NPPzI3GqO/wxCCYM
gXgj3PHsZGUcdvWGIUPUvpMbYFVDWFVfLT/Vo8m5J9j+mxrhD2j2MF3WncYt4jis2QArV4KU/Nqx
E2YLeGyD56JPMLhWLQAefvRRdd+UvZCsPGe1Kqh/6dOZofb3z/Gr8kjBzWo3Gge1dSXyPXjM7y1Y
VY+h5d8h4rVE3qoDQkpwd2fyf/7D/Zs3q1ghUFLkufHKg1VFhU+2zn6T7KX1mP9gDV4MDWXr6NEq
LO+HH2DfPp4YNIht99/P9/36sX//fiWEMnYsbN5MREQEH374IXPnziUjI4PgihUxvZBMwNNbcG/S
BOrV42X/43gFhBHgDV7uIF08ChMQ/7RmLRyGu9fH4fnKK+q+hoDKsWPHsCSqcNgk29w14aI80K5e
cPBNlfg6cak6doXwwN5du1Ie2AdkZ2fbDazsbDCbuXjxInuPKc29dT/+WOI9irBjB76LF9MZ6Ay0
y8ujYVwcftvAL8mlMBwPgI3dlOjH9XBuJeweS8zSh+nesXlRIZGLF7n09NMsCApiQVAQq2rVgrFj
GVm9OgCN77pLJeqeNw/2Gb8j1aqpOLGoKJg2DeobPxidOxe+3xGGzfXKmdPQtCk0bcqJ6pXJ/o8r
U//ZlDjdK3c6fH2VjPuZMyoriJ+f8mSFhSmv1rffQnx8WZdSo9Fobj+0gXWtJG6HY59Axgm13aiY
KpZnJSWEUWMwDDJD5W5QvrlaYr6F2KUw0OH8Goa6YD5w+jsVytV2IZwdAXPuhpenKu9Wu3bwj97Q
Eri0X6kCJkaBr0MHrHx5+7qRI4njkTDC2OdjeIBMfuDiAf2Bu3+DIdVh7wzjJLsLyyX8abVSgoEF
QGDTogZWxYrw2GPw3JNq2wqcPw8mfzBnkJKSzLsr4av1IMq3gHJ3QPox5e1zMakQTF9D6OOZ0TDI
Av1TwcXw1aysCSdnqfXcRLv3zOwK9V8gz+QBAjys0F0sgp+2wbT+kD0BzoxQ6oy//MLY6GiqegE5
YMENTs+H+/rAH0bHu6I7NPdSIZhGKJ8pNpb6X39N7pKJTJ48GXr2VEqPxnISCJ4AbAeZaHiUKgho
ZYGCDKzmLNYYoZzrF3xGftopkq3VIPAOjid4siS2Bzk5xuefylAekWLiJX5+/mTkAN9C2yNH8UtK
AqBuejrcdx8c/RAy9gDg7aIMQ9+EYPh6nVp+Qi0b4L56dm+gdHHH18NC/VDbF1+Xc3t6QQq0sL0O
+fmqhwVgNQzdyK/BmgenvWFiktLgyDyJ9/Eohqw6w0cXL9L+iy9Ukp1Bg5CtW5OxbBlvAG0Ac0EB
9O6tZtHfdx/NOnRg5Esvce+wYUx87DFl5FzKhfbAKLX41Emmssd51n3eFw8TPP/ieLoaM/Dr129A
fiw0jYXOFy7AF1/Ahx8C8PHHH0Omer7LDSl7AE58qYz4Q2+p/88swyuaeApOXy6p/9TDDwMw8ssv
2bp1a9EwwNRUfHx8sO1xvRa1P6NXut/fn+ebNOHNDh2geXMA+t3fh0cN43nFihVk5FjYuX0ze/bs
ufo9o7+BiGfU+qV9mE98yxOtEmkUcILjx4/bz/v8cyrOns1YYCwwOCUFpk+nlpEComP37krSfvVq
lQMQoIkZ5sxR63l54GnoKAYEFHrT8w0Bzcpmi8qpFRlJ+Nk4vI9biToWeevzeGluGe7uSkA0IgKO
HFE/CVFR8OSTUKmSErCdOFEFAOh8WhqNRqMNrGvn4i+w90XlpQIoX0z4oMZj0HwauBg9U8dAdXeb
R8jhfFu4lRkVShbYRN3DVB+S8+HCBdXh6tbNnsA4fj3IODi7Gbz32+/lKMJg82ZVMNml4r2NTnWV
hyCsl3GvA7DgLKzcprZdHEIOvSob5b2CgdViOmQMAX/jmuBgyNgG/dIK1e9ITAT3QHDzY9zzo6j3
4HTa9X+LBg2MjlmuMexZpS/0PQ8+XdR2jx7Kw+AeAA1fsU+OTzTKmX/JbmAZebQuurRUc8uAkVOM
g3dMhfovgjApD2G3bhzKzCTiIrAekpLTlHGbmwvuhgvM2w3q+sLnk6BbF/jkMazG9KA374GIHduV
KAIoUZFatcj0UBXOPyoQnu3VsXeGgDcq/NKSS4QxB2jrPvB4PJc31oZD+x8YMcefsW/OIdCIyNz4
+hcq5MoxJPKttxi1fDkmGQhmSH9xJFabtFdWlurR1JoEjZ5Xj9VSDqxw11c7YPx4tfyAWmaB+MO4
b+2nEH3OsSvzHk5ZW6t95/Kp8tFq9h/rwOHwVmpf+fIqHmjKFHjVyFeWZXhDQttDxBk414uP9ncH
Y2rcKWBPnTqqLhUqIAoKeNMQ+ZgPVPfyUsm8ASwWPPLz8UNFxrpt3AgF2dDLR7l2JqyGKQfAZRS8
DMTawjuNL/yPPxiZmIjlHg82TKjFsW3boG3bwhgmc04OrmaQAtwcvb7HP4VjH6n1Tuugl3qX0mIi
ObVyKGPHjqUIthxUtsEMx15kSgoVKlSgRsOGAPS45x7+FMNIvmPgQD45eJA3t2yxp2gosIuOLF++
nKSUTI4fOcDBYnOdHJkzZw7LvnyO83u+5ZNPPoEmE+k68w6sVqgUYIQ12jDm7v3g7s54Ly8+NTxX
hdhSDSQn238D8lCJk2rWVOI3icbvW2CgMrBq1aLqsL4wDc7Nn6xiygxhG9LA3Q3ybR5EjVPToIH6
OYiJUVHPU6Yo4drJk9V4T0AAtGypHMmzZiln8g3qvmg0Go3TcktFLoQQ3VGyB67AN1LK94odF8bx
nqjAr2FSyr1Xu1YIUR74L6o/dhoYIKW8ZBx7FXgKJZI+Vkq5ttQq414OkJAZrUKL3HyKHverDQ3G
XeHaqxhYBYCPQwdn3Di1OGLOhvBn1RyvMScg+yA0z1JDiQUFRUMEbXOzLPlq/hZAleb2422/U56w
1ENgGmGf32UtYebylQysjZtgxnz7ds2acGm58tJ5GXVKSIB7xkGDcXS15NE164xSxbPlt7IZWIUG
lCHV7qii2HSSUmbc/ABkn1PPwZprN7AML1Ot1kPAaxtkQ0Vb/pZaraBFR2jxYeHtlq9bR35YZdzT
JTm5BVgsnriSC26GgVUuGNrMh+VhcOd0eG4h8RuzqHx2BaZMGHJ/a9iwkXwvL+I2baJatWok/Xso
vm/Pw5TpAZlGeGClqpABSAvCzYc8zyzItYuIhBhzwho2bEjXSlupa+tPZxSozq27e2GZSU7GtH8/
sXdl4Au8P2sGb7XqpuxJ29y7S3ngp4xib1dXuAQiM1uFkT7zjJoDuHYmHIUC79GYHnkXTOq7bVXL
BSxCvQcJ6ju448lPuCMzE366V70DZ88qj6QtzMy1LjyyG9x84duOEJdOSp3WcOkXAA7UrEH77duh
QgUsDz6I66pVVDNyoe27cEENHkChsbJh9myODh/Os5cu8e9hw4Az8JhhwATfo8rqWQMuGGUMAVyM
72zDBtwnTYIfG9Hrzl7qf+GllwoHOBoZAhpmD0GlypXtz9XkD2bjvfaqBP7qpTJnmAnygVOnis0j
shlYthBcW4+xUiXIyyMwMJD2XbvCkSPc36YNf4rtfXeYP2ktXx4XID6xPDa9QC8vL3IL1NyzJEdB
jWKcO3eO1qZsziZAvPG/1bhJc9Jyd9CmeVUV8mfDyPc28KefGGgz1v39weZhsoUnOhpY+YbRf/So
MrTijJxxAQHg5gYnT1L9ws+waTlVunaBCk1VTxwgFcaMGkmVYmImzsLNtGN/Z4RQY4DNmyubOzVV
6S5t26aW2bPt4xBCqPGDunWhVq3C8Slq1VI/4wEBWjxDo9H8vbhlBpYQwhX4AugCnAN2CSFWSimP
OJzWAwg3ltbADKD1n1w7HlgvpXxPCDHe2H5FCNEQFYTXCAgFfhNC1JVSlpDF9AZwNyZQuXhCyH3X
1xrY5jQF2rTbUV6fSsEgE8Cnhto3ZYoymN56q+j1bt725KNNj8LqhkoZ7tAhNYzYtKn9XFtHKj5P
CWkA1DPC705+C9Ffwf2bwbsqVACSjHMs1svLfSUDK9KQIm/fXnlI6teH/emAK7S0wAbIOXuWcaNG
cfHiRWoHZTGt86/QfglUM+TKbdLsnkZX0iY979gJXtvGnlA257w9X5atn/nBBzBlCvl52bh7A8nQ
0BNl3IRcLmkdHBJCirugfLpEAtZcP2VguRhuN08znIuE4cAbh+DfULlFC1ixAlIhKeJXAKJzcojc
vp1q1apRo4XKbyZSPeCSUb7KNVUZfKpBz30MePp5+PRTHm7shfcDTQlrpbxDX331Fenf/4zMUqqA
3hZXewfeRrlykJZGeh7QAeLPgoft3bMZozs+gjzjOeXng/FoadgQ3n9fecRGz4KjFkwXEgqNKwDy
EsCvHgg3SDLeTZ9MOGp4imrXVpMvvvwSvIUKDc0029Ueq1aFbduoGNKP3AgVwVqxUTgVKlQgPj6e
ZatW8U9g/08/0UUIVeadO9W1QUEQHU3nnj3p/NFHMHw44tw5hvZuy9ynIVe44Gkra5jy7sjUfDKG
nMI/0DDM4+KUofaQSuQcGxvL0lOnOH78OFUPHWLCU0/B229jCvSmR48e9nqb/MErDO78WIWoBqh/
BFMe+HtBatL5wlMTEhIwV61KUFQUHmFGygZbz3HWLPsAhzEnrCAtjUTDiPT09KS8YwgvSiVRnj6t
xlsMA8tsNvPuV1/xOvDllCn8+803cXV1pWfPngQm/kjzcoGkN1eexosXLyINL2elSpVwcXEhLi6O
SlUhOg5Cqqt3/7PPPoPIILr51YUaLUlNTSU7O5uKsbHK0ez4v1a5st3AcvRg2eqbZxhYtnOyjHfd
UaDEaniobKG9Nvn/NGhQrzZZLi5cMJ6Lt7f3ZeImmZmZhTnnfHx87Amuy5Cbacf+6rKWNYGBKgDB
9m9mtarmKTJSCU1GRqqpq1u22F8jGx4e6ic7JES9NiEhqokMCLjy4uurxgLc3bVxptFobj9upQfr
LiBaShkDIIT4AXgIcGyYHgLmSdVb2CGECBRCVEZ5p6507UNAR+P6ucAm4BVj/w9SyjzglBAi2iiD
Q7Kom8Dd6PhW6Q2VrzP7os2DVT6QQgOrd2/oGw4zu8KqfSqkb948NbR3NaLTYTEQFgO/j4GUHOh4
0n78jCEdmOYQZuaaBKcWwoXVkLzTmPdUCYIE7DfOczSwPjVk+XKMDvypGBjUGS4kQ4EZThthklu3
wv33Q8ZJJbTg7g8B6YAFz2kTedM1E19PQy8iEvjuBWACxGUoeXYL4L8AvDaojmaLFjBpkr0c588o
mfYCQByFr+6DvEAoSAUXAR99BLVr4x6/pDAc8ikvXyATxvUGN6m8fq5ehUZUoJuEfRAmvHDNNRT+
Ig2FNpEAO7ure21YBdbOsNcYqd/vzwhUJ75aRai+fTwcmASphoUamw25Rq9h52n1prpvhm1taR6k
DNQqsTmMWHMIfn0KXlen+luSkcardde5FPDyUXmbMk8qw+i4Mr7q/uYKIRbeq2jCZY8hBZ80R/3d
MQtMhvz7tgylxACq1/HddzBgAIQGACnwy2olTAAq3DL1ONSxQrY77DWex7M9Ic6YR+RyBJImwN5j
4G1Wiapjz6v3AeDASTh3njGRkYjDJqCAduIATGhEMJJ+9YFj0M7bivTxQfStDWcMy98/CZoHws9d
YYd6Rj7rV/O1NzAcrC0lbBsJQHJUFEGA+NZK3rrGFARVwi2tAHE4QUn696kP5kyC83J5NC4ZzzRU
Eq0FhgfTaoGfHoNgX0jIgo37IS8JImcAMyBP1d3PLOBzyebcs9AyFKQZ95Qk3KSkINQXi6cJby9P
2G38nx1+B7a+rNb/UO+Cy4z3yPjvZKwCPE2edg+p7WvJzsEcp1ytZ1f/m2qJ3+CG0lchEl7xB/lQ
fXBz48GUHOXZFElAN5AWPNJSybHZHp5BUODOZ1LishEaCg+k5/uw6St1PDFLqWNaR2HKysYzPx83
Wwq82MmwMZPcC3GQGo2tlGnLXyag5neQkwTC+F3YeRj6NIC9hvcxy3hXDj4F5z1gWixYs6AOcOxp
5d00nimH3GDw+7jkTcIzW/3+mdzdoWo58HKDAk9IzsYlKw3PHOUZNHl4QNVAlYohz6NwHl0ZcMPt
mJTy4l9f3NsHFxf7dNW+fe37pVQO4ZgYtZw7p6Ykxser8ZKzZ9Vcr6Qku9P8agihDC0vL+Vcta3b
FpNJOVltf4uvl3TM1VXd18VF/XVcv9q+az3/SvX4f9qv0fzdEdJxvkdp3liI/kB3KeUIY3sI0FpK
OdrhnFXAe1LKrcb2epSxVONK1wohUqWUgcZ+AVySUgYKIT4HdkgpFxjHZgE/SymXFCvXSGCksVkP
OI7z4OhzclZ0HcoeZy8/6DrcLvyVdagupaz456eVHjfTjkkpdxe7l2Pb0xiMERvnxNnfXV3+skWX
v2xx9vKDE7Q9Tp1oWEophRDXZSFKKWcCM29RkW4pQojdUsqWZV2Om0HXoexx9vKDrsPtwt+hDn8V
jm2Psz83Xf6yRZe/bNHlL3ucoQ63UkXwPFDVYbsKhTpjf3rO1a6NN8IIMf7aZpxcy+dpNBqNRnOt
3Ew7ptFoNJr/U26lgbULCBdC1BRCuKMEKFYWO2cl8IRQ3A2kGXHrV7t2JTDUWB8KrHDYP1AI4SGE
qImacBxxqyqn0Wg0mr89N9OOaTQajeb/lFsWIiilNAshRgNrUfK2s6WUh4UQzxjHvwTWoKRto1Hy
tsOvdq1x6/eARUKIp4AzwADjmsNCiEWoycdm4NlSUxC8fXDK0MZi6DqUPc5eftB1uF34O9ThitxM
O/YnOPtz0+UvW3T5yxZd/rLntq/DLRO50Gg0Go1Go9FoNJr/N25liKBGo9FoNBqNRqPR/F+hDSyN
RqPRaDQajUajKSW0gXUbIYSYLYRIEEIccthXXgjxqxDihPG3nMOxV4UQ0UKI40KIbmVTajtCiKpC
iI1CiCNCiMNCiOeM/c5UB08hRIQQ4oBRh7eM/U5TBwAhhKsQYp+Ro8cZy39aCBEphNgvhNht7HO2
OgQKIZYIIY4JIY4KIdo4Ux2EEPWM529b0oUQzztTHW43hBDdjWcTLYQYX9blKQndDpVtHXQbdNuU
36nbIGduf/42bY+UUi+3yQLcA7QADjnsmwqMN9bHA+8b6w2BA4AHUBM4CbiWcfkrAy2MdT8gyiin
M9VBAL7GugnYCdztTHUwyvUi8B2wytneI6Ncp4EKxfY5Wx3mAiOMdXcg0Nnq4FAXVyAOqO6sdSjr
xXiGJ4FaxvtwAGhY1uUqoZy6HSrDOqDboNul/Kdx4jaIv+rLyrYAACAASURBVEn7gxO3PWX+8PRS
7AuBGsUatuNAZWO9MnDcWH8VeNXhvLVAm7Iuf7G6rAC6OGsdAG9gL9DameqAysOzHujs0Lg5TfmN
cpTUuDlNHYAA4BSGkJAz1qFYubsC25y5DmW9AG2AtQ7bRZ7X7bToduj2qINug8q0Dk7bBv2d2h9n
bnt0iODtT4i051SJA0KM9TAg1uG8c8a+2wIhRA2gOWr0zanqYIQ27Eclsf5VSulsdfgE+Bdgddjn
TOUHkMBvQog9QoiRxj5nqkNNIBH41giT+UYI4YNz1cGRgcD3xrqz1qGscebn45TfubO2Q7oNKvPy
g3O3QX+n9sdp2x5tYDkRUpnmsqzL8WcIIXyBpcDzUsp0x2POUAcppUVKeQdqFO4uIUTjYsdv2zoI
IR4AEqSUe650zu1cfgfaG99BD+BZIcQ9jgedoA5uqDCrGVLK5kAWKqShECeoAwBCJdjtDSwufsxZ
6qApPZzlO3fmdki3QbcFztwG/S3aH2dve7SBdfsTL4SoDGD8TTD2nweqOpxXxdhXpgghTKhGbaGU
cpmx26nqYENKmQpsBLrjPHVoB/QWQpwGfgA6CyEW4DzlB0BKed74mwAsB+7CuepwDjhnjDwDLEE1
eM5UBxs9gL1Synhj2xnrcDvgzM/Hqb7zv0s7pNugssPJ26C/S/vj1G2PNrBuf1YCQ431oah4ctv+
gUIIDyFETSAciCiD8hUihBDALOColPIjh0POVIeKQohAY90LFbt/DCepg5TyVSllFSllDZRrfYOU
8nGcpPwAQggfIYSfbR0Vg30IJ6qDlDIOiBVC1DN23QccwYnq4MAg7CEa4Jx1uB3YBYQLIWoaI7MD
Uc/MGXCa79zZ2yHdBt0W75BTt0F/o/bHuduesp4Ephf7gnqRLgIFqBGIp4Ag1GTRE8BvQHmH819D
qaUcB3rcBuVvj3LZHgT2G0tPJ6tDU2CfUYdDwBvGfqepg0O5OmKfYOw05UeprB0wlsPAa85WB6NM
dwC7jXfpR6CcE9bBB0gGAhz2OVUdbqfF+D2MMp7Ra2VdniuUUbdDZVt+3QaVfbmdvg1y9vbn79D2
CKNgGo1Go9FoNBqNRqO5SXSIoEaj0Wg0Go1Go9GUEtrA0mg0Go1Go9FoNJpSQhtYGo1Go9FoNBqN
RlNKaANLo9FoNBqNRqPRaEoJbWBpNBqNRqPRaDQaTSmhDSyNphQRQgQJIfYbS5wQ4rzDtnuxc9fa
cm1c5X7nbDlRStj/X4ftgUKIb0qpDlOEEM+Xxr00Go1Gc+vRbY9Gc3vhVtYF0Gj+Tkgpk1H5JxBC
vAlkSimnOZ5jJMIUUspuN/lxrYUQ9aSUx2/yPqWGQ92sZV0WjUaj+X9Btz267dHcXmgPlkbzFyCE
qCOEOCKEWIhKXFjZcYRQCPGTEGKPEOKwEGLENd72Q2BCCZ9VZBRQCHFMCFHFKMMhIcR8IUSUEGKe
EKKbEOIPIcQJIURLh9s0F0LsMPY/6XCv8UKICCHEQSHEG1eq23U/II1Go9GUOrrt0WjKBu3B0mj+
OuoDT0gpdwOoAbdChkopU4QQ3sBuIcRSKeWlP7nf98BoIUTN6yhDPWAAcAzYC+RKKdsKIfoB44H+
xnlNgLaAP7BXCLEauBOoBrQGBLBGCNEWSCheN41Go9HcNui2R6P5i9EeLI3mr+PkVRqBF4QQB4Dt
QBWg9jXcz4waSRx/HWWIllIeMcIojgDrjf2RQA2H836UUuZKKROALUAroCvQA9iHaiDrAHWN869W
N41Go9GUHbrt0Wj+YrQHS6P568gqaacQ4n7gHuBuKWWOEGIr4HmN95wD/AuIcthnpujgieO98hzW
rQ7bVor+HshinyNRI4dTpJSzipW/Dleom0aj0WjKHN32aDR/MdqDpdGUPQFAitHANUKN2F0TUsp8
4DPgOYfdp1EhFQgh7gKq3kCZ+gghPIQQFYEOwG5gLfCUEMLHuHcVIUSFG7i3RqPRaMoe3fZoNLcI
bWBpNGXPasBbCHEEmALsvM7rvwYcZXgXAyFCiEPASCDmBsp0CNgM/AFMlFLGSynXAEuAHUKISGAR
4HsD99ZoNBpN2aPbHo3mFiGkLO6N1Wg0Go1Go9FoNBrNjaA9WBqNRqPRaDQajUZTSmgDS6PRaDQa
jUaj0WhKCW1gaTQajUaj0Wg0Gk0poQ0sjUaj0Wg0Go1GoykltIGl0Wg0Go1Go9FoNKWENrA0Go1G
o9FoNBqNppTQBpZGo9FoNBqNRqPRlBLawNJoNBqNRqPRaDSaUkIbWBqNRqPRaDQajUZTSmgDS6PR
aDQajUaj0WhKCW1gaTQajUaj0Wg0Gk0poQ0sjUaj0Wg0Go1GoykltIGl0fwNEUKcFEK0uYbzPIUQ
UghR5RaUobsQItphO04I0d5Yf0sI8Xlpf+btjhCio/HdZAohupfyvYs/71J5B4QQTwkhfirpXCHE
HCHEv0qrDhqNRqPR/B3QBpZGcwsQQowWQuwWQuQJIeaUcPw+IcQxIUS2EGKjEKL6Fe4z1OiMZwoh
coQQVoft1Ct9vpSytpRyeynUY4cQItf4vEQhxCIhRMWbva+UcqKUcvTN3qc4DgZAllHmc0KI94UQ
4hqvL2Kk3ALeBqZKKX2llL+U8PlxxjuR+T/2zjtcjrL6459vGklI7qUFCCEhgIgoTQj8qKEoUqQq
SBQQUJooAqKAtBCVpoiCINIJRelVEOkgIiVUCSgIJJSEUAI3hfSc3x9nhp27d/vuzS05n+eZZ3dm
3nnfM+/O7s6Z73nPK2mypEsl9auloUZdA2Z2mZntUmTfAWb2a1gkfRcEQRAEXYJwsIKgfZgE/Aq4
PH+HpOWAW4CTgWWAccD1hSoxs7HJzfgAYBfgrXTdzJYqUHevBp5DykFJ+2sCywNntkMbjWbNxOav
AgcC+3awPSmrAOPLlPlaYvsIYDPgp+1uVRAEQRAEDSMcrCBoB8zsFjO7DfiowO5vAOPN7EYzmw2c
Cqwn6Qu1tJWoHj+VNB6YltmWhuNtLulJSZ9ImiTpd7U4YmY2FbgDWD/Tdj9JFyRqyzuSfiOpdwU2
nynp0uT9FyTNl3RgUscHkn6WKTtA0p8T+1+S9PNKlRIz+w/wRJ7Nhybq4XRJ/5P0vWT7ssCtwGoZ
lXBZST0lnSzpDUkfSrpWUhvnNlP/D5PwvI8k3SJphWT7O8BKwL2SZlRg+7vA/bTt799Lejv5jP8g
aYkidlR7DewuaULS/6elqp+kwyTdX6SN6ySdVKTvVkmUxKZM+c2S9nuWO/8gCIIg6KqEgxUEi54v
AS+kK2Y2E/hfsr1W9ga2A5YtsG8e8KNk35a4EnZQtQ0koYG747amjAHWBdYBNgS2BmoZk9MTV2w+
B+wEnCZptWTfr4BBuPrzdWC/Kmz+ErBpns2TgR2BJuAw4AJJXzKzj4A9gDcyKuFHuIL0NWALYGW8
P39XpL2dcGVyD2AI8CFwNYCZrQy8T06hKmf7sKTdrO3nJDasgyuKnweOL98TFV0Du+DO3MbAt4F9
KqgXgCJ9NxF4Evhmpuh+wLVmtqDSuoMgCIKgqxEOVhAsegYALXnbpgED66jzd2Y2ycxm5e8ws6fM
7GkzW2BmrwOXAltVUfdFkqbhzkE/4OjMvn2A0Wb2oZlNwZ2hih2gPEab2Wwzexr4D+64AXwL+JWZ
tSQ37X+soK7xkmYCLwF34ecMgJndYWZvmnM/8AjuPBXjMOD4pH9n407l3kXGde0DXGxmLyZljwW+
KmnFCmxO+Zuk6cBEYALep2n45/eBI83sEzNrwcM1R5WrsMJr4Iyk3jeB83Enq17GkoRnSuqDf5ZX
N6DeIAiCIOi0hIMVBIueGbh6kqUZmC5pWCbEqmwYWYa3i+2Q9EVJf5M0JXGUTgGWq6LuQ82sCdgA
WBEPcyNxMFbEHYGUibhyUy0LzOzDzPqnwABJPZI2sudX9FwzfAl3WL8LbA70T3dI2lXSU5KmyhOF
bEuR/kjOcShwdxJe9wnwHP7bWUgtXIlMf5jZJ7jzXE2f7GhmA3H1am18nF5ad2/ceUxtuQ0fF1eS
Cq+BbL9OTNqrl5uBjSQNwZXJd8zsxQbUGwRBEASdlnCwgmDRMx5YL12RtCSwOj4uK5vEomwYWQYr
se8S4Flg9cRR+gVQUVa9Vg2YPQf8GvhDsm7Ae3joXsow4N1q6y7R5kJgCh4WlzK00mPN7GrgReDn
8Flf3wj8Elg+SRTyILn+sLw6DD+fbc1sqczSN88hTJlEpj+SsVpN1NAnZnYfnvzkrGTTZGA+/jmm
djSbWSFHL59KroFsvw5LzqUqkwucwwx8bNZ3cGUz1KsgCIKg2xMOVhC0A5J6SeqLjy3qKU8fniYV
uBVYW9I3kzKjgReShAztwUCgxcxmJGOSDq6jrkuBz0naPln/CzA6SQaxPHAicE195rbhBuBESc3J
uKQfVHn8GcAPk0QM/XAV6H1goaRd8XFjKVOA5SVlnds/AWdKGgogaXlJBdOW4/1xsKS1k8/2TOBB
M3uvSptTfgvsJmktM5uHZ6U8V9JycoZK2q6Ceiq5Bo5L+ng4Pl6rYGbLEhTqO4Cr8PFeOwDXVlln
EARBEHQ5wsEKgvbhJGAWnoBg3+T9SQBm9gE+8P804GM8qUDZcTR1cDRwUBJyeAHV3zh/RjLG63w8
kQN4qNnLuCr3PPBPXOVqJCfh/TQR+BvucM2p9GAzG4enwv9Jojr9FLgTz/C4O3B3pvgLeKbEiUkY
3jL4+dwPPJiMjXocD5cs1NZfcYfuDlwBWpHax6RhZpOA60iuHeCopN5x+Di+e/DEIOWo5Bq4Cz//
cbjKV62jXKjvAB7CHdvHzGxylXUGQRAEQZdDHgETBEHQNZB0NLCDmW1ftnDQKZD0OPBHM2u0uhkE
QRAEnY5QsIIg6NQkYXCbSOqRhLcdiYdZBl0ASZvj6eRv7mhbgiAIgmBRUPVko0EQBIuYJfCxR6sA
U/FxPJeWPCLoFEi6Dtge+GGhKQSCIAiCoDsSIYJBEARBEARBEAQNIkIEgyAIgiAIgiAIGsRiHSK4
3HLL2fDhwzvajCAIgiAIGsAzzzzzoZkN6mg7giBYvFmsHazhw4czbty4jjYjCIIgCIIGIGliR9sQ
BEEQIYJBEARBEARBEAQNokMcLEmXS3pf0kuZbctIuk/Sa8nr0pl9P5f0P0n/lbR9sm0JSfdIeknS
4ZmyF0sqOAloEARBEARB0H5IOiCZ1LxTIWmCpJ9WUX5rSSZpuXayxyTt2R5157XToZ+HpL9KurKj
2u8oOkrBuhLYIW/b8cADZrYG8ECyjqQvAqOALyXH/FFSTzz172PAusB+Sdn1gJ5m9uwiOIcgCIIg
CIJ2IXm4bJLOz9suSadKmiRplqSHkzkCS9V1avahdgNtLOQkXA+s1ui2CrRdrQO0EfDH9rSpSgYD
d3a0EYWo1hkN2tIhY7DM7FFJw/M27wZsnbwfCzwMHJdsv87M5gBvSvofsDEwD+gP9AaUHPdL4LB2
NL3T8cgjcNVV/n7LLeGAA9qvrTvugNtvL12mZ084+mhYa63q6r7sMnj8cVh3XTjyyOptM4Pf/x72
2gtWXrm6Y88/H557rrpj+vaF0aNh+eXb7rv1VvjrX6urrxH07QunngqDKhjebQZjxsDbb8NOO8E3
v9m2TEsL/OEP8POf++eaz4UXQqEhjOutBz/+cdvtCxbACSfAhx9Cnz5w8smw0kpty911F9xyS9vt
yy4Lp58Over41Zo718/nk09Kl1tlFbdPKl2uEqZNg5NOgpkzfV2Cgw+G//u/tmXHj/fruE+f4tdX
yp13wm23VW7HUkvBGWd43aV4/nm44AJYuLD19v32g623blv+jTfgrLNg/vzK7PjqV+Hb366s7Lhx
8Kc/+fVaKauv7tdZKd59F371K78ewK/vn/wEvvCFytqYORNOPBGmT6/crnx693Y7hw1ru+/ee+H6
6/19Nf1ViDlzvJ3sNb/PPrDttm3Lvvmmf5bz5hWua4cd/Dc2n6lT4ZRTvK0f/ch/A6rhmmvgoYeq
O6a7I2kT4BDgxQK7jwWOAQ4A/gucAtwnaU0zq+OqbAzJnHedZt47SX3MbK6ZfdDRtmQxs/c62oag
HTGzDlmA4cBLmfVPMu+VrgPnA/tm9l0G7Ik7h38GngO+A+wKnFpBu4cA44Bxw4YNs67ON75h1quX
Wf/+Ziuv3L5tbbqp2RJLeDvFFjA78cTq615hBT+2Z0+zhQurP/6NN/z4jTeu/ti+fc0GDix9Xtll
xRW9rauvLlzfRht5nZXW14gltenaays75ylTvDyYbbBB4TKHHeb7b7qp8P6BA80GDGhtx8CBfj0W
+gxfesnrW2opf7344sL1brWVWZ8+retddlk/5sUXKzu/Yjz5pNez3HLF+zK174MP6msr5a67vL7l
l/f6e/Y0O+CAwmWPPz73uVx1Vel6N9+8/PcxXZZbzut84ony9h5xhJnU+vhevcx2371w+TPP9LqH
DClvR79+ZmuuWd6GlEMPNevRo/LvQXOz29LSUrreiy7ycoMH5363Tj65crseeMCPGTSotu/rSiv5
8eeeW7j+7bf370C1/VWI/Gu+d2+zXXYpXPbss4t/ln37mq2zTuHjbrwxd90eeWT1Nn7uc36ujfgt
BMZZB93XNGoBmoHXgW3wh83nZ/YJmAycmNnWD5gOHFqkvgMAy1sOyLR1MfB+UscjwIg8W65O9s8G
3gCOSvZNyKtzQqa9GZk6TgVewqORXk/auQ1YLlOmF/A74GN8QvmzcbXp4SLnNLzAOV2Z7HsYuDCp
4wPg6Yy9P83U8RPcgZ0JvItPYL9UZv/WSb3LleuLIjYOBW5PzudT4D/AqMx+A/bMO59RyWcwC7+/
XRdYG3g8sfMxYNX8vi3wec8osb56Ytd7SZ3PAjtn9j+c37eZfZsl9n2a9NmFQFNmf388Sm0GMAU4
Afhr+tksTkunzCJoZiap5DNLM5uPO1ZI6g38HdhN0jnAMOAqM7ujwHEX4z8mjBgxosvPstzSAhtv
DBttBFdc0f5t7bwz3HRT8TJLL+1P7GupG1zlmDUL+vev7vg5c/z1ww+rP272bFcYTjyxsmOmTIEV
Vyx+ni0tsNtucN111dlSD5MnuxpUad+n/Q3Fj0mfeM+e3XbfggX+9H70aFfNUs480xWi2bOhX7/C
bV58MXzrW6X7b/vtXTFNufde31bLtVXIhltuccW3EGPHuhI8bRos14DI+7TNRx5xheRLXyp97inl
zrWlxdXHQmpfPv/4B4wcWVn/tbS4qjJhQm7bFluUtrlnT1dDyyl+Bx/sCmWltLS4IvXqq5WVv+QS
OOQQt7WpqXS94PUOGADNzdVdW+nx994L669f+XEp8+a5kliqT7faypXUavqrWF3gyvoWW3i9pdqV
4K23oEfeAIL99/druFQbUPvv//77uypeL41QnTsBFwM3mdlDkkbn7VsVWBG4N91gZrMkPYrf/F5U
oL7r8Zv0nclFCrVIEnAX0JLsmwrsDzyYqGGTgV8B6yT7pyTtp3ESG+HOxsH4TfSCEuc0HNgb2ANY
ErgOOA04NNn/U9wROAh3xg4H9sGdjEK8DXwTuBkfQjKV1qrZvng/bkkuyimfhcBRuKO0CvCHZNmv
SPlSfVGIPwJ9cUd5GrBmibIpY4CjE5suBP6C9/GJyetY4DxglwrqKsYA4G/ASXif7Q3cImldM/sP
8A3gBeDyxAYAJK2DX3ej8c9pGeD3Sbk0TPRsYDv8s3k3KTsSqOCfqnvRmRysKZIGm9lkSYPxCwn8
AxqaKbdysi3L4cBVwCb4D8XewINAGweru9HS4iFhzc1+w7twYds/xka21dxcukxzc+s/20qYO9dv
yIcM8dCdlpbqHaxaQ3XSm4Fy55UlLVvsPCvpp0ZTzqZ80nJDhlT/eUGuv/PPM2tHvoOV9vVKK/lN
UKn+W3vt4vXWQ3p8qc+nUW0Va7PUzXxLi4e4vvNO+faruc7ScpU6WIU+1ylTSpev5Ma22t+Har9L
2c+uVKhwS4v/Ti65ZO12Zdurlt69/ftR6juw8sq1/Z4WqgtaX39vv128bFNT4f+QUrbU+3vSEb+Z
nRVJBwOfwx2EQqyYvOZ/I6cAQwodkDhgM4D5lglLk7QtsD4wyDysD+BkSbvgTsavccfjWTN7Ktk/
MVPvB+6j8YmVD3frhatmLUnbFwMHZvYfCZxlZjcn+4+i7Vj97DktkDQ1WX3fzPIfr75pZseUMsjM
fp9ZnSDpWOB2Sfub2cIChxTtiyKsAtxsZi+kNpUpD3COmd0NIOm3+Bitk83soWTb+XhkV80k9ryQ
2XRa8pnvCfzKzKZKWgBMz/tcfwZcb2a/TTdI+gHwnKTlcVXr+8D3zOzvyf4DgXfqsber0pnStN+B
Pzkheb09s31UkjVwVWANIL24SbIN7ow7WP3xJxKGS+bdnvRJbVOTB2jMaMc8MeWeCoPvr/YJZlp+
6NDW67XUUetx5c4ryxJL+A1SsTYr6adG06+fqwiV9kO2z8sdU2hsTbF+S9cL1ZneeC21FAwcWF3/
laq3Gir5vBvVVrE2m5qK34ROm+YPTEqpG9mylV5nablKbn6L9X8jrvemJvj008rHa1X7Xar0s0vr
TZ3Can+3avndyKeSPq22v4rVlbZXabul7LUCcR9pfUOGVP+9mTPHH7It6t/MzoikNYHTge+YWZGR
cA1lQ/y+6QNJM9IFV7tWT8pcCOwt6QVJZ0vaqsa2JqbOVcIkYHkASc244/jZvZ15vNlT1M4z5QpI
2jbJWv2OpOm4ytKHnBObT7V9cS5wkqR/SfqVpA0rsDs75i51ov+dt21JSVU+hs4haUlJv5b0sqSP
k898BB79VYoNgX3zrpV/JvtWT5Y+wL/SA8xsRp79iw0dlab9L/gHsGZyYX8fOBPYTtJrwFeTdcxs
PHAD8DJwD/BDM8vK0KcApyVPG/6Oy8H/xuNkuz3pk79GP3XPJw0Jaw8FK9/BqvUJaC3U8iRaKn6e
8+Z5iOOivlkoZVMh0nJDh/oNThpiWYhSzlIpBatYPen1WqiMWXEFpVi91dBRClbPnjlVtpyCVap/
UhYu9DoWpYLVCMW2Wue1HgWrmnprVbDq+Z6X69Ompuo+t2IUUrBq+Sybm/26S5O15B83YAAss0z1
35t61cBuxqbAcsB4SfMlzQe2Ag5P1pfAx80ArJB37AqZfZXSA79hXz9v+QJwMoCZ/Q1XYs5ObLtL
Ui0DEvIdRqN970ELXKk5JK2Ch0e+AuyFOw/fS3YXTAVUbV+Y2WV4GOEVwOeBxyWdWsbubD9ZiW1p
3y2kbQhk7zJtnI2f88n49bU+7syWSYFED3ycWvZaWQ8XPp4vc+xiR0dlESyWE+krRcqfhsfqFtp3
dOb9bOBrdRvYhcg+6UzX24NUGatEwSoWSlSM7M0+dH4FKy1fqM1aQg4bRTVP4QuphsWyD5Y6z1oU
rPR6LVRm9mx/Wt/eCtbAgcXLtIeCla+WlFKwVlml/Gc5c6Y7o5Vet+n5VqpgrZk3UqBRClbWYVhm
mcpsaU8FK3vcRx9V3s60aa4a9y53G1OCYn26YIH/3jY3tz6fSvqrmK2QuwaySlR+WGc5BSstM2BA
4eOamjwTYS32hYIFeOKH/NysVwCv4crWXDzM7D18nMvTAJL64g+Xf1ai7rlAfj7YZ3HHbKGZvVHs
wCT87mrgakl/A/4i6TDz7M7zCtRbFWbWIuk9fEzXg+Cp6JP1Uk5jkge0pvZH4A7F0elDe0k7V2Br
qb4oVP4dfCzYxZKOw0MhT63B3mJ8AKwgSYnqB+74lGILPE9BGo7ZF1efsqNdi10vXzKz/xWqVNLr
+PWwCT6GDElL4oro6xWfUTehM4UIBlWSqiWLQsGq9Cnj4qBgpeULtdmIJ9u1UquClV3PMmtW8X21
KlhSLqlANfUuuaSPC2mEgrXkkqVTvbeHgpWvltSrYFV73fbq5eddj4I1e3YurXm58sWoJlSx2rph
0SpY9T5EKdZmOr4xq2DVcy2m6lI61UJzs/93FEpeU07BKmZLpddtMfuy9S/OmNknZvZSdsGVmKnJ
uiU30b8HjpP0DUlrk8va9ucS1U8AVpG0gaTlEjXsfjzE63ZJO0paVdKmksZI2hJA0i8k7S5pDUlr
4QkQ3sg4FBOAr0haMRmyUSvnAsdK2iMJlfwtPk9UqWRkE5P9X5c0SNKAEmXzeQ2/Bz4qOe9v4wkv
ilJBX+SXP1fSDpJWk7Q+Pqbs5SpsrISH8WQTJ0haPYkIKzd58avAHsm1sA5wDZ6MI8sEYEtJQ5Sb
Z+wsYGNJf5L0ZUmfk7SzpIvgs3DAy4CzJG0nn5vtcvIcNUlnSHqg5jPuIoSD1YXJPvlrbwWr0qeM
tYzBaqSCVWh8QCXHLa4KVpoIoNS5NFLBGjjQHaVy/Zdfr1TbtVWo/kqu4awt9VJILfn008JzDWWV
gFLt13LdllLOStmbbafYZ1aLglUOs0WrYFU7BqvehyiV/IY04losdK7F6qxUwSp2XD1jcEPBqopf
4ynNL8AVr8HA16z0HFg3A3cDD+Cqx7cTZ20nXDW6BJ9T6wY8292k5Lg5eATRC7gzNpDWGeyOwbPk
vU3xjH+VcDauDF0BPIGHvd2Kp0MviJmlWepOw0MdK07+YGYv4mrST3Cn5yA8k2EpyvVFPj3wrIQv
A/clNu5fonzVmNkrwA/IzZm2Ha52luIneCK5f+DZBJ9I3mc5BU8w9zp+vaR9NhLPCPkI3g9n0Drh
yk+Bh/DP7iE8I+SjeXUPJjfGr9vSmbIIBlWSffIXCpa/fvppbcfVomBNLJA/qKMVrHfz82sWIc3y
l6YhL6UmNVLByo4DeaNAQEqpz6NRGdXKfdZ9+3qSifZUsMCVimzYV3b8WXNz6VCrWq7bSlKRp+Px
Sn2u+anr20vBmjHD+6SacxwwoHSGypSWFvj853Pr/wLZQQAAIABJREFUnUnByv6GNErBKnT9tbTA
CiuULptvbzFbWlo8eU1zc+7hQaXhk6FglcbMti6wzfAws1OrqGcOBZSNxCk7MlkKHVd0iEay/048
011225W4qpaut7G1QJn5uIL0mYok6Tl83qeimNkvgV/mbdu6SNnheevn4SnPs9yQ2f8wmfFN5fqi
QHtHlNmfrXsCeWOpzGxcgW33FNh2EW3T85+b2X8lrft6Ip7rIMvZeXU+gY+vyrd5HKWzO84Evpss
xcocUGxfdyIUrC5MZ1WwyiVNKFZ3KTWl0joW1dPT7qBglbtualGw0nEe5Z6OV6tglTqmGipVHhrR
VrE2i/X5rFk+/qY9FaxKlJ1C9RazuVqVqRoFq5Zz7NGjdIbKbN35n8msWYVVxUqOr4XuqmBBddNm
hIIVgCedkHSIpDUlrS3pXHyS3bEdbVsQ1EI4WF2YzqpgQXU3BGndyyzj40TqUbCquUlKj1tiCV+q
oTuMwSp33ZRTsLLzCKX07u3Z8kqN0yhla2dQsBrVVrE2i/V5/ve5kWOw0rKVKDuF6i1m8+zZ/n1r
DwWr0eMj8+su9JlU+rvVKAVr2jTPzJdfNywaBStLdkxvMXuL2VLJ70kp+7L1B4stC3HV4yk8y/Qm
wI6JYhIEXY4IEWwQLS1w1FHwu995qESjueoquPLK1tvSrFdNTX6jK8EFF3iGsdH5c7/n8eij8Itf
tP1zL8bkybm2SpHu3203D7WqhDfe8BvzJZbw46+7zv/sz8sX7oswfjzccENu/StfKZ3EIMurr9bm
DDU1wccfw7bbtt4+aVJu/6KmqQmmTm1t02qrwSWXwIUXwk035ba/+CKsumrOztNOK359vfBC2/N8
7bXWmfHy7fjLX+DZZ1tvf+YZ2GyzXJlZs2CbbVrX8d57uf2F6r37bvjXv2DTTdvuv+suOOccDxO7
/HJYdtm2ZaZNy4WilqKpCe6918/729+Ggw/27WZw0EEevrf22t4PqVq7ww5w7LEepnnooblw1bff
hu22a103wPe+1/qmMk0qkioBn3wCO+4I558Pq2ei1f/0Jzj33OL9VOqcHn/cz0mCE0/Mfa4nneT7
imULTdePPLJ1v6YPMyq1Iy3329/CjTf6+x494JRTPGTv4INzacAbrS4DvPIKHH20f2aFVJ0xYwr/
7kyZ4p97atvrr8NGG1VnVyE7zfw70DMzBPz993P7U7sOPxz237/wBOzHHQdPP+2ZHydNaqsevfSS
X0fZdsH7IRvumc61VU7BOussuPba1vvef7+1vaNGtX34Uow01DoUrMUbM3sbz24XBN2CULAaxLnn
+g1qpU5BtYwd6zeo8+fnluZm2HVXWGstv0n50Y/8xuGPfyxf3y23wMMPt66v1DJokN9orlhs+r2E
Lbf0m8levSqve9gw+OEP/abv0EPd2frDHyqfXPPvf/fX/fZz5woqb3u11bzNatl5Z9hqq7b1Lb88
7LOPvy5qdt4ZRo7M2fLWW3DZZX6jfskl7lSl+774Rb9hW3ZZ77cVVmh7LiNHwhFHwJe/3HbfqqvC
YYcVtuOQQ9whyD9mvfXgO9/xMjvu6DeWCxa0LrPccvCtbxV2gvbZx1/vuKNwu9ddBw8+6PufKTLF
ZEtLZTdyBx7o36vnn3dnLeWjj3z9oYf8Gr3nHr/pHj/eHR+Af/7Tnb1p0/ycNtkE9tgjV8cGG8BO
O/kNaPbce/eGr33NndCdd3Yn8p57/Hua5bLL/IZ21ChYaaXy55IyapS3PX8+/OMfcPPNuX3nnecP
G/r2dUdx441bH7vWWv7QpLm5tc2Sf+fyHfBi9O/vjspKK+XqePhhuPVWd5z/+lf/jObP97I77ggj
RlR+jlBawbrvPv+9GDnSzzNl5Eh/veuuwsc99ZTb9sknbtvGG8Oe5fJ0lWG77bzfzFr36TLLwDe+
4d+hfv1yKfNfeaVwPeed59fjn/7k1/7HH7eub4MNYO+9c+XXXBN2390fBGbLgX+WXyk4WYr36wEH
+H9A/nd7iy38v2iTTWD77f1hWaW/wUOG+G9wn3Iz8ARBEHQlzGyxXTbccENrFKecYgb+2h5suKHZ
TjuVL/ezn5n17Vu+3IEHmq28cv12tQe/+5335dSplZUfPdrLz5/frmZ1OS67zPtlwgSz1VYz22ef
jraofpZd1uzwwwvv23VXsx49/JxvvLFwmQEDzI4+uvL2vvlNsy9+Mbf++utef9oOmH38sdu07LJe
5pJLfPtbb1XeTiE+/tjrOeec1ts//3mzvfeur+5VVzXbd19/v2BB+/52lWPIELPvfc/syivdjtdf
r6++HXYw22ijwvt++UtvY+7ctvsOO8xs0KDCx119tR/36qv12VYLDz7obT/4YNt9c+b4vp49c9fj
v/+96G3sTADjrBPcX8QSSyyL9xIKVheh0kHVpearydKIMQTtRbVx/Gn67551TXXY/cj2Y6XKTWen
lDrR0lI6E2U6gWs1/ZDfXv6UAuDXXpoC3axxY0qKTQ7c6Ax2aUhZR/0epLY0avxiuWuk2ATBpY7r
yOQ15TJzQuvrsbP+rgdBECxOhIPVRaj0pqrWeWA6E9VmzurM59KRZBMKZFOkd2VKja/Jjq8qVKYW
RyK/vfwb2v793bFPQ+dmz249oXI99OzpdeSfS6Mz2HV0FrfUlkbZUe4aKTXGaO7cwhlQOzJ5Tanf
w0IOf/wWBkEQdDx1OViSQjNISJNFVJo0olqqUbCg+ixanYlaFKzOei4dSdon77/vyQi6w41XOXWi
VKr/Wm6Sm5vdMVuwoHUd6Q1tqppmndnshMr1kn++5TK91VJvR2dxyypY6RxkjaivEKUeNJRTivr0
qTxxTyOpVsFKlc8gCIKg46j3FuA1Sb+R9MWGWNOFSZ+OVzP/R6XMnetPxkPBKkxnPpeOJO2Tt9/2
1+7ghJZTJ5Zeuniq/1rCvNI+TLPrFbqhzdaZKjGNuh6LKWjdVcFqxDVaak6rUqGy5ZSijuwfqFzB
aoRjHwRBENRHvT/F6wGvApdKeiKZJG6xvNUtNWdQvVRzAxQKVpCS9knqYHUHJ7SYOpGOfUrn4mmk
gpU9ttANbbbOVIlp1PVYbAxYd1WwGnGNlprTqpTzW04p6qj+Sefpq1TBCoIgCDqeuhwsM5tuZpeY
2WbAccBoYLKksZI+1xALuwj5T4MbSTU3hqFgBSn5ClZ36KNiCtacObkwyDThRD71KFj53+9iDlZX
U7DMOpeC1Qgbyik+xT7/zqpgQfHrvpjDHwRBEHQsdY/BkrSrpFuB3wO/BVYD7gTuboB9XYZFoWBV
cmNYifozb55PgtpZVZ9QsBpDmoChO4UIpmqHWevtWRWmvRWsPn3aznOWn7GxKyhY8+Z56HFnULBm
zvRJshthQzklqqspWFBcuQ0FKwiCoHNS9xgsYDfgN2b2ZTM7x8ymmNlNwD21VCjpaEnjJb0k6S+S
+kpaRtJ9kl5LXpdOym4u6UVJ4yStkWxbStK9khZpJHpXUrDScWKdVdHo188dg1Cw6kPyfuluClaa
rS9LVoVpbwUrbaNYua6iYKX1dQYFC+Cdd0LBKkY5BStN7hIEQRB0Dup1Qr5rZt83s8fTDZI2BzCz
H1dbmaQhwI+BEWa2NtATGAUcDzxgZmsADyTrAMcAOwFHAYcl204CTjezdsrnV5iupGB19BPrckil
M4FlmT/fn3531nPpaJqb4d13c++7OsWu70WpYKVtFCvXVRSstL6Wlsakla/XlnffbV8FKw2H7G4K
1hJLwLLLLnqbgsULSbtLelTS+5JmSZoo6TZJO9RY3/eSh+ZzJX1SxXFLSTpV0ga1tFuiXsssCyV9
KOl2SV+qsb7hiZ2rFdg3QdKVdRsddGrqdbDOK7DtD3XW2QvoJ6kX0B+YhKtkY5P9Y4Hdk/fzkjL9
gXmSVgeGmtnDddpQlt12g+uug4kTYZ114PXXffvrr8Nqq/lyxRXl6znuOPjCF+DZZwvvnz8f9trL
31dyY7jEEh7C9Otfw4EHtt43ZQqsvz5suWXl9XUUTU0wdmyuL9Nl883h//4vt/65z+XKB23J9kt3
6KP0HDbaqPV1seuuuf1ZBeu99/yaX201OOMMz7C25JLVt3fEEV7Hbbf5tjQVdnpjm5YbM8a/Z41U
sGbOzJ3nUUe1bq+eegG22QbOO8/XpfrqrNeW/Pf11vf97/tnnjJjhjtZ5RSs0aNz/X3YYXD55fDm
mx2vYD31lNt0+OG+7c034ayz/HqOzIFBeyLpx8CteNTS94GvA79Kdm9bQ30rARcDjyfHf7WKw5fC
x/s31MFKuBLYFBgJnAxsBtwjaaka6hqO29nGwQL2AH5Zm4lBV6FXLQdJ2hS/8AZJ+klmVxOuOtWE
mb0r6WzgLWAWcK+Z3StpBTObnBR7D1gheX8GcFVSdj/gbFzBKmX7IcAhAMOGDavRTrjjDl9uvx1e
egl22QW22w7GjfP9t98O993X1snJ59JLfezBk0/CBgV+Lj76yNO0Dxnif66VcOaZcMklfjOYdfLG
j4cXXoDtt4edd4att674lBc5p5wCDz3Uett//gOPJ1rpyJGwyir+vndvd3iDtvz853D33TB8ePd4
yr3ddnDIIZ6GO5+mJne8br89p2Cl1/wOO8CgQbDuutU5Eiut5H34zju5bTvv7Dfp55zj33vwa/D0
0+GVV/xmd7/9aj/HLHvv7Q9x5s/PbRs2rO0YsGrZckt3HmbO9PWNN66vvnrYems49FAP+zzggPrr
W311+OlP4Zpr4G9/888PyodC9umT+wzBf5PvuCPnrJf7LW9PjjjC7X7iCbfpj3+E55/3faNG+evl
l/v1HQTtwE+B28zs+5ltDwKX1DgcYw38XnGsmT3WCAMbxLtm9kTy/jFJ04BrgB2A6xrViJk916i6
gk6MmVW9AFuRZAxMXtPlJ8AatdSZ1Ls0/qUdBPQGbgP2BT7JK/dxgWNHAr8DPg9cj38pVijV3oYb
bmi1MG2ambtRZldd5a+vvtq6zIYbmu20U/m6ll3Wjz/jjML7//tf33/NNdXZeNJJZj16mC1cmNt2
yy1e13PPVVdXZ+Hyy3P9ftddHW1N0Fk59VS/RubPN7v5Zn///PMdbVWwqNltN7N1182tjx/v18J1
11V2/DHHmPXvb7bjjmYjRrSPjdVy9NFmAwb4+yuu8PN5440ONanTAYyzGu9BYil6bzYDuLCCcoOA
i/Dpez4F3gb+DAzJlLkSsLzlysz+Q4AXgNnAh8BlwDLJvuEFjjXgADx6agrQO8+mgcB04Mwythvw
q7xtayXbj83b/iPgX8BU4BPgCeDrmf1bF7Fz62T/hLxzPiDZvwlwLTANj946D+ib1/ZqeBK5T4H3
8eRyhyTHD+/oayWW3FKTgmVmjwCPSLrSzCbWUkcRvgq8aWYfAEi6BVfKpkgabGaTJQ3GL6rPkCRc
uRqFf8mOxb+IPwZObKB9QGWDzktNiJplzpy2dRZqq9r4/6YmWLjQn1CnYys6ejB7vXS3cLegfUiv
jenTu/41H9ROvQlCmpo82+pHH3We8YtNTR7quGBB7f8NQVADTwH7S3oDuN3MXi1SbhlgLn4/NgUY
jI+V/6ekL5jZbDw07hncefgh8CyQ3vOdmZQ/D/gZMAQPRVxb0mb4Q/1vALfgEUx3JO2+ntj4Izz8
7oaMTd8BlsQdv2oZnqk/y6q4o/g6rsTtAvxV0o5mdk9yTj8ELsDvQ59Ojnu5THtXA3/Bz3FT4FTg
Y1zAQFIf4D5gCeAHeL8dBOyZX5GkU5PjVjWzCWXPNGg4tYYI/t7MjgLOl2T5+81s1xrteQvYRFJ/
POzvK8A4YCawP3Bm8np73nHfBe42s6nJsQuTpX+NdpQkO9i42J9cc3NuXFYxFizwP8v8Ogu1Ve0N
YnbAdupgdfbkFuXI2t1VzyFof/ITOGS3BYsP9SYIScu9846P++wMpDZNn547n3Q8YBC0I4cBNwG/
Bn4t6SP8Rv8KM7s3LWRm/wWOSNcl9QT+id/b7QjcamavS0oCcXnZkpA8ScNxp2qMmf0iU8erwGPA
LmZ2m6Q0vO4Ny4XzAXwg6RHgUFo7WIfiw03erOA8lYz/7wWsk5zvE+QcufQ8j8kc0ANPvvZ53Om5
x8ymSUqdqVfy7CzFn81sdPL+fkn/B3ybxMHCla7VgP8zs6eS9v8GPA/kj3lZCCzAla2gA6jJwcK9
bPAxTw3DzJ6UdBPu/c8HnsMHQg4AbpD0fWAi8K30mMShOgD4WrLpHFw+nYs/uWg42aeiLS0+/mKJ
JVqXqUTBSp2r/DoLtVWLgpUeP2RI67q66tP8ULCCSiiUgjxuQhc/spMpS7UpWACTJnUeBz3/2u7f
3/9/gqA9MbNXJX0Z2By/19oEV4pGSTrZzNKEF0j6Ae6QrY4rRylrlmlmOzzx2rWJk5PyJB7iNxIf
NlKKPwLXSVrDzF6TtBHwZVwRqoQTkiVlArCtmc3LFpK0ITAG2AgPi0xH9v63wnaKcVfe+r9pnQBk
E+Ct1LkCMDOTdDPQagRm4qT+gqDDqDVE8Jnk9ZHGmgOJ9z46b/McXM0qVP5TYJvM+j/wJw/tRr6C
1dzcduB8JWnGCz1dLVamHgUrW1e/fl33DzkUrKAS8hWsrnzNB7XT3OxRAp9+6pn2alWwoPM80Mm/
tjuLXUH3x8wWAI8mS5oJ8B5gtKQLzOxjSUfg4X3n4GrUx7jT9ATQt0wTaeqe/xXZX0maplvxRGiH
4ok5DsPHMt1ZwbEAlwMX4rZ+BTgFd9i+auYDoCQNxRWrl3G17i1cEPglPmarHqbmrc/BwwFT2gyR
SZhSZ7tBO1BriOC/KSE7mlm3zmWUr2AV+pPLf3paiEKhhsXaqkfBytbVlf+Qs7aHIhEUI/8pfzjj
iyfZ62DJJWtXsKDzXENxbQedBTObJOlS4Fw8K+BT+Dj4B/JC6FatsMqPktev4Y5Zsf2lbJqX2HS4
pF8n9vzWzOaXOTRlspmNS94/lozvH42Pcbox2b4D0Ax8y8w+yy+bRFO1N5OBLxbYvkKBbUEHU2uI
4M4NtaKLka8KFfqTa25um2Qin/QPf4klyitY1ToUxRSsrvyHnLW9Z82TAQTdnXjKH0Dr62Dw4Op/
S0PBCgInTTJWYNcXktf3ktf+eAa8LJVOcHAfPm5omJndV6JckhqMfkX2X4SH+d2Iqz+XVNh+Ic4C
DgZOkXRTomKljtRnYYOSPo+HT2Ym9ChrZy08ARwoaePMGCwB32xgG0GDqDVEsJGZA7scWafl/feL
K1jgTlQxByutZ+jQ0gpWLSFO3VHB6tfIn6mg2xJP+QNo+xs4bZo7V5VOytsZx3zmX9udxa6g2/OS
pPvx8e1v4nOe7oSH4N1gZm8l5e4BjpN0Aq5obUuBDHeFSJJfnIUnT1sTeARP1T4UH591qZk9hIfD
fYSP/3oRT4L2ppl9lNTzrqQ78DFid5rZ27WetJnNknQ6cD4+jutm4H48JPAqSb/Fw/bG4KGC2V+X
V5Ny35M0FXe4/mtm02u1B89ceBxwi6QTyWURXDrZvzAtKOkUPMRx9cX9nr2jqGn+d0mPJa/TJU3L
f22siZ2PrNPy9tvFFSwoPQ4rrWfo0NIKVi03iN1Rwapmgthg8SWe8gfQ9jew2t+/zjjmM//a7ix2
Bd2eE3El5hfAvfhco5sCxwPZadV/gStIR+PjodYFtq+0ETM7AZ/TaSSeCfB23KH4GHgtKbOQnFNx
P54CfZe8qtJwvlpSs+dzCZ5c7SRJMrPxwD7AKnh2wWPxfng071w+wtPGr4c7i08DG9ZjiJnNxUMo
XwT+BIzF5xq7ICmSvZPsgaeQjzunDqJWBWuL5HWxHAmTdVree6+8glWunqFD4bHHCo/XqvUpZf7c
V+n7FSJSN+jm9O/vIaTpU/7BgzvaoqAjKKRgVfNbGgpWEDhm9if8hr5cuVl4qvIf5O1SXrn787dl
9l1NLlN1sXZuo3RGwZ1xp+hvZUzO1lnMnrnk5sNKt91A61TwANcVOPYiCjh5ZpZf35W4OpVf7lR8
Lqzsttdx9fAzJP0VT1vfUurYYNFS6xisz5C0AbAFnvTiMTN7rswhXZ58p2nppduWSbdtsgmcdhqc
kCT+3HNPuPlmf5+OIxo6FObNg9mz24bB1fqUsmdPD4dpaYE33oD11/e5UzbeuPq6gqArIcFSS/n3
DjrPHEbBoiX9Dd4zE6C0+eaVH5/9LS70G98RLLmkh4sfe6yvdxa7gqAzIGkTYH1gb+AnidrVrZD0
E2AGrugNBPYCvk5bpzboYOpysJIYz73wWbUBrpR0Y3ZOhO5ISwusuiocdpinAN5337ZlNtoIzj4b
fvMbeOaZ3PZx49zZ+egjDy/s0QNWWsn3peOtstTzlDLNZPjqq+5cHXQQ/OQntdXVWXj88RiLFZTn
0kvhuefc2dp77462JugIVl4ZLrwQJmeG5m+3XeXHS3D99f47PWJE4+2rBQmuvhpeftn/Owr99wTB
Ysy/cOdjLD4nVndkDh6COQwPAfwvcJCZXdahVgVtUJLav7aDpf8C65nZ7GS9H/C8mZWbUK5TMGLE
CBs3blz5gnlst51nB3z88fJlN9/cHYL77/f1pZeG/fZzp+fvf/cn7eef73+U//0vfP7zrY9fe21Y
c82c6lUNX/oSrLUW7LUXjBoF48fDFwsl+AyCIAiCboCkZ8ysk7jEQRAsrtSU5CLDJFpPHrcE8G6d
dXZ6qhk4n6pI4GOsUkUqPb6pKRcCWGi8ViMUrGrnfwmCIAiCIAiCoDZqnWj4D/iYqxZgvKT7kvXt
8NSc3Zpp0zxEsBKam30MFLjqtXChb0udqubmnONTKJNgPZmimpvh449z9UbGqSAIgiAIgiBoX2od
g5XG1T2Dp+JMebgua7oItSpYWSWpEgVr4UIfO1WPgjVxotcr+QDpIAiCIAiCIAjaj1rTtI9ttCFd
iWomL21ubj0PS7otPX7JJYsrWDNmeFhhPQpWdi6gSifYDIIgCIIg6M5ojO4HvpLZtBAYbqNrn5w4
CFLquuWWtIakmyS9LOmNdGmUcZ2RefM8c2A1CtasWX5cIQVLKq5g1Tt2KjsGK8ZfBUEQBEEQgMZo
KLBN3uYeQOTmDBpCvZrGFcCFwHz8Qr0KuKZeozoz06f7azUKFriTU0jBAp+vCtoqWPWOnWpu9nFf
U6fG+KsgCIIgCIKE/Sh8D7z/ojYk6J7U62D1M7MH8HTvE5OZo79eT4WSlkpUsf9IekXSppKWkXSf
pNeS16WTsptLelHSOElrZI6/V1K7BMSlTk81Cha0zeaXPb53b+jfv30ULIB33gkFKwiCIAiCIKGY
I7Wmxiimpw/qpl4nZE7iyLwm6UeS9gAG1FnnucA9ZvYFYD3gFeB44AEzWwN4IFkHOAbYCTgKOCzZ
dhJwenvN4J06PdUqWOlYqHRb376tyzU1tY+CBT5RZihYQRAEQRAs7miMNgGys47+La9IqFhB3dTr
YB0J9Ad+DGyIS641X5iSmoGRwGUAZjbXzD4BdsNn5iZ53T15Py9pvz8wT9LqwFAze7hWGwqxYAHM
n+/va1Ww3n/fM/ql26TW5ZqbWytYkybl0rvXq2B98EEoWEEQBEEQBLS9Tz0WGJ9ZH6Ux6rMI7Qm6
IbWmaQfAzJ5O3s4ADqzfHFYFPgCukLQengb+SGAFM5uclHkPWCF5fwY+7msW7tydjStYDWXzzeHJ
J3MTBUP1Ctb22/trnz4+5mqF5AzWXddfswrWDTfA3nvn6lh22drsHjQo93655WqrIwiCIAiCoDug
MVoCyNxh8YKNtpc0RlcDZybblgZ2AW5e1PYF3YdaJxr+vZkdJelOfILhVpjZrnXYswFwhJk9Kelc
cuGAad0myZL3zwObJDaNBCb7W12Pq1vHmNmUPNsPAQ4BGDZsWEVGPflk7n2tChbA1lvDmDHQsyds
uCE88ghsuqnvy6ZzT5Wriy+GlVeGFVesrK18Nt/cnbWZM2GHHWqrIwiCIAiCoJuwK+5ApaSJ2f6M
P7RP44v2JxysoA5qVbCuTl7PbpQhCe8A75hZ6tLchDtYUyQNNrPJkgYD72cPkiRcuRoF/AGXe4fj
oYsnZsua2cXAxQAjRoxo4xyWo1YFy9uDkSNz69n3TU0weXKujd694aCD2oYSVkPPnrDXXrUfHwRB
EARB0I3IhgcuxB0rbLS9rTF6BNg62bejxmh5G23vEwQ1UOtEw88kr4800hgze0/S25LWNLP/4hPA
vZws++Py7f7A7XmHfhe428ymSuqPf2kW4mOzGsbChfUpWKWOyZ+QuNA4rSAIgiAIgqB6NEYrANtn
Nj1ko21SZv1qcg5WL+A7wO8XjXVBd6PWEMF/UyA0EJdWzczWrcOmI4BrJfUB3sDHdvUAbpD0fWAi
8K2MLf2BA4CvJZvOAe4G5uJfjoYxfbqrS716Qb9+lR2TzRZYSvVKJwUGf42sf0EQBEEQBA1jH1rf
9+bP23oTcAGQ3rntTzhYQY3UGiK4c0OtyJCMqxpRYNdXipT/lMxs3Gb2D2Cd9rAtnSy4GnUpW66c
gjV9ek4li6x/QRAEQRAEDSMbHjiLvDFWNtqmaYzuIPcQf32N0To22v69qAwMug81pWlPJhWeaGZJ
4nHWSN6/D0xtmHWdgIWZ2bTSuazqnZeqEE1NnqVwxoxQsIIgCIIgCBqFxmh9IBtddaeNtukFiuar
WjEnVlATdc2DJelgXFK9KNm0MnBbvUZ1JqZnvn7TpvlS77xUhcifkDgUrCAIgiAIgoaQ7yjlO1Ip
9wAfZtb30Rj1bB+Tgu5MvRMN/xDYHJgGYGavAcvXa1RnIjv5b3srWGl7oWAFQRAEQRDUj8YoTViR
8iHuSLXBRts84PrMphVpnRgjCCqiXgdrjpnNTVck9aJw8osuS5rZD0LBCoIgCIIg6GLsROuH/zck
jlQxIkwwqJt6HaxHJJ0A9JO0HXAjcGf9ZnW4VEEIAAAcMUlEQVQeFrWC1dISClYQBEEQBEGDqDQ8
EAAbbU8A/8ts2lVjFHdlQVXU62AdD3wA/Bs4FE+PflK9RnUmsgrW88/D1Kntq2CNHw/z5oWCFQRB
EARBUA8ao2Vonfn6dRtt/6rg0KwT1hfYu6GGBd2euhwsM1toZpeY2V5mtqeZXQJs1iDbOgVZBevC
C93hGjy4ujp22cVfs3Ni5ZM6VD/7mb9W20YQBEEQBEHQim8DfTLr11Z4XIQJBnVR60TDPfF5AoYA
95jZS5J2Bk4A+gFfbpyJHUuqYD38sKdR79EDNt64ujpuuMGVr1JzZ2VDAocNg332qdrUIAiCIAiC
IEe+Y3SKxuiUGurZTGO0ho221xphVND9qXWi4cuAocBTwHmSJuGTAx9vZt0qTXuqYG2wAQwcWFsd
ffvCSiuVLrPkku6AmcE220CvWj+ZIAiCIAiCxRyN0VrARg2s8rvAyQ2sL+jG1HobPwJY18wWSuoL
vAesbmYfNc60zkFLizs+Awa0bzs9eniYYGQQDIIgCIIgqJtGh/XtqzE6xUZbt8qWHbQPtTpYc81s
IYCZzZb0Rnd0riCXlr1UeF+jGDiwviyFQRAEQRAEizsaox7AvplNM/HhK6XSsxfiVHKO2nBgK+Dh
+qwLFgdqdbC+IOnF5L2A1ZN1AWZm6zbEuk7AonR40rDAULCCIAiCIAhq5qt4noCUu2oZP6UxuorW
Stj+hIMVVECtDtZaDbWiE1PPxMK1EgpWEARBEARBzeSHB15fYz2PAFOAFZL1PTVGP7LRNrNmy4LF
gprStJvZxFJLo43sSDoiZC8UrCAIgiAIgurRGDUBe2Q2zcDnaa0aG20LgJszmwYA36jdumBxod6J
hrs9oWAFQRAEQRB0GfbCpwxKudNG2+w66rshb/27ddQVLCZ0SgdLUk9Jz0n6a7K+jKT7JL2WvC6d
bN9c0ouSxklaI9m2lKR7JTXk3ELBCoIgCIIg6DI0Kjww5R/A5Mz6thqjleusM+jm1OSESHogeT2r
seZ8xpHAK5n144EHzGwN4IFkHeAYYCfgKOCwZNtJwOlplsN6CQUrCIIgCIKg86MxWg3YIrNpGnBP
PXXaaFsI3JTZ1APYr546g+5PrSrPYEmbAbtK+rKkDbJLPQZJWhn4OnBpZvNuwNjk/Vhg9+T9PKB/
ssyTtDow1MwerrS92bPhn//Mrc+aBX/+M1xxBdx5Z8coWP36lS8TBEEQBEEQtOK7eEbrlDtstM1p
QL35Klij59gKuhm1ZhE8BZ/NemXgnLx9Bmxbh02/B44FBma2rWBmqTz7HrlsLmcAVwGz8KcJZ+MK
VlEkHQIcAjBs2DAOP9ydqQkTYJVV4OabYb+85xKLSsE65BA44QQYNGjRtBcEQRAEQdBdsNF2Kj53
VaPr/SetHbcgKEmtWQRvMrMdgV+b2TZ5S83OlaSdgffN7JkSbRvuxGFmz5vZJma2DbAaHiMrSddL
ukbSCgWOv9jMRpjZiEGDBvH00779k0/89YMP/PXcc3PHLCoF6/jjYe7cGIMVBEEQBEEQBF2VWhUs
AMzsl5J2BUYmmx42s7/WUeXmeNjhTkBfoEnSNcAUSYPNbLKkwcD72YMkCVeuRgF/wBWw4cCPgRMr
aXhOIiBPm+av62amSl5UDo8EvXsvmraCIAiCIAiCIGg8dWXak3QGnpDi5WQ5UtLptdZnZj83s5XN
bDjuLD1oZvsCd5CLd90fuD3v0O8Cd5vZVHw81sJk6V9p2y0tudcBA2CZZXL7IulEEARBEARBEASV
UJeChSejWD/N2CdpLPAccEK9huVxJnCDpO8DE4FvpTsk9QcOAL6WbDoHn1BuLvCdShtIlas0a2BW
tYqQvSAIgiAIgiAIKqFeBwtgKWBq8r5hWk+SCfDh5P1HwFeKlPsU2Caz/g9gnWrbyypYzc2tVatQ
sIIgCIIgCIIgqIR6HawzgOckPYRnVxlJbo6qLkW+gjUwk8MwFKwgCIIgCIIgCCqhrjFYZvYXYBPg
FuBmYFMzq3fG7EWKmb/mK1i9Mq5nKFhBEARBEHRlJB0gyTLLXEmvSzpdUt8a6zxVkuVtM0mn1lDX
lZLeqaBceh7DM9smSLqyTJlTJdUzjVAhWybk9eknku6TtEX5owvWt1RiZ5s5ZSU9LOnhuo0OFgl1
hwgm81Pd0QBbOoSZM/01q2ANHdq6TChYQRAEQRB0E/YC3sHnG90D+Hny/ogG1b9pUn97cVfSxuQq
y4wGTgMebLA9f8fn3uoBrJG0c7ekdc1sQpV1LZUc/w7wbN6+w+szM1iUNGIMVpcmq1ylr/mKVd+a
nusEQRAEQRB0Op43s/8l7++TtAbwPUlHpknL6sHMnqi3jjL1fwB8UG+ZBvJh5pwfl/Q/4DE8G/aZ
jWrEzF5uVF1B+1NXiGB3IFWuUgcrHYMVBEEQBEGwGPAsPq3NctmNklaVdK2kDyTNkfS8pD3KVZYf
Iijpc5KulvSmpFmS3pB0oaSlixy/maSnJc1OQvCOyNvfJvyvQB2tymTCGE/MhPOdKumY5NwG5R2v
xM7ryp1vAVLlaVhenaMkPZj05wxJz0naP7N/OPBmsnpJxs4Dkv2tQgQlbZ3s31XS+ZI+TJZrJC2V
1/YgSX+RNE3Sx5KuSI4zSVvXcI5BGep2sCRtIenA5P0gSavWb9ai4d13YcECfz9tGvzrXzBjRoy5
CoIgCIJgsWE40AJ8lG6QNBR4ElgPOBrYFXccbpa0a5X1rwRMAo4BdgB+gWeGvrtA2SbgemAssDue
Tfq81Mmog02T1yuT95sClwJX4POmHphX/mvAqsCfamhrePL6et721YHbgP3wc7sTuFTSYcn+ycA3
kvdnZOy8q0x75wKGT000Bvhmsi3LLcCOeDjoKGAe8If8ijKO6dZl2gzKUFeIoKTRwAhgTfwi7Q1c
A2xev2ntz3vvQe/eMG+eK1gnn+zb11vPX/fdF159tePsC4IgCIIgaDA9JfUiNwbrm8BRZrYgU+ZU
PDv0VslUOQB/TxyvX1DF2HszexR4NF2X9E/gf8A/JH3ZzJ7LFB8IHGJmqXJ0j6QhwBhJY82sVUKN
Kmx4QhLAu/khjJKuBw6R9JtM/YcC/0mmDCqHkv7sAXwOuBB4Dbg8z4bTMgf0wJ3HwcAPgD+Z2RxJ
aV+8UUWo5aNmlqp890paEzhI0gFmZpK+BmwB7G1mNyTl/i7pDvJUNtzZXIA7bEEd1Ktg7YE/1ZgJ
YGaT8C9Hl2DDDWHuXNhzT1ewWlpgxx1h9919/9VXw5NPdqyNQRAEQRAEDeQ/uIIxFbgMuMjMzs8r
swOuMLVI6pUueEKH9SRVPJhCUh9JJ0j6j6RZSdv/SHavmVd8AZ6VOst1uCMwpNI2q+SPuLr0lcTe
wcAuwMUVHv8d/JzmAOOBtYFdzOzjbCFJayRheu8m5ecBB9G2D6olX+H6N7AEsEKyvgner7fmlbsp
vyIzu8rMepnZI3XatNhTr4M1N/H2DUDSkvWbtOhpanLnKsZfBUEQBEHQzdkD2AjYCbgfOFzSd/PK
LA98l5wjkC6/SfYvW0V7Z+CK2DXA14GNyYXC5acR+9jM5uVtm5K8touDZWZPAc8AaajeQcB8PEyx
Ev6G9+dmwFFAP+AWZVLfSxoA3IeHXB4PbJkccznuDNXD1Lz1Oclr2v5gSvdr0A7Um0XwBkkXAUtJ
Ohj4Hh7T2qVobnbnasGCGH8VBEEQBEG35qU0i6CkB4EXgd9IutnMkslr+AhXmc4qUsekKtobBVxl
Zr9KNyQORyGWltQ7zxlIlZh3q2izWv4IXJSEIx4E3Ghm+Y5LMaaa2bjk/b8kteDDZo4g55BuCqwC
bGlmj6UHJqpgezOZ0v0atAP1TjR8Ni4x3oxLnKeY2XmNMGxR0tTkyS0+/jgUrCAIgiAIFg/MbA7w
M1yxys6zdA+wLjDezMYVWOYUqq8I/XH1K0t+UomUnviYsCyjgLeo38Gai6tLhfgLMB34Mx6OWEty
i5SxeEKQn0nqn2xLXz/rhySL4m55x6b9WszOWngC79f8DJB7NbCNII96k1ycZWbH4bJn/rYuQ6pa
zZ0bClYQBEEQBIsPZnaHpKeBYySdb2azgFOAp4BHJZ0PTACWxscXrWZm36uiiXuA/SX9G09u8Q08
nK4Q04FfS1oOTxTxbeCrwAG1JrjI8DLwdUn3AB8Dk5LcAZjZLElX4hkT/21mj9faSJJY4hTgr3gC
i98CjwPTgAuSBHFLAicBHwLZO88puHo4StKLeI6DNzOJRmqx594kscjFSb/+D9gTD1cET2wBQBIq
ejnwlRiHVR/1jsHarsC2Heusc5GTVa1CwQqCIAiCYDHjJDxk7DAAM3sLzxL9AnA6/iD9QmAr4MEq
6z4Czzp4Gp6CfSDuOBViGq5Y7Q/cDmwDHGlmlY6HKsWPcIflTuBp4JC8/TcmrxfV25CZ3QX8C/ip
pH7JxMd74ErSTfi4tEvxcWnZ4xbiIYpL4+PjnsYTbtTLHrijexZwAz4+K8mdTUumXI/ERjWgzcUa
1fJAQNIPcCl5NVrn+R8I/NPM9m2Mee3LiBEjbNy4cdx0E+yVCKVXXgn771/ysCAIgiAIOiGSnjGz
ER1tR9D1kHQacCSwkplN62h72ptEmTwQWKbKkM+gAmoNEfwznjXlDDwbSsr0KgYFdhpCwQqCIAiC
IFj8kPRlPI/AkcDF3dG5SiZqbsbTyPfB0/D/APhNOFftQ00hgmbWYmYTzOzbZjYRmIWnah8gKX/S
soqRNFTSQ5JeljRe0pHJ9mUk3SfpteR16WT75pJelDRO0hrJtqUk3ZtM4lYR2XFXMQYrCIIgCIJg
seFWPOvf/cDoDralvZiJq1W3ArcB2wMnJEvQDtSb5GIX4BxgJeB9PAXlK8CXaqxyPnCMmT0raSDw
jKT7gAOAB8zsTEnH46rZccAx+DwOw/G44WPwOOLTkzjWiggFKwiCIAiCYPHDzIZ3tA3tjZndSG6M
WbAIqDfJxa/wGaJfNbNV8Vmwn6i1MjObbGbPJu+n487aEDyNZTrAcSywe/J+Hp76sj8wT9LqwFAz
e7iadkPBCoIgCIIgCIKgEdQ7wdk8M/tIUg9JPczsIUm/b4RhkoYDXwaeBFYws8nJrvfITY52BnAV
HqK4H3A2rmCVqvcQkswxw4Z5NOPyy8PIkZ6mfejQRlgfBEEQBEEQBMHiSL0O1ifJbNyPAtdKeh+P
86yLpM6bgaPMbJqUyxaZzC9gyfvncQUNSSPx2aol6Xpc3TrGzKZk6zazi4GLwbMIAvTqBY9Etv8g
CIIgCILFAo3R/XjkVcpCYLiNtrc7yKSgG1FviOBuwKf4xGz34Cnb68rXL6k37lxda2a3JJunSBqc
7B+Mj/fKHiNcufolPkDxWOAS4Mf12BIEQRAEQRB0LzRGQ/E5trL0ALrENENB56cuB8vMZprZQrP/
b+/eg+0qzzqOf39NuFNoK4gMAaHcKhUIbQoIFZV7agdRWwpyibUM/lE64LTDgKM9nAHH6ihTtfVS
QQuFAinYCiXCVMReplCacCkhQLkIQygQkdYALZeQxz/2Oj0rx4Qk5+ycnez1/czsOet99trvefYz
O5n9nPWutWpFcxO4z9C79OOkNI3SZcADVXVJ66kb6N10DsZvPtd2BrCguUT81vT+CrGy2ZYkSZLG
nM7qvwN7J1T1xaQarCTbJbkgyWeSHJues4HHgJOmkM/h9D70Rya5p3m8D/gUcEySh4Gjm/FYLlvT
u8rgZ5vQJcAC4NPA308hF0mSJA2fNTVS+2Y0h0xrJhpKkz0H6wvAD4HbgTPpXUc/wInNeVGTUlXf
auZZnaNWF6yqH9M6zFtV3wT2n2wOkiRJGk4ZzaHAPq3QvwFzW+N59C6wJk3aZJcIvr2qfreq/gE4
BdgPOG4qzZUkSZK0gU08enUecH9rfHJGs/k05qMhNNkG67Wxjap6HVhaVS/3JyVJkiSpvzKaLYAP
tUL31kgtprcya8xbmeIF26TJNlgHJlnePF4ADhjbTrK8nwlKkiRJfXACvQZqzJXNzy8C1Yp7sQtN
yaQarKqaUVXbNY83V9XM1vZ2/U5SkiRJmqJ247SSXmNFc++r9h1R52Y0PzudiWm4TPU+WJIkSdJG
LaPZCTiuFbqtRuoHrXF7meBM4HemJTENJRssSZIkDbtTWfXq2VdOeP46oH09AZcJatJssCRJkjTs
2g3TT4Dr20/WSC0HbmiFZmc03vZHk2KDJUmSpKGV0cwGDmiFbqyRemE1u048quVRLE2KDZYkSZKG
2cRGaWIjNeZm4LnW+NSMZsaGSUnDzAZLkiRJQymjmXjBiufoNVL/T43Ua8C1rdDPseqFMaR1YoMl
SZKkYfU+oH3J9flNI7UmLhPUlNlgSZIkaVit6/JAAGqk7gAeaYVOyGi273tWGmo2WJIkSRo6Gc3b
gPe3Qo/WSN2+Di9tN2FbAh/qa2IaejZYkiRJGkanAJu3xlet4+tcJqgpSVUNOoeBmTNnTi1cuHDQ
aUiSpD5Isqiq5gw6D20cMpo7gff0abp9aqQe7tNcGnKbzBGsJMcneSjJI0nOb2J/luR7Sa5o7Xda
knMHl6kkSZIGKaP5BfrXXAGc0ce5NOQ2iQYryQzgs8BcYD/glCQHAu+qqgOAV5Psn2Qr4MPNvpIk
Seqmfi/rOy2jSZ/n1JCaOegE1tHBwCNV9RhAkmuAE4DNkgTYGngN+ATwN1VvePlNSZIkDamM5k3A
aa3QS8BB9L4rro8LGW/Udgd+BfjPqWWnLthUGqxdgCdb46XAIcAC4G7gVuB/gUOq6qI3mijJWcBZ
ALvtttsGSVaSJEkDczS9745jbprM+VMZzRWseiRsHjZYWgebxBLBNamqP6+q2VX1ceAi4JNJzkwy
P8kfreE1n6uqOVU1Z8cdd5zehCVJkrShTVweeO0k5/k68Gxr/IGMZptJzqUO2VQarKeAXVvjWU0M
gCQHAQEeAj5YVScBeybZe1qzlCRJ0sBkNNsBv9kKvUhvxdN6q5F6Hbi+FdoW+K3JZ6eu2FQarO8C
eyfZI8nmwMnADa3nLwL+GNgMmNHEVtI7N0uSJEnd8EFgq9b4xhqpl6cw3/wJY68mqLXaJBqsqloB
nA3cAjwAzK+q+wGSnAgsrKofVNWPgHuS3AdsWVX3DixpSZIkTbd+LQ8c803g6db4yIxm1hTn1JDb
JBosgKpaUFX7VNWeVfUnrfhXqurC1vgTVbV/VZ06kEQlSZI07TKatwPvbYWWAzdPZc4aqZXAda3Q
m4DTpzKnht+mchXBDWLRokXPJXli0HlMsx2A5wadxEbCWoyzFquyHuOsxThrMW5jrcXPDzoBDdQZ
9M7JH3NDjdQrfZj3WuBjrfE84E/7MK+GVKpq0DloGiVZWFVzBp3HxsBajLMWq7Ie46zFOGsxzlpI
0pptMksEJUmSJGljZ4MlSZIkSX1ig9U9nxt0AhsRazHOWqzKeoyzFuOsxThrIUlr4DlYkiRJktQn
HsGSJEmSpD6xwZIkSZKkPrHBGiJJ/inJsiSLW7G3Jflakoebn29tPXdBkkeSPJTkuMFkvWEk2TXJ
bUmWJLk/yTlNvKv12DLJnUnubeox2sQ7WQ+AJDOS3J3kq824k7VI8niS+5Lck2RhE+tqLd6S5Lok
DyZ5IMkvdbgW+zafibHH8iTndrUekrQ+bLCGy+eB4yfEzgduraq9gVubMUn2A04G3tm85m+TzJi+
VDe4FcDHq2o/4FDgo8177mo9XgGOrKoDgdnA8UkOpbv1ADgHeKA17nItfq2qZrfua9TVWvwVcHNV
vQM4kN7no5O1qKqHms/EbODdwI+BL9PRekjS+rDBGiJV9Q3g+Qnh3wAub7YvB05sxa+pqleq6r+A
R4CDpyXRaVBVT1fVXc32C/S+KO1Cd+tRVfViM9yseRQdrUeSWcCvA5e2wp2sxRp0rhZJtgeOAC4D
qKpXq+pHdLAWq3EU8GhVPYH1kKS1ssEafjtV1dPN9jPATs32LsCTrf2WNrGhk2R34CDgO3S4Hs2S
uHuAZcDXqqrL9fg0cB6wshXrai0K+Pcki5Kc1cS6WIs9gP8G/rlZOnppkm3oZi0mOhm4utm2HpK0
FjZYHVK9a/J36rr8SbYFrgfOrarl7ee6Vo+qer1Z7jMLODjJL054vhP1SPJ+YFlVLVrTPl2pReO9
zediLr2ltEe0n+xQLWYC7wL+rqoOAl6iWf42pkO1+KkkmwMnAF+a+FwX6yFJ68IGa/g9m2RngObn
sib+FLBra79ZTWxoJNmMXnN1VVX9SxPubD3GNMuebqN3nkQX63E4cEKSx4FrgCOTXEk3a0FVPdX8
XEbvHJuD6WYtlgJLmyO7ANfRa7i6WIu2ucBdVfVsM+56PSRprWywht8NwLxmex7wr634yUm2SLIH
sDdw5wDy2yCShN65FA9U1SWtp7pajx2TvKXZ3go4BniQDtajqi6oqllVtTu9pU//UVWn0cFaJNkm
yZvHtoFjgcV0sBZV9QzwZJJ9m9BRwBI6WIsJTmF8eSBYD0laq5mDTkD9k+Rq4FeBHZIsBUaATwHz
k3wEeAI4CaCq7k8yn94XiBXAR6vq9YEkvmEcDpwO3NecdwTwh3S3HjsDlzdX9XoTML+qvprkdrpZ
j9Xp4mdjJ+DLvb9HMBP4YlXdnOS7dK8WAB8DrmqWxT0GfJjm30sHazHWdB8D/H4r3MV/J5K0XtJb
Qi1JkiRJmiqXCEqSJElSn9hgSZIkSVKf2GBJkiRJUp/YYEmSJElSn9hgSZIkSVKf2GBJ2uCS/EyS
e5rHM0meao03n7DvLWP3ZnqD+ZaO3ddrNfFrW+OTk1zap/dwcZJz+zGXJEkaXt4HS9IGV1X/A8wG
SHIh8GJV/UV7n+bm0Kmq46b46w5Jsm9VPTTFefqm9d5WDjoXSZK0YXkES9LAJNkryZIkVwH3Azu3
j04luTHJoiT3JzlzHaf9S3o3lZ74u1Y5ApXkwSSzmhwWJ/lCku8nuSLJcUm+neThJHNa0xyU5I4m
/nutuc5PcmeS7yX55Jre23oXSJIkbXI8giVp0N4BnFFVCwF6B3t+al5VPZ9ka2Bhkuur6odrme9q
4Owke6xHDvsCJwEPAncBL1fVYUl+Gzgf+ECz3/7AYcB2wF1JbgLeDewGHAIEWJDkMGDZxPcmSZKG
n0ewJA3ao2/QgPxBknuB24FZwJ7rMN8Kekexzl+PHB6pqiXNEr4lwK1N/D5g99Z+X6mql6tqGfAN
4D3AscBc4G56zdlewD7N/m/03iRJ0hDyCJakQXtpdcEkRwNHAIdW1U+SfAvYch3n/DxwHvD9VmwF
q/5RqT3XK63tla3xSlb9f7Im/J6id9Tq4qq6bEL+e7GG9yZJkoaXR7Akbay2B55vmqt30jtatE6q
6lXgr4FzWuHH6S3nI8nBwK6TyOnEJFsk2RH4ZWAhcAvwkSTbNHPPSrLDJOaWJElDwAZL0sbqJmDr
JEuAi4HvrOfr/xFoXwL+S8BOSRYDZwGPTSKnxcDXgW8DI1X1bFUtAK4D7khyHzAf2HYSc0uSpCGQ
qokrXiRJkiRJk+ERLEmSJEnqExssSZIkSeoTGyxJkiRJ6hMbLEmSJEnqExssSZIkSeoTGyxJkiRJ
6hMbLEmSJEnqk/8DbWGM1t8THyQAAAAASUVORK5CYII=
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-7">Question 7<a class="anchor-link" href="#Question-7">&#182;</a></h3><p>Using the visualization above that was produced from your improved Q-Learning simulation, provide a final analysis and make observations about the improved driving agent like in <strong>Question 6</strong>. Questions you should answer:</p>
<ul>
<li><em>What decaying function was used for epsilon (the exploration factor)?</em></li>
<li><em>Approximately how many training trials were needed for your agent before begining testing?</em></li>
<li><em>What epsilon-tolerance and alpha (learning rate) did you use? Why did you use them?</em></li>
<li><em>How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section?</em></li>
<li><em>Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy?</em></li>
<li><em>Are you satisfied with the safety and reliability ratings of the </em>Smartcab<em>?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>I used a^t as my decaying function.a was set to 0.97.</p>
<p>The agent took approximately 750 training trails before it started testing.</p>
<p>I set the epsilon tolerance to 0.0000000001.This was done to increase the number of training trails the agent would go through before starting the testing.Since there are 384 possible states,I wanted the smart cab to go through close to twice the amount of training trails.Hence I chose that epsilon tolerance value.The learning rate was set to 0.97.This was done to increase the weight of the rewards for the actions.</p>
<p>Compared to the default Q-learner, the relative frequency of bad actions has come down and the rolling average rewards has remained in the positive region.Also, the testing trails got  A rating for safety and reliability, where as the default learner got 'F' rating.</p>
<p>Based on the relative frequency of bad actions and rolling average rewards, it seems the driving agent has learned an appropriate policy.</p>
<p>Yes.Both Safety and Reliability rating is A.Am satisfied with the ratings.
**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-an-Optimal-Policy">Define an Optimal Policy<a class="anchor-link" href="#Define-an-Optimal-Policy">&#182;</a></h3><p>Sometimes, the answer to the important question <em>"what am I trying to get my agent to learn?"</em> only has a theoretical answer and cannot be concretely described. Here, however, you can concretely define what it is the agent is trying to learn, and that is the U.S. right-of-way traffic laws. Since these laws are known information, you can further define, for each state the <em>Smartcab</em> is occupying, the optimal action for the driving agent based on these laws. In that case, we call the set of optimal state-action pairs an <strong>optimal policy</strong>. Hence, unlike some theoretical answers, it is clear whether the agent is acting "incorrectly" not only by the reward (penalty) it receives, but also by pure observation. If the agent drives through a red light, we both see it receive a negative reward but also know that it is not the correct behavior. This can be used to your advantage for verifying whether the <strong>policy</strong> your driving agent has learned is the correct one, or if it is a <strong>suboptimal policy</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-8">Question 8<a class="anchor-link" href="#Question-8">&#182;</a></h3><ol>
<li><p>Please summarize what the optimal policy is for the smartcab in the given environment. What would be the best set of instructions possible given what we know about the environment? 
<em>You can explain with words or a table, but you should thoroughly discuss the optimal policy.</em></p>
</li>
<li><p>Next, investigate the <code>'sim_improved-learning.txt'</code> text file to see the results of your improved Q-Learning algorithm. <em>For each state that has been recorded from the simulation, is the <strong>policy</strong> (the action with the highest value) correct for the given state? Are there any states where the policy is different than what would be expected from an optimal policy?</em></p>
</li>
<li><p>Provide a few examples from your recorded Q-table which demonstrate that your smartcab learned the optimal policy. Explain why these entries demonstrate the optimal policy.</p>
</li>
<li><p>Try to find at least one entry where the smartcab did <em>not</em> learn the optimal policy.  Discuss why your cab may have not learned the correct policy for the given state.</p>
</li>
</ol>
<p>Be sure to document your <code>state</code> dictionary below, it should be easy for the reader to understand what each state represents.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>**Answer:</p>
<p>1.Optimal policy can be summarized as follows</p>
<table>
<thead><tr>
<th style="text-align:center">Waypoint</th>
<th style="text-align:center">Light</th>
<th style="text-align:center">left</th>
<th style="text-align:center">right</th>
<th style="text-align:center">oncoming</th>
<th style="text-align:center">Optimal action</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">forward</td>
<td style="text-align:center">Red</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">None</td>
</tr>
<tr>
<td style="text-align:center">forward</td>
<td style="text-align:center">Green</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">forward</td>
</tr>
<tr>
<td style="text-align:center">Right</td>
<td style="text-align:center">Red</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">forward/left</td>
<td style="text-align:center">forward</td>
<td style="text-align:center">None</td>
</tr>
<tr>
<td style="text-align:center">Right</td>
<td style="text-align:center">Red</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">right</td>
<td style="text-align:center">Not forward</td>
<td style="text-align:center">Right</td>
</tr>
<tr>
<td style="text-align:center">Right</td>
<td style="text-align:center">Green</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Right</td>
</tr>
<tr>
<td style="text-align:center">Left</td>
<td style="text-align:center">Red</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">None</td>
</tr>
<tr>
<td style="text-align:center">Left</td>
<td style="text-align:center">Green</td>
<td style="text-align:center">Not forward</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">None</td>
<td style="text-align:center">Left</td>
</tr>
<tr>
<td style="text-align:center">Left</td>
<td style="text-align:center">Green</td>
<td style="text-align:center">Not forward</td>
<td style="text-align:center">Any</td>
<td style="text-align:center">forward</td>
<td style="text-align:center">None</td>
</tr>
</tbody>
</table>
<ol>
<li><p>Most states have the optimal policy.However some states do not have the optimal policy rewards.Please see examples below.</p>
</li>
<li><p>Example1:</p>
</li>
</ol>
<p>(waypoint:'left', light:'red',left: None, right:'left',oncoming: 'right')</p>
<p>-- forward : -8.78</p>
<p>-- right : 0.00</p>
<p>-- None : 2.77</p>
<p>-- left : -39.56</p>
<p>Since the light is red and the waypoint is left, the optimal policy for smartcab is to  not turn left and stay at its current position.In the Q-table, I see that the reward for moving left is -39.56 and the highest reward is for No action.This matches with the optimal policy</p>
<p>Example2:</p>
<p>(waypoint:'left', light:'green', left:'right', right:'forward', oncoming:'forward')</p>
<p>-- forward : 0.00</p>
<p>-- right : 0.46</p>
<p>-- None : -3.93</p>
<p>-- left : -19.86</p>
<p>When the light is green and the waypoint is left, the optimal policy is not to turn left when there is an oncoming vehicle travelling forward.Also, since the light is green, the smartcab should also not be in stopped state.The reward for None and left are negative and they match the optimal policy.</p>
<p>4.</p>
<p>Here is an example where the smartcab hasn't learned the optimal policy.</p>
<p>(waypoint: 'left', 'green', 'left', 'right', 'left')</p>
<p>-- forward : 0.00</p>
<p>-- right : 0.93</p>
<p>-- None : 0.00</p>
<p>-- left : 0.00</p>
<p>In this case, the cab should be turning left.The light is green and the oncoming vehicle is turning left.The optimal action in this case should be turning left.There is no reward for turning left in this case.Turning right has a reward of 0.93.</p>
<p>Initially rewards for all actions is set to 0.When this state was first reached, the action to be taken would have been selected in random.This random action would have been 'right'.Since this action wouldnt have caused any violation, based on the learning rate, the reward could have been set to 0.93.The agent might not have reached this state again during the training trails.Hence the agent might not havr reached the optimal policy for this state.
**</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h3 id="Optional:-Future-Rewards---Discount-Factor,-'gamma'">Optional: Future Rewards - Discount Factor, <code>'gamma'</code><a class="anchor-link" href="#Optional:-Future-Rewards---Discount-Factor,-'gamma'">&#182;</a></h3><p>Curiously, as part of the Q-Learning algorithm, you were asked to <strong>not</strong> use the discount factor, <code>'gamma'</code> in the implementation. Including future rewards in the algorithm is used to aid in propagating positive rewards backwards from a future state to the current state. Essentially, if the driving agent is given the option to make several actions to arrive at different states, including future rewards will bias the agent towards states that could provide even more rewards. An example of this would be the driving agent moving towards a goal: With all actions and rewards equal, moving towards the goal would theoretically yield better rewards if there is an additional reward for reaching the goal. However, even though in this project, the driving agent is trying to reach a destination in the allotted time, including future rewards will not benefit the agent. In fact, if the agent were given many trials to learn, it could negatively affect Q-values!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Optional-Question-9">Optional Question 9<a class="anchor-link" href="#Optional-Question-9">&#182;</a></h3><p><em>There are two characteristics about the project that invalidate the use of future rewards in the Q-Learning algorithm. One characteristic has to do with the </em>Smartcab<em> itself, and the other has to do with the environment. Can you figure out what they are and why future rewards won't work for this project?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>Note</strong>: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to<br>
<strong>File -&gt; Download as -&gt; HTML (.html)</strong>. Include the finished document along with this notebook as your submission.</p>
</blockquote>

</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
